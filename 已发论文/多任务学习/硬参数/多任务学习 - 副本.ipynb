{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>日期</th>\n",
       "      <th>AQI</th>\n",
       "      <th>质量等级</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>O3_8h</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>年份</th>\n",
       "      <th>AQI平均值</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-02</td>\n",
       "      <td>180</td>\n",
       "      <td>中度污染</td>\n",
       "      <td>136</td>\n",
       "      <td>253</td>\n",
       "      <td>87</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-03</td>\n",
       "      <td>234</td>\n",
       "      <td>重度污染</td>\n",
       "      <td>184</td>\n",
       "      <td>297</td>\n",
       "      <td>108</td>\n",
       "      <td>2.5</td>\n",
       "      <td>133</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-04</td>\n",
       "      <td>386</td>\n",
       "      <td>严重污染</td>\n",
       "      <td>336</td>\n",
       "      <td>455</td>\n",
       "      <td>51</td>\n",
       "      <td>2.5</td>\n",
       "      <td>130</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-05</td>\n",
       "      <td>328</td>\n",
       "      <td>严重污染</td>\n",
       "      <td>278</td>\n",
       "      <td>386</td>\n",
       "      <td>52</td>\n",
       "      <td>2.3</td>\n",
       "      <td>110</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-06</td>\n",
       "      <td>319</td>\n",
       "      <td>严重污染</td>\n",
       "      <td>269</td>\n",
       "      <td>389</td>\n",
       "      <td>61</td>\n",
       "      <td>2.2</td>\n",
       "      <td>92</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>2022-08-24</td>\n",
       "      <td>40</td>\n",
       "      <td>优</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>13</td>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>2022-08-25</td>\n",
       "      <td>59</td>\n",
       "      <td>良</td>\n",
       "      <td>18</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>21</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>2022-08-26</td>\n",
       "      <td>68</td>\n",
       "      <td>良</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>2022-08-27</td>\n",
       "      <td>49</td>\n",
       "      <td>优</td>\n",
       "      <td>28</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>15</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>2022-08-28</td>\n",
       "      <td>70</td>\n",
       "      <td>良</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>11</td>\n",
       "      <td>123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3192 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             日期  AQI  质量等级  PM2.5  PM10  SO2   CO  NO2  O3_8h  Unnamed: 9  \\\n",
       "0    2013-12-02  180  中度污染    136   253   87  2.0  120     25         NaN   \n",
       "1    2013-12-03  234  重度污染    184   297  108  2.5  133     17         NaN   \n",
       "2    2013-12-04  386  严重污染    336   455   51  2.5  130     43         NaN   \n",
       "3    2013-12-05  328  严重污染    278   386   52  2.3  110     59         NaN   \n",
       "4    2013-12-06  319  严重污染    269   389   61  2.2   92     36         NaN   \n",
       "...         ...  ...   ...    ...   ...  ...  ...  ...    ...         ...   \n",
       "3187 2022-08-24   40     优      8    33    5  0.4   13     79         NaN   \n",
       "3188 2022-08-25   59     良     18    48    6  0.6   21    110         NaN   \n",
       "3189 2022-08-26   68     良     19    40    6  0.5   22    121         NaN   \n",
       "3190 2022-08-27   49     优     28    36    5  0.6   15     97         NaN   \n",
       "3191 2022-08-28   70     良     10    30    4  0.4   11    123         NaN   \n",
       "\n",
       "      Unnamed: 10  Unnamed: 11  年份 AQI平均值 Unnamed: 14 Unnamed: 15 Unnamed: 16  \\\n",
       "0             NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "1             NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "2             NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "3             NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "4             NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "...           ...          ...  ..    ...         ...         ...         ...   \n",
       "3187          NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "3188          NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "3189          NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "3190          NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "3191          NaN          NaN NaN    NaN         NaN         NaN         NaN   \n",
       "\n",
       "     Unnamed: 17 Unnamed: 18 Unnamed: 19  \n",
       "0            NaN         NaN         NaN  \n",
       "1            NaN         NaN         NaN  \n",
       "2            NaN         NaN         NaN  \n",
       "3            NaN         NaN         NaN  \n",
       "4            NaN         NaN         NaN  \n",
       "...          ...         ...         ...  \n",
       "3187         NaN         NaN         NaN  \n",
       "3188         NaN         NaN         NaN  \n",
       "3189         NaN         NaN         NaN  \n",
       "3190         NaN         NaN         NaN  \n",
       "3191         NaN         NaN         NaN  \n",
       "\n",
       "[3192 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\duduu\\Desktop\\2014-2022年8月空气污染数据(均值填充).xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分数据\n",
    "from sklearn.model_selection import train_test_split\n",
    "def data_one(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        ma = torch.max(data[:,i])\n",
    "        mi = torch.min(data[:,i])\n",
    "        cha = ma-mi\n",
    "        for k in range(data.shape[0]):\n",
    "            data[k,i] = (data[k,i]-mi)/cha\n",
    "    return data\n",
    "x = df.loc[:,(\"SO2\",\"CO\",\"NO2\",\"PM10\")].values\n",
    "y = df.loc[:,(\"PM2.5\",\"O3_8h\")].values\n",
    "x = torch.tensor(x,dtype=torch.float32)\n",
    "y = torch.tensor(y,dtype=torch.float32)\n",
    "x = data_one(x)\n",
    "y = data_one(y)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class share_model:\n",
    "\n",
    "    def __init__(self,lr=0.1,hidden_layer=3,epoches=50):\n",
    "        self.lr = lr\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.epoches = epoches\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        m = x.shape[0]\n",
    "        input_feature = x.shape[1]\n",
    "        output_feature = y.shape[1]\n",
    "        w1 = torch.randn(input_feature,self.hidden_layer,requires_grad=True)\n",
    "        b1 = torch.zeros(self.hidden_layer,requires_grad=True)\n",
    "        w2 = torch.randn(self.hidden_layer,output_feature,requires_grad=True)\n",
    "        b2 = torch.randn(output_feature,requires_grad=True)\n",
    "        func = torch.nn.ReLU()\n",
    "        for epoch in range(self.epoches):\n",
    "            for x1,y1 in TensorDataset(x,y):\n",
    "                y_hat = func(func(x1@w1+b1)@w2+b2)\n",
    "                loss = F.mse_loss(y_hat,y1)\n",
    "                loss.backward()\n",
    "                w1.data -= self.lr*w1.grad.data/m\n",
    "                w2.data -= self.lr*w2.grad.data/m\n",
    "                b1.data -= self.lr*b1.grad.data/m\n",
    "                b2.data -= self.lr*b2.grad.data/m\n",
    "                w1.grad.data.zero_()\n",
    "                b1.grad.data.zero_()\n",
    "                w2.grad.data.zero_()\n",
    "                b2.grad.data.zero_()\n",
    "            if epoch%10==0:\n",
    "                print(\"epoch:%d ,loss:%f\"%(epoch,loss))\n",
    "        self.parameters = {\n",
    "                    \"w1\":w1,\n",
    "                    \"b1\":b1,\n",
    "                    \"w2\":w2,\n",
    "                    \"b2\":b2}\n",
    "\n",
    "        return self.parameters\n",
    "\n",
    "    def predict(self,x):\n",
    "        w1 = self.parameters[\"w1\"]\n",
    "        w2 = self.parameters[\"w2\"]\n",
    "        b1 = self.parameters[\"b1\"]\n",
    "        b2 = self.parameters[\"b2\"]\n",
    "        func = torch.nn.ReLU()\n",
    "        y_pred = func(func(x@w1+b1)@w2+b2)\n",
    "        return y_pred\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class new_share_model:\n",
    "\n",
    "    def __init__(self,lr=0.1,hidden_layer=3,epoches=100,threshold=2):\n",
    "        self.lr = lr\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.epoches = epoches\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        m = x.shape[0]\n",
    "        batch_size = int(x.shape[0]*0.1)\n",
    "        input_feature = x.shape[1]\n",
    "        output_feature = y.shape[1]\n",
    "        w1 = torch.randn(input_feature,self.hidden_layer,requires_grad=True)\n",
    "        b1 = torch.zeros(self.hidden_layer,requires_grad=True)\n",
    "        w2 = torch.randn(self.hidden_layer,output_feature,requires_grad=True)\n",
    "        b2 = torch.randn(output_feature,requires_grad=True)\n",
    "        func = torch.nn.ReLU()\n",
    "        for epoch in range(self.epoches):\n",
    "            for x_batch,y_batch in DataLoader(TensorDataset(x,y),batch_size,shuffle=True):\n",
    "                m1 = x_batch.shape[0]\n",
    "                y_hat = func(func(x_batch@w1+b1)@w2+b2)\n",
    "                loss = F.mse_loss(y_hat,y_batch)\n",
    "                loss1 = F.mse_loss(y_hat[:,0],y_batch[:,0])\n",
    "                loss2 = F.mse_loss(y_hat[:,1],y_batch[:,1])\n",
    "                loss.backward(retain_graph=True)\n",
    "                with torch.no_grad():\n",
    "                    w1.data -= self.lr*w1.grad.data*(m1/m)\n",
    "                    w2.data -= self.lr*w2.grad.data*(m1/m)\n",
    "                    b1.data -= self.lr*b1.grad.data*(m1/m)\n",
    "                    b2.data -= self.lr*b2.grad.data*(m1/m)\n",
    "                w1.grad.data.zero_()\n",
    "                b1.grad.data.zero_()\n",
    "                w2.grad.data.zero_()\n",
    "                b2.grad.data.zero_()\n",
    "                loss1.backward(retain_graph=True)\n",
    "                with torch.no_grad():\n",
    "                    grad1_w1 = w1.grad.data.clone()\n",
    "                    w1.grad.data.zero_()\n",
    "                    b1.grad.data.zero_()\n",
    "                    w2.grad.data.zero_()\n",
    "                    b2.grad.data.zero_()\n",
    "                loss2.backward()\n",
    "                with torch.no_grad():\n",
    "                    grad2_w1 = w1.grad.data.clone()\n",
    "                    w1.grad.data.zero_()\n",
    "                    b1.grad.data.zero_()\n",
    "                    w2.grad.data.zero_()\n",
    "                    b2.grad.data.zero_()\n",
    "                count = torch.zeros(input_feature,self.hidden_layer)\n",
    "                for i in range(grad1_w1.shape[0]):\n",
    "                    for k in range(grad1_w1.shape[1]):\n",
    "                        if (grad1_w1[i,k]>=0 and grad2_w1[i,k]<0) or (grad1_w1[i,k]<0 and grad2_w1[i,k]>=0):\n",
    "                            count[i,k] += 1\n",
    "                if torch.max(count) >= self.threshold:\n",
    "                    index = (torch.argmax(count)+1)%self.hidden_layer\n",
    "                    if index == 0:\n",
    "                        index = self.hidden_layer-1\n",
    "                    else:\n",
    "                        index -= 1\n",
    "                    self.index = index\n",
    "                    break\n",
    "            if epoch%10==0:\n",
    "                print(\"epoch:%d , loss:%f\"%(epoch,loss))\n",
    "            #\n",
    "            if torch.max(count)>=self.threshold:\n",
    "                w1_new = w1[:,index].reshape(input_feature,1)\n",
    "                b1_new = b1[index].reshape(1)\n",
    "                w1 = torch.cat([w1,w1_new],1)\n",
    "                b1 = torch.cat([b1,b1_new])\n",
    "                w2 = torch.cat([w2,w2[index,:].reshape(1,2)],0)\n",
    "                w21 = w2[:,0]\n",
    "                w21 = w21[torch.arange(w21.size(0))!=index] \n",
    "                w22 = w2[:,1]\n",
    "                w22 = w22[torch.arange(w22.size(0))!=index]\n",
    "                for epoch1 in range(self.epoches):\n",
    "                    for x_batch,y_batch in DataLoader(TensorDataset(x,y),batch_size,shuffle=True):\n",
    "                        m = x.shape[0]\n",
    "                        hidden = func(x_batch@w1+b1)#m*4\n",
    "                        y1_hat = func(hidden[:,0:-1]@w21+b2[0]).reshape(-1,1)\n",
    "                        y2_hat = func(torch.cat([hidden[:,:index],hidden[:,index+1:]],1)@w22+b2[1]).reshape(-1,1)\n",
    "                        y_hat = torch.cat([y1_hat,y2_hat],1)\n",
    "                        loss = F.mse_loss(y_hat,y_batch)\n",
    "                        loss.backward()\n",
    "                        m1 = x_batch.shape[0]\n",
    "                        with torch.no_grad():\n",
    "                            print(w1.grad)\n",
    "                            w1.data -= self.lr*w1.grad.data*(m1/m)\n",
    "                            w21.data -= self.lr*w21.grad.data*(m1/m)\n",
    "                            w22.data -= self.lr*w22.grad.data*(m1/m)\n",
    "                            b1.data -= self.lr*b1.grad.data*(m1/m)\n",
    "                            b2.data -= self.lr*b2.grad.data*(m1/m)\n",
    "                        w1.grad.data.zero_()\n",
    "                        b1.grad.data.zero_()\n",
    "                        w21.grad.data.zero_()\n",
    "                        w22.grad.data.zero_()\n",
    "                        b2.grad.data.zero_()\n",
    "                    if epoch1%10==0:\n",
    "                        print(\"更新节点epoch:%d , loss:%f\"%(epoch1,loss))\n",
    "                \n",
    "                self.parameters = {\n",
    "                        \"w1\":w1,\n",
    "                        \"b1\":b1,\n",
    "                        \"w21\":w21,\n",
    "                        \"w22\":w22,\n",
    "                        \"b2\":b2}\n",
    "            if torch.max(count)>=self.threshold:\n",
    "                break     \n",
    "            if epoch == self.epoches-1:\n",
    "                self.parameters = {\n",
    "                        \"w1\":w1,\n",
    "                        \"b1\":b1,\n",
    "                        \"w2\":w2,\n",
    "                        \"b2\":b2}\n",
    "        \n",
    "\n",
    "        return self.parameters\n",
    "\n",
    "    def predict1(self,x):\n",
    "        w1 = self.parameters[\"w1\"]\n",
    "        w2 = self.parameters[\"w2\"]\n",
    "        b1 = self.parameters[\"b1\"]\n",
    "        b2 = self.parameters[\"b2\"]\n",
    "        func = torch.nn.ReLU()\n",
    "        y_pred = func(func(x@w1+b1)@w2+b2)\n",
    "        return y_pred\n",
    "\n",
    "    def predict2(self,x):\n",
    "        w1 = self.parameters[\"w1\"]\n",
    "        w21 = self.parameters[\"w21\"]\n",
    "        w22 = self.parameters[\"w22\"]\n",
    "        b1 = self.parameters[\"b1\"]\n",
    "        b2 = self.parameters[\"b2\"]\n",
    "        func = torch.nn.ReLU()\n",
    "        hidden = func(x@w1+b1)#m*4\n",
    "        y1_hat = func(hidden[:,0:-1]@w21+b2[0])\n",
    "        y2_hat = func(torch.cat([hidden[:,:self.index],hidden[:,self.index+1:]],1)@w22+b2[1])\n",
    "        y_pred = torch.cat([y1_hat,y2_hat],1)\n",
    "        return y_pred\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模拟均匀数据\n",
    "n = 3000#n为样本量，后文同理\n",
    "from sklearn.model_selection import train_test_split\n",
    "def data_one(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        ma = torch.max(data[:,i])\n",
    "        mi = torch.min(data[:,i])\n",
    "        cha = ma-mi\n",
    "        # print(ma,mi,min(data[:,i]))\n",
    "        for k in range(data.shape[0]):\n",
    "            data[k,i] = (data[k,i]-mi)/cha\n",
    "    return data\n",
    "x = torch.tensor(np.random.uniform(0,8,(n,5)),dtype=torch.float32)\n",
    "w1 = torch.tensor(np.random.uniform(0,4,(5,4)),dtype=torch.float32)\n",
    "w2 = torch.tensor(np.random.uniform(0,4,(4,2)),dtype=torch.float32)\n",
    "noise = torch.randn(n,2)\n",
    "y = data_one(x@w1@w2+noise)\n",
    "x = data_one(x)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
    "\n",
    "count = 0\n",
    "for i in range(2):\n",
    "    for k in range(y.shape[0]):\n",
    "        if y[k,i] >1:\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正态分布\n",
    "n = 1000\n",
    "x = torch.tensor(np.random.normal(0,1,(n,5)),dtype=torch.float32)\n",
    "w1 = torch.tensor(np.random.normal(0,1,(5,4)),dtype=torch.float32)\n",
    "w2 = torch.tensor(np.random.normal(0,1,(4,2)),dtype=torch.float32)\n",
    "noise = torch.normal(0,0.2,(n,2))\n",
    "y = data_one(x@w1@w2+noise)\n",
    "x = data_one(x)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指数分布\n",
    "n = 3000\n",
    "x = torch.tensor(np.random.exponential(10,(n,5)),dtype=torch.float32)\n",
    "w1 = torch.normal(0,1,(5,4),dtype=torch.float32)\n",
    "w2 = torch.normal(0,1,(4,2),dtype=torch.float32)\n",
    "noise = torch.normal(0,1,(n,2))\n",
    "y = data_one(x@w1@w2+noise)\n",
    "x = data_one(x)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#硬参数网络搭建\n",
    "class MTLnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MTLnet, self).__init__()\n",
    "\n",
    "        self.sharedlayer = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4,3),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout()\n",
    "        )\n",
    "        self.y1 = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout(),\n",
    "        )\n",
    "        self.y2 = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_shared = self.sharedlayer(x)\n",
    "        y1 = self.y1(h_shared)\n",
    "        y2 = self.y2(h_shared)\n",
    "        if len(y1)==1:\n",
    "            return torch.cat([y1, y2])\n",
    "        else:\n",
    "            return torch.cat([y1,y2],1)\n",
    "\n",
    "def param_read():\n",
    "    count = 0\n",
    "    for name,param in model.named_parameters():\n",
    "        if count==2:\n",
    "            a1 = param.grad.data.clone().T\n",
    "        count += 1\n",
    "    return a1\n",
    "\n",
    "def param_read1():\n",
    "    count = 0\n",
    "    for name,param in model1.named_parameters():\n",
    "        if count==2:\n",
    "            a1 = param.grad.data.clone().T\n",
    "        count += 1\n",
    "    return a1\n",
    "\n",
    "def param_read2():\n",
    "    count = 0\n",
    "    for name,param in model2.named_parameters():\n",
    "        if count==2:\n",
    "            a1 = param.grad.data.clone().T\n",
    "        count += 1\n",
    "    return a1\n",
    "\n",
    "def matr(m1,m2):\n",
    "    return (m1>=0) == (m2>=0)\n",
    "\n",
    "def index_func(x):\n",
    "    if x == 0:\n",
    "        return [0,2,3],[1,2,3]\n",
    "    elif x == 1:\n",
    "        return [0,1,3],[1,2,3]\n",
    "    else:\n",
    "        return [0,1,2],[1,2,3]\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 , test_loss:0.015810 , train_loss:0.015401\n",
      "epoch:20 , test_loss:0.015330 , train_loss:0.014803\n",
      "epoch:30 , test_loss:0.015099 , train_loss:0.014549\n",
      "epoch:40 , test_loss:0.014935 , train_loss:0.014370\n",
      "epoch:50 , test_loss:0.014802 , train_loss:0.014212\n",
      "epoch:60 , test_loss:0.014668 , train_loss:0.014037\n",
      "epoch:70 , test_loss:0.014524 , train_loss:0.013861\n",
      "epoch:80 , test_loss:0.014568 , train_loss:0.013976\n",
      "达到终止条件，迭代训练终止\n",
      "复制的结点序号为：1\n"
     ]
    }
   ],
   "source": [
    "model = MTLnet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "cm1 = torch.zeros(4,3)\n",
    "threshold = 10\n",
    "for epoch in range(200):\n",
    "    for x_batch,y_batch in TensorDataset(x_train,y_train):\n",
    "        y_hat = model.forward(x_batch)\n",
    "        #计算各损失函数\n",
    "        loss_f = nn.MSELoss(reduction='none')\n",
    "        loss = loss_f(y_batch,y_hat).sum()\n",
    "        # print(y_batch,y_hat,loss)\n",
    "        loss1 = loss_f(y_batch[0],y_hat[0])\n",
    "        loss2 = loss_f(y_batch[1],y_hat[1])\n",
    "        optimizer.zero_grad()\n",
    "        #对任务1损失函数进行反向传播\n",
    "        loss1.backward(retain_graph=True)\n",
    "        w2_grad1 = param_read()\n",
    "        optimizer.zero_grad()\n",
    "        #对任务2损失函数进行反向传播\n",
    "        loss2.backward(retain_graph=True)\n",
    "        w2_grad2 = param_read()\n",
    "        optimizer.zero_grad()\n",
    "        #对总损失进行反向传播并更新参数\n",
    "        loss.backward()\n",
    "        # optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        # print(w2_grad1,w2_grad2)\n",
    "        #求梯度方向异号位置\n",
    "        dm = matr(w2_grad1,w2_grad2)\n",
    "        cm2 = cm1.clone()\n",
    "        for i in range(dm.shape[0]):\n",
    "            for k in range(dm.shape[1]):\n",
    "                if dm[i,k]==False:\n",
    "                    cm1[i,k] += 1 \n",
    "                if cm1[i,k]!=cm2[i,k]+1:\n",
    "                    cm1[i,k]=0\n",
    "        # print(dm)\n",
    "        if torch.max(cm1) >= threshold and epoch+1>=80:\n",
    "            index = (torch.argmax(cm1)+1)%3\n",
    "            if index == 0:\n",
    "                index = 3-1\n",
    "            else:\n",
    "                index -= 1\n",
    "            break\n",
    "\n",
    "    if (epoch+1)%10==0:\n",
    "        y_pred = model.forward(x_test)\n",
    "        loss = ((y_pred-y_test)**2).sum()/(len(y_pred)*2)\n",
    "        y_pred1 = model.forward(x_train)\n",
    "        loss1 = ((y_pred1-y_train)**2).sum()/(len(y_pred1)*2)\n",
    "        print(\"epoch:%d , test_loss:%f , train_loss:%f\"%(epoch+1,loss,loss1))\n",
    "\n",
    "    if torch.max(cm1) >= threshold and epoch+1 >= 80:\n",
    "        \n",
    "        print(\"达到终止条件，迭代训练终止\")\n",
    "        print(f\"复制的结点序号为：{index}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 , test_loss:0.014299 , train_loss:0.013645\n",
      "epoch:10 , test_loss:0.014256 , train_loss:0.013587\n",
      "epoch:15 , test_loss:0.014224 , train_loss:0.013539\n",
      "epoch:20 , test_loss:0.014197 , train_loss:0.013498\n",
      "epoch:25 , test_loss:0.014174 , train_loss:0.013460\n",
      "epoch:30 , test_loss:0.014154 , train_loss:0.013423\n",
      "epoch:35 , test_loss:0.014137 , train_loss:0.013389\n",
      "epoch:40 , test_loss:0.014122 , train_loss:0.013359\n",
      "epoch:45 , test_loss:0.014110 , train_loss:0.013333\n",
      "epoch:50 , test_loss:0.013916 , train_loss:0.013189\n",
      "达到终止条件，迭代训练终止\n",
      "复制的结点序号为：3\n"
     ]
    }
   ],
   "source": [
    "#第一次复制节点\n",
    "class MTLnet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MTLnet1, self).__init__()\n",
    "\n",
    "        self.sharedlayer = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4,4),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout()\n",
    "        )\n",
    "        self.y1 = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout(),\n",
    "        )\n",
    "        self.y2 = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_shared = self.sharedlayer(x)\n",
    "        index_list1,index_list2 = index_func(index)\n",
    "        if x.shape[0] == 4:\n",
    "            y1 = self.y1(h_shared[index_list1])\n",
    "            y2 = self.y2(h_shared[index_list2])\n",
    "        else:\n",
    "            y1 = self.y1(h_shared[:,index_list1])\n",
    "            y2 = self.y2(h_shared[:,index_list2])\n",
    "        if len(y1)==1:\n",
    "            return torch.cat([y1, y2])\n",
    "        else:\n",
    "            return torch.cat([y1,y2],1)\n",
    "\n",
    "model1 = MTLnet1()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.005)\n",
    "w0 = model.state_dict()[\"sharedlayer.0.weight\"].T\n",
    "b0 = model.state_dict()[\"sharedlayer.0.bias\"]\n",
    "w1 = model.state_dict()[\"sharedlayer.2.weight\"].T\n",
    "b1 = model.state_dict()[\"sharedlayer.2.bias\"]\n",
    "w21 = model.state_dict()[\"y1.0.weight\"].T\n",
    "w22 = model.state_dict()[\"y2.0.weight\"].T\n",
    "b21 = model.state_dict()[\"y1.0.bias\"]\n",
    "b22 = model.state_dict()[\"y2.0.bias\"]\n",
    "index_list = [0,1,2]\n",
    "index_list.pop(index)\n",
    "w1 = torch.cat([w1[:,index].reshape(4,1),w1],1)\n",
    "b1 = torch.cat([b1[index].reshape(1),b1])\n",
    "w21 = torch.cat([w21[index].reshape(1,1),w21[index_list]])\n",
    "count = 0\n",
    "for m in model1.parameters():\n",
    "    if count == 0:\n",
    "        m.data = w0.T\n",
    "    if count == 1:\n",
    "        m.data = b0\n",
    "    if count == 2:\n",
    "        m.data = w1.T\n",
    "    if count == 3:\n",
    "        m.data = b1\n",
    "    if count == 4:\n",
    "        m.data = w21.T\n",
    "    if count == 5:\n",
    "        m.data = b21\n",
    "    if count == 6:\n",
    "        m.data = w22.T\n",
    "    if count == 7:\n",
    "        m.data = b22\n",
    "    count += 1\n",
    "\n",
    "# print(model1.forward(x_test))\n",
    "# print(model.forward(x_test))\n",
    "\n",
    "cm1 = torch.zeros(4,4)\n",
    "threshold = 8\n",
    "for epoch in range(50):\n",
    "    for x_batch,y_batch in TensorDataset(x_train,y_train):\n",
    "        y_hat = model1.forward(x_batch)\n",
    "        #计算各损失函数\n",
    "        loss_f = nn.MSELoss(reduction='none')\n",
    "        loss = loss_f(y_batch,y_hat).sum()\n",
    "        # print(y_batch,y_hat,loss)\n",
    "        loss1 = loss_f(y_batch[0],y_hat[0])\n",
    "        loss2 = loss_f(y_batch[1],y_hat[1])\n",
    "        optimizer.zero_grad()\n",
    "        #对任务1损失函数进行反向传播\n",
    "        loss1.backward(retain_graph=True)\n",
    "        w2_grad1 = param_read1()\n",
    "        optimizer.zero_grad()\n",
    "        #对任务2损失函数进行反向传播\n",
    "        loss2.backward(retain_graph=True)\n",
    "        w2_grad2 = param_read1()\n",
    "        optimizer.zero_grad()\n",
    "        #对总损失进行反向传播并更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(w2_grad1,w2_grad2)\n",
    "        #求梯度方向异号位置\n",
    "        dm = matr(w2_grad1,w2_grad2)\n",
    "        cm2 = cm1.clone()\n",
    "        for i in range(dm.shape[0]):\n",
    "            for k in range(dm.shape[1]):\n",
    "                if k == 0 or k == index+1:\n",
    "                    continue\n",
    "                if dm[i,k]==False:\n",
    "                    cm1[i,k] += 1 \n",
    "                if cm1[i,k]!=cm2[i,k]+1:\n",
    "                    cm1[i,k]=0\n",
    "        # print(dm)\n",
    "        if torch.max(cm1) >= threshold and epoch+1 >= 50:\n",
    "            index1 = (torch.argmax(cm1)+1)%4\n",
    "            if index1 == 0:\n",
    "                index1 = 4-1\n",
    "            else:\n",
    "                index1 -= 1\n",
    "            break\n",
    "\n",
    "    if (epoch+1)%5==0:\n",
    "        y_pred = model1.forward(x_test)\n",
    "        loss = ((y_pred-y_test)**2).sum()/(len(y_pred)*2)\n",
    "        y_pred1 = model1.forward(x_train)\n",
    "        loss1 = ((y_pred1-y_train)**2).sum()/(len(y_pred1)*2)\n",
    "        print(\"epoch:%d , test_loss:%f , train_loss:%f\"%(epoch+1,loss,loss1))\n",
    "\n",
    "    if torch.max(cm1) >= threshold and epoch+1 >= 50:\n",
    "        print(\"达到终止条件，迭代训练终止\")\n",
    "        print(f\"复制的结点序号为：{index1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 , test_loss:0.014090 , train_loss:0.013307\n",
      "epoch:10 , test_loss:0.014080 , train_loss:0.013278\n",
      "epoch:15 , test_loss:0.014074 , train_loss:0.013258\n",
      "epoch:20 , test_loss:0.014069 , train_loss:0.013241\n",
      "epoch:25 , test_loss:0.014064 , train_loss:0.013227\n",
      "epoch:30 , test_loss:0.014061 , train_loss:0.013215\n",
      "epoch:35 , test_loss:0.014058 , train_loss:0.013204\n",
      "epoch:40 , test_loss:0.014055 , train_loss:0.013195\n",
      "epoch:45 , test_loss:0.014051 , train_loss:0.013186\n",
      "epoch:50 , test_loss:0.014048 , train_loss:0.013178\n",
      "达到终止条件，迭代训练终止\n",
      "复制的结点序号为：2\n"
     ]
    }
   ],
   "source": [
    "#第二次复制结点\n",
    "class MTLnet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MTLnet2, self).__init__()\n",
    "\n",
    "        self.sharedlayer = nn.Sequential(\n",
    "            nn.Linear(5,4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4,5),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout()\n",
    "        )\n",
    "        self.y1 = nn.Sequential(\n",
    "            nn.Linear(3,1),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout(),\n",
    "        )\n",
    "        self.y2 = nn.Sequential(\n",
    "            nn.Linear(3,1),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_shared = self.sharedlayer(x)\n",
    "        a = [0,1,2]\n",
    "        a.remove(index)\n",
    "        a.remove(index1-1)\n",
    "        b = a[0]+2\n",
    "        index_list1 = [1,0,b]\n",
    "        index_list2 = [2,3,4]\n",
    "        if x.shape[0] == 4:\n",
    "            y1 = self.y1(h_shared[index_list1])\n",
    "            y2 = self.y2(h_shared[index_list2])\n",
    "        else:\n",
    "            y1 = self.y1(h_shared[:,index_list1])\n",
    "            y2 = self.y2(h_shared[:,index_list2])\n",
    "        if len(y1)==1:\n",
    "            return torch.cat([y1,y2])\n",
    "        else:\n",
    "            return torch.cat([y1,y2],1)\n",
    "model2 = MTLnet2()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.005)\n",
    "w0 = model1.state_dict()[\"sharedlayer.0.weight\"].T\n",
    "b0 = model1.state_dict()[\"sharedlayer.0.bias\"]\n",
    "w1 = model1.state_dict()[\"sharedlayer.2.weight\"].T\n",
    "w1 = torch.cat([w1[:,index1].reshape(4,1),w1],1)\n",
    "b1 = model1.state_dict()[\"sharedlayer.2.bias\"]\n",
    "b1 = torch.cat([b1[index1].reshape(1),b1])\n",
    "w21 = model1.state_dict()[\"y1.0.weight\"].T\n",
    "index_list = [0,1,2]\n",
    "index_list.pop(index1-1)\n",
    "w21 = torch.cat([w21[index1-1].reshape(1,1),w21[index_list]])\n",
    "w22 = model1.state_dict()[\"y2.0.weight\"].T\n",
    "b21 = model1.state_dict()[\"y1.0.bias\"]\n",
    "b22 = model1.state_dict()[\"y2.0.bias\"]\n",
    "count = 0\n",
    "for m in model2.parameters():\n",
    "    if count == 0:\n",
    "        m.data = w0.T\n",
    "    if count == 1:\n",
    "        m.data = b0\n",
    "    if count == 2:\n",
    "        m.data = w1.T\n",
    "    if count == 3:\n",
    "        m.data = b1\n",
    "    if count == 4:\n",
    "        m.data = w21.T\n",
    "    if count == 5:\n",
    "        m.data = b21\n",
    "    if count == 6:\n",
    "        m.data = w22.T\n",
    "    if count == 7:\n",
    "        m.data = b22\n",
    "    count += 1\n",
    "\n",
    "# print(model1.forward(x_test))\n",
    "# print(model2.forward(x_test))\n",
    "# print(model2.state_dict()[\"sharedlayer.2.weight\"].T)\n",
    "# print(model1.state_dict()[\"sharedlayer.2.weight\"].T)\n",
    "# print(w21)\n",
    "# print(model1.state_dict()[\"y1.0.weight\"].T)\n",
    "\n",
    "cm1 = torch.zeros(4,5)\n",
    "threshold = 8\n",
    "for epoch in range(100):\n",
    "    for x_batch,y_batch in TensorDataset(x_train,y_train):\n",
    "        y_hat = model2.forward(x_batch)\n",
    "        #计算各损失函数\n",
    "        loss_f = nn.MSELoss(reduction='none')\n",
    "        loss = loss_f(y_batch,y_hat).sum()\n",
    "        # print(y_batch,y_hat,loss)\n",
    "        loss1 = loss_f(y_batch[0],y_hat[0])\n",
    "        loss2 = loss_f(y_batch[1],y_hat[1])\n",
    "        optimizer.zero_grad()\n",
    "        #对任务1损失函数进行反向传播\n",
    "        loss1.backward(retain_graph=True)\n",
    "        w2_grad1 = param_read2()\n",
    "        optimizer.zero_grad()\n",
    "        #对任务2损失函数进行反向传播\n",
    "        loss2.backward(retain_graph=True)\n",
    "        w2_grad2 = param_read2()\n",
    "        optimizer.zero_grad()\n",
    "        #对总损失进行反向传播并更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(w2_grad1,w2_grad2)\n",
    "        #求梯度方向异号位置\n",
    "        dm = matr(w2_grad1,w2_grad2)\n",
    "        cm2 = cm1.clone()\n",
    "        for i in range(dm.shape[0]):\n",
    "            for k in range(dm.shape[1]):\n",
    "                if k == 0 or k == 1 or k == index+2 or k == index1+1:\n",
    "                    continue\n",
    "                if dm[i,k]==False:\n",
    "                    cm1[i,k] += 1 \n",
    "                if cm1[i,k]!=cm2[i,k]+1:\n",
    "                    cm1[i,k]=0\n",
    "        # print(dm)\n",
    "        if torch.max(cm1) >= threshold and epoch>=50:\n",
    "            index2 = (torch.argmax(cm1)+1)%5\n",
    "            if index2 == 0:\n",
    "                index2 = 5-1\n",
    "            else:\n",
    "                index2 -= 1\n",
    "            break\n",
    "\n",
    "    if (epoch+1)%5==0:\n",
    "        y_pred = model2.forward(x_test)\n",
    "        loss = ((y_pred-y_test)**2).sum()/(len(y_pred)*2)\n",
    "        y_pred1 = model2.forward(x_train)\n",
    "        loss1 = ((y_pred1-y_train)**2).sum()/(len(y_pred1)*2)\n",
    "        print(\"epoch:%d , test_loss:%f , train_loss:%f\"%(epoch+1,loss,loss1))\n",
    "\n",
    "    if torch.max(cm1) >= threshold and epoch>=50:\n",
    "        print(\"达到终止条件，迭代训练终止\")\n",
    "        print(f\"复制的结点序号为：{index2}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 , test_loss:0.014157 , train_loss:0.013497\n",
      "epoch:20 , test_loss:0.013859 , train_loss:0.013142\n",
      "epoch:30 , test_loss:0.013812 , train_loss:0.013052\n",
      "epoch:40 , test_loss:0.013796 , train_loss:0.013009\n",
      "epoch:50 , test_loss:0.013791 , train_loss:0.012986\n",
      "epoch:60 , test_loss:0.013789 , train_loss:0.012971\n",
      "epoch:70 , test_loss:0.013789 , train_loss:0.012960\n",
      "epoch:80 , test_loss:0.013789 , train_loss:0.012952\n",
      "epoch:90 , test_loss:0.013789 , train_loss:0.012945\n",
      "epoch:100 , test_loss:0.013789 , train_loss:0.012939\n",
      "epoch:110 , test_loss:0.013789 , train_loss:0.012933\n",
      "epoch:120 , test_loss:0.013790 , train_loss:0.012928\n",
      "epoch:130 , test_loss:0.013790 , train_loss:0.012924\n",
      "epoch:140 , test_loss:0.013790 , train_loss:0.012919\n",
      "epoch:150 , test_loss:0.013790 , train_loss:0.012915\n",
      "epoch:160 , test_loss:0.013790 , train_loss:0.012910\n",
      "epoch:170 , test_loss:0.013790 , train_loss:0.012906\n",
      "epoch:180 , test_loss:0.013791 , train_loss:0.012902\n",
      "epoch:190 , test_loss:0.013791 , train_loss:0.012898\n",
      "epoch:200 , test_loss:0.013791 , train_loss:0.012894\n",
      "epoch:210 , test_loss:0.013791 , train_loss:0.012891\n",
      "epoch:220 , test_loss:0.013791 , train_loss:0.012887\n",
      "epoch:230 , test_loss:0.013791 , train_loss:0.012883\n",
      "epoch:240 , test_loss:0.013791 , train_loss:0.012879\n",
      "epoch:250 , test_loss:0.013791 , train_loss:0.012876\n",
      "epoch:260 , test_loss:0.013791 , train_loss:0.012872\n",
      "epoch:270 , test_loss:0.013791 , train_loss:0.012868\n",
      "epoch:280 , test_loss:0.013791 , train_loss:0.012865\n",
      "epoch:290 , test_loss:0.013790 , train_loss:0.012861\n",
      "epoch:300 , test_loss:0.013790 , train_loss:0.012857\n"
     ]
    }
   ],
   "source": [
    "#第三次复制节点\n",
    "class MTLnet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MTLnet3, self).__init__()\n",
    "\n",
    "        self.sharedlayer = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4,6),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout()\n",
    "        )\n",
    "        self.y1 = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout(),\n",
    "        )\n",
    "        self.y2 = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_shared = self.sharedlayer(x)\n",
    "        index_list1 = [0,1,2]\n",
    "        index_list2 = [3,4,5]\n",
    "        if x.shape[0] == 4:\n",
    "            y1 = self.y1(h_shared[index_list1])\n",
    "            y2 = self.y2(h_shared[index_list2])\n",
    "        else:\n",
    "            y1 = self.y1(h_shared[:,index_list1])\n",
    "            y2 = self.y2(h_shared[:,index_list2])\n",
    "        if len(y1)==1:\n",
    "            return torch.cat([y1,y2])\n",
    "        else:\n",
    "            return torch.cat([y1,y2],1)\n",
    "model3 = MTLnet3()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=0.001)\n",
    "w0 = model2.state_dict()[\"sharedlayer.0.weight\"].T\n",
    "b0 = model2.state_dict()[\"sharedlayer.0.bias\"]\n",
    "w1 = model2.state_dict()[\"sharedlayer.2.weight\"].T\n",
    "w1 = torch.cat([w1[:,index2].reshape(4,1),w1],1)\n",
    "b1 = model2.state_dict()[\"sharedlayer.2.bias\"]\n",
    "b1 = torch.cat([b1[index2].reshape(1),b1])\n",
    "w21 = model2.state_dict()[\"y1.0.weight\"].T\n",
    "index_list = [0,1,2]\n",
    "index_list.pop(index2-2)\n",
    "w21 = torch.cat([w21[index2-2].reshape(1,1),w21[index_list]])\n",
    "w22 = model2.state_dict()[\"y2.0.weight\"].T\n",
    "b21 = model2.state_dict()[\"y1.0.bias\"]\n",
    "b22 = model2.state_dict()[\"y2.0.bias\"]\n",
    "\n",
    "count = 0\n",
    "for m in model3.parameters():\n",
    "    if count == 0:\n",
    "        m.data = w0.T\n",
    "    if count == 1:\n",
    "        m.data = b0\n",
    "    if count == 2:\n",
    "        m.data = w1.T\n",
    "    if count == 3:\n",
    "        m.data = b1\n",
    "    if count == 4:\n",
    "        m.data = w21.T\n",
    "    if count == 5:\n",
    "        m.data = b21\n",
    "    if count == 6:\n",
    "        m.data = w22.T\n",
    "    if count == 7:\n",
    "        m.data = b22\n",
    "    count += 1\n",
    "# print(model3.state_dict()[\"sharedlayer.2.weight\"].T)\n",
    "# print(model2.forward(x_test))\n",
    "# print(model3.forward(x_test))\n",
    "\n",
    "for epoch in range(300):\n",
    "    for x_batch,y_batch in TensorDataset(x_train,y_train):\n",
    "        y_hat = model3.forward(x_batch)\n",
    "        loss_f = nn.MSELoss(reduction='none')\n",
    "        loss = loss_f(y_batch,y_hat).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1)%10==0:\n",
    "        y_pred = model3.forward(x_test)\n",
    "        loss = ((y_pred-y_test)**2).sum()/(len(y_pred)*2)\n",
    "        y_pred1 = model3.forward(x_train)\n",
    "        loss1 = ((y_pred1-y_train)**2).sum()/(len(y_pred1)*2)\n",
    "        print(\"epoch:%d , test_loss:%f , train_loss:%f\"%(epoch+1,loss,loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 , test_loss:0.016356 , train_loss:0.014841\n",
      "epoch:20 , test_loss:0.015518 , train_loss:0.014408\n",
      "epoch:30 , test_loss:0.015243 , train_loss:0.014279\n",
      "epoch:40 , test_loss:0.015013 , train_loss:0.014134\n",
      "epoch:50 , test_loss:0.014841 , train_loss:0.013995\n",
      "epoch:60 , test_loss:0.014748 , train_loss:0.013898\n",
      "epoch:70 , test_loss:0.014689 , train_loss:0.013825\n",
      "epoch:80 , test_loss:0.014646 , train_loss:0.013768\n",
      "epoch:90 , test_loss:0.014614 , train_loss:0.013721\n",
      "epoch:100 , test_loss:0.014588 , train_loss:0.013678\n",
      "epoch:110 , test_loss:0.014565 , train_loss:0.013638\n",
      "epoch:120 , test_loss:0.014543 , train_loss:0.013599\n",
      "epoch:130 , test_loss:0.014524 , train_loss:0.013564\n",
      "epoch:140 , test_loss:0.014508 , train_loss:0.013535\n",
      "epoch:150 , test_loss:0.014494 , train_loss:0.013513\n",
      "epoch:160 , test_loss:0.014483 , train_loss:0.013495\n",
      "epoch:170 , test_loss:0.014473 , train_loss:0.013482\n",
      "epoch:180 , test_loss:0.014464 , train_loss:0.013471\n",
      "epoch:190 , test_loss:0.014456 , train_loss:0.013462\n",
      "epoch:200 , test_loss:0.014448 , train_loss:0.013455\n",
      "epoch:210 , test_loss:0.014441 , train_loss:0.013448\n",
      "epoch:220 , test_loss:0.014434 , train_loss:0.013441\n",
      "epoch:230 , test_loss:0.014427 , train_loss:0.013436\n",
      "epoch:240 , test_loss:0.014420 , train_loss:0.013430\n",
      "epoch:250 , test_loss:0.014412 , train_loss:0.013423\n",
      "epoch:260 , test_loss:0.014403 , train_loss:0.013416\n",
      "epoch:270 , test_loss:0.014392 , train_loss:0.013408\n",
      "epoch:280 , test_loss:0.014380 , train_loss:0.013397\n",
      "epoch:290 , test_loss:0.014365 , train_loss:0.013386\n",
      "epoch:300 , test_loss:0.014349 , train_loss:0.013374\n"
     ]
    }
   ],
   "source": [
    "#硬参数共享神经网络(不复制节点)\n",
    "class MTLnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MTLnet, self).__init__()\n",
    "\n",
    "        self.sharedlayer = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4,3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.y1 = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.y2 = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_shared = self.sharedlayer(x)\n",
    "        y1 = self.y1(h_shared)\n",
    "        y2 = self.y2(h_shared)\n",
    "        if len(y1)==1:\n",
    "            return torch.cat([y1, y2])\n",
    "        else:\n",
    "            return torch.cat([y1,y2],1)\n",
    "model = MTLnet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "for epoch in range(300):\n",
    "    for x_batch,y_batch in TensorDataset(x_train,y_train):\n",
    "        y_hat = model.forward(x_batch)\n",
    "        loss_f = nn.MSELoss(reduction='none')\n",
    "        loss = loss_f(y_batch,y_hat).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1)%10==0:\n",
    "        y_pred = model.forward(x_test)\n",
    "        loss = ((y_pred-y_test)**2).sum()/(len(y_pred)*2)\n",
    "        y_pred1 = model.forward(x_train)\n",
    "        loss1 = ((y_pred1-y_train)**2).sum()/(len(y_pred1)*2)\n",
    "        print(\"epoch:%d , test_loss:%f , train_loss:%f\"%(epoch+1,loss,loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net:\n",
    "    def __init__(self,hidden_layer=3):\n",
    "        self.hidden_layer = hidden_layer\n",
    "    \n",
    "    def _sigmoid(self,x):\n",
    "        for i in range(x.shape[0]):\n",
    "            for k in range(x.shape[1]):\n",
    "                x[i,k] = 1/(1+np.exp(-x[i,k]))\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        n = x.shape[1]\n",
    "        w1 = np.random.randn(n,self.hidden_layer)\n",
    "        b1 = np.zeros((self.hidden_layer,1)).T\n",
    "        w2 = np.random.randn(self.hidden_layer,2)\n",
    "        b2 = np.zeros((2,1)).T\n",
    "        y_hat = self._sigmoid((x@w1+b1)@w2+b2)\n",
    "        params = {\"w1\":w1,\n",
    "                    \"w2\":w2,\n",
    "                    \"b1\":b1,\n",
    "                    \"b2\":b2}\n",
    "        return y_hat,params\n",
    "\n",
    "def mse_loss(y,y_hat):\n",
    "    m = y.shape[0]\n",
    "    n = y.shape[1]\n",
    "    return np.sum((y-y_hat)**2)/(m*n)\n",
    "\n",
    "def matr(m1,m2):\n",
    "    return (m1>=0) == (m2>=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "        for i in range(x.shape[0]):\n",
    "            for k in range(x.shape[1]):\n",
    "                x[i,k] = 1/(1+np.exp(-x[i,k]))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到满足条件，迭代循环终止\n"
     ]
    }
   ],
   "source": [
    "model = net()\n",
    "threshold = 3\n",
    "n = x_train.shape[1]\n",
    "cm1 = np.zeros((n,3))\n",
    "lr = 0.001\n",
    "epoches = 1000\n",
    "y_hat,params = model.forward(x_train)\n",
    "w1 = params[\"w1\"]\n",
    "w2 = params[\"w2\"]\n",
    "b1 = params[\"b1\"]\n",
    "b2 = params[\"b2\"]\n",
    "for epoch in range(epoches):\n",
    "    y_hat = sigmoid((x_train@w1+b1)@w2+b2)\n",
    "    m = x_train.shape[0]\n",
    "    loss = mse_loss(y_hat,y_train)\n",
    "    dw2 = (np.vstack((-(y_train[:,0].reshape(m,1)-y_hat[:,0].reshape(m,1)).T@y_hat[:,0].reshape(m,1)@(1-y_hat[:,0].reshape(m,1)).T@(x_train@w1+b1),-(y_train[:,1].reshape(m,1)-y_hat[:,1].reshape(m,1)).T@y_hat[:,1].reshape(m,1)@(1-y_hat[:,1].reshape(m,1)).T@(x_train@w1+b1)))).T#3*2\n",
    "    dw1 = (-((y_train[:,0].reshape(m,1)-y_hat[:,0].reshape(m,1))@y_hat[:,0].reshape(m,1).T@(1-y_hat[:,0].reshape(m,1))@(w2[:,0].reshape(3,1)).T).T@x_train).T+(-((y_train[:,1].reshape(m,1)-y_hat[:,1].reshape(m,1))@y_hat[:,1].reshape(m,1).T@(1-y_hat[:,1].reshape(m,1))@(w2[:,1].reshape(3,1)).T).T@x_train).T\n",
    "    db2 = (np.hstack((np.sum(-(y_train[:,0].reshape(m,1)-y_hat[:,0].reshape(m,1)).T@y_hat[:,0].reshape(m,1)@(1-y_hat[:,0].reshape(m,1)).T),np.sum(-(y_train[:,1].reshape(m,1)-y_hat[:,1].reshape(m,1)).T@y_hat[:,1].reshape(m,1)@(1-y_hat[:,1].reshape(m,1)).T)))).T.reshape(1,2)#1*2\n",
    "    db1 = (-((y_train[:,0].reshape(m,1)-y_hat[:,0].reshape(m,1))@y_hat[:,0].reshape(m,1).T@(1-y_hat[:,0].reshape(m,1))@(w2[:,0].reshape(3,1)).T).T@np.ones((m,1))).T+(-((y_train[:,1].reshape(m,1)-y_hat[:,1].reshape(m,1))@y_hat[:,1].reshape(m,1).T@(1-y_hat[:,1].reshape(m,1))@(w2[:,1].reshape(3,1)).T).T@np.ones((m,1))).T\n",
    "    w1 -= lr*dw1\n",
    "    w2 -= lr*dw2\n",
    "    b1 -= lr*db1\n",
    "    b2 -= lr*db2\n",
    "    if (epoch+1)%100==0:\n",
    "        print(f\"loss:{loss} , epoch:{epoch+1}\")\n",
    "    d1w1 = (-((y_train[:,0].reshape(m,1)-y_hat[:,0].reshape(m,1))@y_hat[:,0].reshape(m,1).T@(1-y_hat[:,0].reshape(m,1))@(w2[:,0].reshape(3,1)).T).T@x_train).T\n",
    "    d2w1 = (-((y_train[:,1].reshape(m,1)-y_hat[:,1].reshape(m,1))@y_hat[:,1].reshape(m,1).T@(1-y_hat[:,1].reshape(m,1))@(w2[:,1].reshape(3,1)).T).T@x_train).T\n",
    "    dm = matr(d1w1,d2w1)\n",
    "    cm2 = cm1.copy()\n",
    "    for i in range(dm.shape[0]):\n",
    "        for k in range(dm.shape[1]):\n",
    "            if dm[i,k]==False:\n",
    "                cm1[i,k] += 1 \n",
    "            if cm1[i,k]!=cm2[i,k]+1:\n",
    "                cm1[i,k]=0\n",
    "    if np.max(cm1) >= threshold:\n",
    "        print(\"达到满足条件，迭代循环终止\")\n",
    "        index = (np.argmax(cm1)+1)%3\n",
    "        if index == 0:\n",
    "            index = 3-1\n",
    "        else:\n",
    "            index -= 1\n",
    "        break\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b89859744d7c350b12eb8b3275b0a21a12a464476cbd0474ddca5e429d7bc1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
