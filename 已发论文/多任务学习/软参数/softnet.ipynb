{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建模型前向传播\n",
    "class soft_net(nn.Module):\n",
    "\n",
    "    def __init__(self,num1,num2,num3,num4):\n",
    "        super(soft_net,self).__init__()\n",
    "        self.net1 = nn.Sequential(nn.Linear(num1,num2),\n",
    "                                    nn.Sigmoid(),\n",
    "                                    nn.Linear(num2,num3),\n",
    "                                    nn.Sigmoid(),\n",
    "                                    nn.Linear(num3,num4),\n",
    "                                    nn.Sigmoid())\n",
    "        \n",
    "        self.net2 = nn.Sequential(nn.Linear(num1,num2),\n",
    "                                    nn.Sigmoid(),\n",
    "                                    nn.Linear(num2,num3),\n",
    "                                    nn.Sigmoid(),\n",
    "                                    nn.Linear(num3,num4),\n",
    "                                    nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        y1 = self.net1(x)\n",
    "        y2 = self.net2(x)\n",
    "\n",
    "        return torch.cat([y1,y2],1)\n",
    "\n",
    "#定义损失函数\n",
    "class loss_funcation():\n",
    "    def __init__(self,model,punish=0.01):\n",
    "        self.punish = punish\n",
    "        self.model = model\n",
    "    \n",
    "    def __call__(self,y_hat,y,index=None):\n",
    "        loss1 = (torch.sum((y-y_hat)**2)/(2*y.shape[0]))\n",
    "        #读取参数\n",
    "        param_list = []\n",
    "        for name,parameter in self.model.named_parameters():\n",
    "            param_list += list(parameter.flatten())  \n",
    "        param = torch.tensor(param_list).reshape(2,-1)\n",
    "        if index is None:\n",
    "            loss2 = ((torch.sum((param[0]-param[1])**2))**0.5)*self.punish\n",
    "        else:\n",
    "            loss2 = ((torch.sum((param[0][index]-param[1][index])**2))**0.5)*self.punish\n",
    "        return loss1+loss2\n",
    "\n",
    "#导出参数序号\n",
    "def param_index(model,rate):\n",
    "    #读取参数\n",
    "    param_list = []\n",
    "    for name,parameter in model.named_parameters():\n",
    "        param_list += list(parameter.flatten())  \n",
    "    param = torch.tensor(param_list).reshape(2,-1)\n",
    "    difference = abs(param[0]-param[1])\n",
    "    index = torch.argsort(difference)[:int(difference.shape[0]*(1-rate))]\n",
    "    return index\n",
    "\n",
    "#归一化\n",
    "def data_one(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        ma = torch.max(data[:,i])\n",
    "        mi = torch.min(data[:,i])\n",
    "        cha = ma-mi\n",
    "        # print(ma,mi,min(data[:,i]))\n",
    "        for k in range(data.shape[0]):\n",
    "            data[k,i] = (data[k,i]-mi)/cha\n",
    "    return data\n",
    "\n",
    "#创建模拟数据\n",
    "def gen_data(w11,w12,w21,w22,m=1000,n=5,require=None,x_type=\"e\"):\n",
    "    if x_type == \"n\":\n",
    "        x = torch.normal(0,1,(m,n))\n",
    "    if x_type == \"e\":\n",
    "        x = torch.tensor(np.random.exponential(10,(m,n)),dtype=torch.float32)\n",
    "    if x_type == \"u\":\n",
    "        x = torch.tensor(np.random.uniform(0,8,(m,n)),dtype=torch.float32)\n",
    "\n",
    "    if require == None:\n",
    "        noise = 0\n",
    "        \n",
    "    else:\n",
    "        noise = torch.normal(0,1,(m,2))\n",
    "    y1 = x@w11@w12\n",
    "    # y1 = np.sin(x)@w11@w12\n",
    "    y2 = x@w21@w22\n",
    "    label = torch.cat([y1,y2],1)+noise\n",
    "    return x,label\n",
    "\n",
    "class gen_data_set(torch.utils.data.Dataset):\n",
    "    #封装dataset\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x,y\n",
    "#训练模型\n",
    "def train_model(model,loader_dict,optim,epoches,loss_f,index=None):\n",
    "    for epoch in range(epoches):\n",
    "        # print(f\"epoch {epoch+1} / {epoches}\")\n",
    "        # print(\"-\"*100)\n",
    "        for phase in [\"train\",\"eval\"]:\n",
    "            epoch_loss = 0\n",
    "            for input,label in loader_dict[phase]:\n",
    "                optim.zero_grad()\n",
    "                with torch.set_grad_enabled(phase==\"train\"):\n",
    "                    output = model(input)\n",
    "                    loss = loss_f(output,label,index)*input.shape[0]\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                    epoch_loss += loss\n",
    "            print(f\"{epoch}  {phase} : {epoch_loss/len(loader_dict[phase].dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\duduu\\Desktop\\deep learning\\尧尧研创\\2014-2022年8月空气污染数据(均值填充).xlsx\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "def data_one(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        ma = torch.max(data[:,i])\n",
    "        mi = torch.min(data[:,i])\n",
    "        cha = ma-mi\n",
    "        for k in range(data.shape[0]):\n",
    "            data[k,i] = (data[k,i]-mi)/cha\n",
    "    return data\n",
    "x = df.loc[:,(\"SO2\",\"CO\",\"NO2\",\"PM10\")].values\n",
    "y = df.loc[:,(\"PM2.5\",\"O3_8h\")].values\n",
    "x = torch.tensor(x,dtype=torch.float32)\n",
    "y = torch.tensor(y,dtype=torch.float32)\n",
    "x = data_one(x)\n",
    "y = data_one(y)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
    "\n",
    "input,label = gen_data(w11,w12,w21,w22,m=m,n=n,require=noise_require)\n",
    "input1 = data_one(input.clone())\n",
    "label1 = data_one(label.clone())\n",
    "train_x,test_x,train_y,test_y = train_test_split(input1,label1,train_size=0.8)\n",
    "train_dataset = gen_data_set(x_train,y_train)\n",
    "eval_dataset = gen_data_set(x_test,y_test)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=min_batch,shuffle=True)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset,batch_size=min_batch,shuffle=False)\n",
    "dataloader_dict = {\"train\":train_dataloader,\n",
    "                    \"eval\":eval_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各类超参数\n",
    "m = 1000#样本量\n",
    "n = 4#特征维度\n",
    "min_batch = 64#小批量\n",
    "w11 = torch.randn(n,3)#第一个任务第一层权重\n",
    "# w12 = torch.randn(3,1)#第一个任务第二层权重\n",
    "w21 = torch.normal(0,1,(n,3))\n",
    "# w22 = torch.randn(3,1)\n",
    "w12 = torch.tensor([[1.0],[2.0],[3.0]])\n",
    "w22 = torch.tensor([[1.15],[2.15],[3.15]])\n",
    "punish = 0.0005#惩罚项\n",
    "lr = 0.005#学习率\n",
    "noise_require = True #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input,label = gen_data(w11,w12,w21,w22,m=m,n=n,require=noise_require)\n",
    "input1 = data_one(input.clone())\n",
    "label1 = data_one(label.clone())\n",
    "train_x,test_x,train_y,test_y = train_test_split(input1,label1,train_size=0.8)\n",
    "train_dataset = gen_data_set(train_x,train_y)\n",
    "eval_dataset = gen_data_set(test_x,test_y)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=min_batch,shuffle=True)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset,batch_size=min_batch,shuffle=False)\n",
    "dataloader_dict = {\"train\":train_dataloader,\n",
    "                    \"eval\":eval_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  train : 0.03363516181707382\n",
      "0  eval : 0.02923865243792534\n",
      "1  train : 0.02927461639046669\n",
      "1  eval : 0.026414060965180397\n",
      "2  train : 0.027056138962507248\n",
      "2  eval : 0.025089265778660774\n",
      "3  train : 0.02614506334066391\n",
      "3  eval : 0.0247429758310318\n",
      "4  train : 0.025846095755696297\n",
      "4  eval : 0.024739230051636696\n",
      "5  train : 0.025839922949671745\n",
      "5  eval : 0.02476973831653595\n",
      "6  train : 0.025876276195049286\n",
      "6  eval : 0.024757249280810356\n",
      "7  train : 0.02585025131702423\n",
      "7  eval : 0.024774976074695587\n",
      "8  train : 0.02586258202791214\n",
      "8  eval : 0.0247653741389513\n",
      "9  train : 0.02584569901227951\n",
      "9  eval : 0.02471121773123741\n",
      "10  train : 0.025896960869431496\n",
      "10  eval : 0.0247981995344162\n",
      "11  train : 0.025843942537903786\n",
      "11  eval : 0.02472580410540104\n",
      "12  train : 0.025858068838715553\n",
      "12  eval : 0.02474614605307579\n",
      "13  train : 0.025850262492895126\n",
      "13  eval : 0.0247260220348835\n",
      "14  train : 0.02588540129363537\n",
      "14  eval : 0.02475167065858841\n",
      "15  train : 0.02585773542523384\n",
      "15  eval : 0.02473161369562149\n",
      "16  train : 0.025858929380774498\n",
      "16  eval : 0.024774711579084396\n",
      "17  train : 0.02585158497095108\n",
      "17  eval : 0.024741530418395996\n",
      "18  train : 0.025852197781205177\n",
      "18  eval : 0.024735450744628906\n",
      "19  train : 0.02587883546948433\n",
      "19  eval : 0.024778742343187332\n",
      "20  train : 0.025864355266094208\n",
      "20  eval : 0.024734610691666603\n",
      "21  train : 0.025867179036140442\n",
      "21  eval : 0.02477094903588295\n",
      "22  train : 0.02588766999542713\n",
      "22  eval : 0.024770233780145645\n",
      "23  train : 0.025867242366075516\n",
      "23  eval : 0.024760544300079346\n",
      "24  train : 0.025873880833387375\n",
      "24  eval : 0.02473612129688263\n",
      "25  train : 0.025890951976180077\n",
      "25  eval : 0.024807479232549667\n",
      "26  train : 0.025874128565192223\n",
      "26  eval : 0.02475026622414589\n",
      "27  train : 0.025879811495542526\n",
      "27  eval : 0.024792712181806564\n",
      "28  train : 0.025884617120027542\n",
      "28  eval : 0.024776186794042587\n",
      "29  train : 0.02588936872780323\n",
      "29  eval : 0.02479998581111431\n",
      "30  train : 0.025928344577550888\n",
      "30  eval : 0.024740587919950485\n",
      "31  train : 0.02589988522231579\n",
      "31  eval : 0.0248345248401165\n",
      "32  train : 0.025907276198267937\n",
      "32  eval : 0.024772727862000465\n",
      "33  train : 0.025900593027472496\n",
      "33  eval : 0.024784622713923454\n",
      "34  train : 0.02589978277683258\n",
      "34  eval : 0.024766884744167328\n",
      "35  train : 0.0259280726313591\n",
      "35  eval : 0.0248103030025959\n",
      "36  train : 0.025912506505846977\n",
      "36  eval : 0.024760842323303223\n",
      "37  train : 0.025909457355737686\n",
      "37  eval : 0.024792185053229332\n",
      "38  train : 0.025910064578056335\n",
      "38  eval : 0.02478937618434429\n",
      "39  train : 0.02590259537100792\n",
      "39  eval : 0.024810384958982468\n",
      "40  train : 0.025910964235663414\n",
      "40  eval : 0.024790408089756966\n",
      "41  train : 0.025912199169397354\n",
      "41  eval : 0.024793941527605057\n",
      "42  train : 0.02591489814221859\n",
      "42  eval : 0.0248083658516407\n",
      "43  train : 0.02594532072544098\n",
      "43  eval : 0.024848349392414093\n",
      "44  train : 0.02592795342206955\n",
      "44  eval : 0.02481638826429844\n",
      "45  train : 0.025932447984814644\n",
      "45  eval : 0.02477864734828472\n",
      "46  train : 0.025927945971488953\n",
      "46  eval : 0.024804867804050446\n",
      "47  train : 0.02593115344643593\n",
      "47  eval : 0.024859962984919548\n",
      "48  train : 0.025973191484808922\n",
      "48  eval : 0.024782920256257057\n",
      "49  train : 0.025916369631886482\n",
      "49  eval : 0.024845054373145103\n",
      "50  train : 0.02592558227479458\n",
      "50  eval : 0.02485697716474533\n",
      "51  train : 0.025938326492905617\n",
      "51  eval : 0.024814998731017113\n",
      "52  train : 0.026022134348750114\n",
      "52  eval : 0.024801187217235565\n",
      "53  train : 0.025918230414390564\n",
      "53  eval : 0.024870920926332474\n",
      "54  train : 0.025949981063604355\n",
      "54  eval : 0.02484820783138275\n",
      "55  train : 0.02593834139406681\n",
      "55  eval : 0.024845413863658905\n",
      "56  train : 0.025946693494915962\n",
      "56  eval : 0.024846810847520828\n",
      "57  train : 0.025944506749510765\n",
      "57  eval : 0.02483500726521015\n",
      "58  train : 0.025951214134693146\n",
      "58  eval : 0.024853041395545006\n",
      "59  train : 0.02594682201743126\n",
      "59  eval : 0.024839425459504128\n",
      "60  train : 0.025971703231334686\n",
      "60  eval : 0.024846170097589493\n",
      "61  train : 0.025962648913264275\n",
      "61  eval : 0.02484583854675293\n",
      "62  train : 0.02595418132841587\n",
      "62  eval : 0.024830015376210213\n",
      "63  train : 0.025969812646508217\n",
      "63  eval : 0.024865049868822098\n",
      "64  train : 0.02598731964826584\n",
      "64  eval : 0.0248317401856184\n",
      "65  train : 0.025957979261875153\n",
      "65  eval : 0.0248862411826849\n",
      "66  train : 0.0259846281260252\n",
      "66  eval : 0.024869946762919426\n",
      "67  train : 0.025985658168792725\n",
      "67  eval : 0.02488759532570839\n",
      "68  train : 0.025977730751037598\n",
      "68  eval : 0.024836812168359756\n",
      "69  train : 0.026004932820796967\n",
      "69  eval : 0.02487870492041111\n",
      "70  train : 0.02598351426422596\n",
      "70  eval : 0.02484874054789543\n",
      "71  train : 0.026009060442447662\n",
      "71  eval : 0.024837365373969078\n",
      "72  train : 0.025985414162278175\n",
      "72  eval : 0.02487216889858246\n",
      "73  train : 0.026005815714597702\n",
      "73  eval : 0.024937760084867477\n",
      "74  train : 0.025999365374445915\n",
      "74  eval : 0.0248855073004961\n",
      "75  train : 0.025997910648584366\n",
      "75  eval : 0.024868950247764587\n",
      "76  train : 0.025990458205342293\n",
      "76  eval : 0.024893512949347496\n",
      "77  train : 0.02599906735122204\n",
      "77  eval : 0.024900928139686584\n",
      "78  train : 0.02601950615644455\n",
      "78  eval : 0.024859225377440453\n",
      "79  train : 0.025999706238508224\n",
      "79  eval : 0.024905119091272354\n",
      "80  train : 0.026004726067185402\n",
      "80  eval : 0.02492576651275158\n",
      "81  train : 0.02600795030593872\n",
      "81  eval : 0.024872291833162308\n",
      "82  train : 0.02600817196071148\n",
      "82  eval : 0.02488461509346962\n",
      "83  train : 0.02601471357047558\n",
      "83  eval : 0.02490183152258396\n",
      "84  train : 0.02605540305376053\n",
      "84  eval : 0.024919653311371803\n",
      "85  train : 0.026030421257019043\n",
      "85  eval : 0.024899153038859367\n",
      "86  train : 0.026026975363492966\n",
      "86  eval : 0.024901097640395164\n",
      "87  train : 0.026021406054496765\n",
      "87  eval : 0.024898547679185867\n",
      "88  train : 0.026026180014014244\n",
      "88  eval : 0.024904213845729828\n",
      "89  train : 0.026037992909550667\n",
      "89  eval : 0.02494891732931137\n",
      "90  train : 0.026039745658636093\n",
      "90  eval : 0.02495087869465351\n",
      "91  train : 0.026043763384222984\n",
      "91  eval : 0.024931687861680984\n",
      "92  train : 0.026045475155115128\n",
      "92  eval : 0.02491309866309166\n",
      "93  train : 0.026050303131341934\n",
      "93  eval : 0.024935808032751083\n",
      "94  train : 0.026041440665721893\n",
      "94  eval : 0.02495752088725567\n",
      "95  train : 0.02604008838534355\n",
      "95  eval : 0.024945832788944244\n",
      "96  train : 0.026044625788927078\n",
      "96  eval : 0.024927830323576927\n",
      "97  train : 0.02606312744319439\n",
      "97  eval : 0.02494749240577221\n",
      "98  train : 0.0260598324239254\n",
      "98  eval : 0.024931317195296288\n",
      "99  train : 0.026054175570607185\n",
      "99  eval : 0.024936556816101074\n",
      "100  train : 0.026073027402162552\n",
      "100  eval : 0.02493673376739025\n",
      "101  train : 0.026075057685375214\n",
      "101  eval : 0.024999387562274933\n",
      "102  train : 0.02607337199151516\n",
      "102  eval : 0.024925941601395607\n",
      "103  train : 0.026072917506098747\n",
      "103  eval : 0.024970276281237602\n",
      "104  train : 0.026065731421113014\n",
      "104  eval : 0.024947190657258034\n",
      "105  train : 0.02608402445912361\n",
      "105  eval : 0.024973582476377487\n",
      "106  train : 0.026077192276716232\n",
      "106  eval : 0.02494262531399727\n",
      "107  train : 0.026094680652022362\n",
      "107  eval : 0.025012090802192688\n",
      "108  train : 0.026104243472218513\n",
      "108  eval : 0.024966774508357048\n",
      "109  train : 0.026108114048838615\n",
      "109  eval : 0.025000672787427902\n",
      "110  train : 0.02610391192138195\n",
      "110  eval : 0.024950580671429634\n",
      "111  train : 0.026104705408215523\n",
      "111  eval : 0.024979759007692337\n",
      "112  train : 0.026097211986780167\n",
      "112  eval : 0.025012502446770668\n",
      "113  train : 0.026125425472855568\n",
      "113  eval : 0.02496383897960186\n",
      "114  train : 0.02610011026263237\n",
      "114  eval : 0.024985844269394875\n",
      "115  train : 0.026103856042027473\n",
      "115  eval : 0.025022534653544426\n",
      "116  train : 0.026125967502593994\n",
      "116  eval : 0.024981370195746422\n",
      "117  train : 0.026130666956305504\n",
      "117  eval : 0.025028111413121223\n",
      "118  train : 0.0261592585593462\n",
      "118  eval : 0.024991940706968307\n",
      "119  train : 0.02612495608627796\n",
      "119  eval : 0.025027062743902206\n",
      "120  train : 0.026171429082751274\n",
      "120  eval : 0.025079742074012756\n",
      "121  train : 0.02617211826145649\n",
      "121  eval : 0.024992993101477623\n",
      "122  train : 0.026143230497837067\n",
      "122  eval : 0.025038499385118484\n",
      "123  train : 0.026147998869419098\n",
      "123  eval : 0.025073764845728874\n",
      "124  train : 0.026133673265576363\n",
      "124  eval : 0.025025663897395134\n",
      "125  train : 0.026148967444896698\n",
      "125  eval : 0.025017624720931053\n",
      "126  train : 0.02618362382054329\n",
      "126  eval : 0.025060024112462997\n",
      "127  train : 0.02614906057715416\n",
      "127  eval : 0.025032026693224907\n",
      "128  train : 0.026182763278484344\n",
      "128  eval : 0.02503020130097866\n",
      "129  train : 0.026178140193223953\n",
      "129  eval : 0.025050289928913116\n",
      "130  train : 0.02617604285478592\n",
      "130  eval : 0.025114648044109344\n",
      "131  train : 0.02616889961063862\n",
      "131  eval : 0.025067823007702827\n",
      "132  train : 0.026200411841273308\n",
      "132  eval : 0.025034796446561813\n",
      "133  train : 0.026194341480731964\n",
      "133  eval : 0.02508997917175293\n",
      "134  train : 0.026183202862739563\n",
      "134  eval : 0.025076959282159805\n",
      "135  train : 0.026196200400590897\n",
      "135  eval : 0.025068921968340874\n",
      "136  train : 0.026204945519566536\n",
      "136  eval : 0.02510848268866539\n",
      "137  train : 0.02622041665017605\n",
      "137  eval : 0.025125723332166672\n",
      "138  train : 0.026291988790035248\n",
      "138  eval : 0.02505827136337757\n",
      "139  train : 0.026243047788739204\n",
      "139  eval : 0.025155814364552498\n",
      "140  train : 0.026236746460199356\n",
      "140  eval : 0.025089679285883904\n",
      "141  train : 0.026248641312122345\n",
      "141  eval : 0.02513629011809826\n",
      "142  train : 0.02620859071612358\n",
      "142  eval : 0.025118596851825714\n",
      "143  train : 0.02627984806895256\n",
      "143  eval : 0.025119099766016006\n",
      "144  train : 0.02624310553073883\n",
      "144  eval : 0.025121260434389114\n",
      "145  train : 0.026265142485499382\n",
      "145  eval : 0.025149598717689514\n",
      "146  train : 0.026247767731547356\n",
      "146  eval : 0.025132564827799797\n",
      "147  train : 0.026249699294567108\n",
      "147  eval : 0.02512327954173088\n",
      "148  train : 0.026254067197442055\n",
      "148  eval : 0.025169312953948975\n",
      "149  train : 0.02626934088766575\n",
      "149  eval : 0.02511429972946644\n",
      "150  train : 0.02629397064447403\n",
      "150  eval : 0.025153446942567825\n",
      "151  train : 0.026268087327480316\n",
      "151  eval : 0.025134949013590813\n",
      "152  train : 0.02629532851278782\n",
      "152  eval : 0.025162823498249054\n",
      "153  train : 0.026270268484950066\n",
      "153  eval : 0.025157488882541656\n",
      "154  train : 0.026290221139788628\n",
      "154  eval : 0.025147326290607452\n",
      "155  train : 0.026291977614164352\n",
      "155  eval : 0.02516450732946396\n",
      "156  train : 0.026290185749530792\n",
      "156  eval : 0.02518940158188343\n",
      "157  train : 0.026302780956029892\n",
      "157  eval : 0.025179950520396233\n",
      "158  train : 0.026293335482478142\n",
      "158  eval : 0.02519552782177925\n",
      "159  train : 0.026303306221961975\n",
      "159  eval : 0.02519635111093521\n",
      "160  train : 0.026323188096284866\n",
      "160  eval : 0.025163503363728523\n",
      "161  train : 0.026337748393416405\n",
      "161  eval : 0.025234369561076164\n",
      "162  train : 0.026322277262806892\n",
      "162  eval : 0.02523452788591385\n",
      "163  train : 0.026348823681473732\n",
      "163  eval : 0.025248553603887558\n",
      "164  train : 0.026338182389736176\n",
      "164  eval : 0.025202209129929543\n",
      "165  train : 0.026339704170823097\n",
      "165  eval : 0.025213027372956276\n",
      "166  train : 0.026403464376926422\n",
      "166  eval : 0.02520877681672573\n",
      "167  train : 0.0263335183262825\n",
      "167  eval : 0.025285782292485237\n",
      "168  train : 0.026362000033259392\n",
      "168  eval : 0.025245359167456627\n",
      "169  train : 0.026386432349681854\n",
      "169  eval : 0.025228824466466904\n",
      "170  train : 0.026385799050331116\n",
      "170  eval : 0.02523251250386238\n",
      "171  train : 0.02636880613863468\n",
      "171  eval : 0.02530144713819027\n",
      "172  train : 0.026401357725262642\n",
      "172  eval : 0.025260088965296745\n",
      "173  train : 0.026385324075818062\n",
      "173  eval : 0.025265265256166458\n",
      "174  train : 0.026391441002488136\n",
      "174  eval : 0.02527909353375435\n",
      "175  train : 0.026407117024064064\n",
      "175  eval : 0.025274883955717087\n",
      "176  train : 0.026446877047419548\n",
      "176  eval : 0.025269081816077232\n",
      "177  train : 0.02641315385699272\n",
      "177  eval : 0.025347882881760597\n",
      "178  train : 0.026455247774720192\n",
      "178  eval : 0.025295816361904144\n",
      "179  train : 0.026451475918293\n",
      "179  eval : 0.025303762406110764\n",
      "180  train : 0.026449043303728104\n",
      "180  eval : 0.025310594588518143\n",
      "181  train : 0.02642727456986904\n",
      "181  eval : 0.02533438242971897\n",
      "182  train : 0.026441389694809914\n",
      "182  eval : 0.025290166959166527\n",
      "183  train : 0.026446273550391197\n",
      "183  eval : 0.025309693068265915\n",
      "184  train : 0.026456821709871292\n",
      "184  eval : 0.025355055928230286\n",
      "185  train : 0.026462800800800323\n",
      "185  eval : 0.025314144790172577\n",
      "186  train : 0.026457862928509712\n",
      "186  eval : 0.025346701964735985\n",
      "187  train : 0.026459284126758575\n",
      "187  eval : 0.025366904214024544\n",
      "188  train : 0.026478447020053864\n",
      "188  eval : 0.02533733658492565\n",
      "189  train : 0.026468824595212936\n",
      "189  eval : 0.025363871827721596\n",
      "190  train : 0.026526035740971565\n",
      "190  eval : 0.02540997415781021\n",
      "191  train : 0.026552744209766388\n",
      "191  eval : 0.025329144671559334\n",
      "192  train : 0.026487795636057854\n",
      "192  eval : 0.025423919782042503\n",
      "193  train : 0.02653995342552662\n",
      "193  eval : 0.025357788428664207\n",
      "194  train : 0.02652205526828766\n",
      "194  eval : 0.025418918579816818\n",
      "195  train : 0.026537645608186722\n",
      "195  eval : 0.025374270975589752\n",
      "196  train : 0.026521027088165283\n",
      "196  eval : 0.025399500504136086\n",
      "197  train : 0.026518702507019043\n",
      "197  eval : 0.02541305497288704\n",
      "198  train : 0.0265611931681633\n",
      "198  eval : 0.025437304750084877\n",
      "199  train : 0.026548944413661957\n",
      "199  eval : 0.02541494183242321\n",
      "200  train : 0.026583049446344376\n",
      "200  eval : 0.025418655946850777\n",
      "201  train : 0.026534222066402435\n",
      "201  eval : 0.02545863389968872\n",
      "202  train : 0.0265524722635746\n",
      "202  eval : 0.025440309196710587\n",
      "203  train : 0.026554062962532043\n",
      "203  eval : 0.025432594120502472\n",
      "204  train : 0.02658320963382721\n",
      "204  eval : 0.025436537340283394\n",
      "205  train : 0.02657186985015869\n",
      "205  eval : 0.025458337739109993\n",
      "206  train : 0.02657741867005825\n",
      "206  eval : 0.025513717904686928\n",
      "207  train : 0.026580335572361946\n",
      "207  eval : 0.025467656552791595\n",
      "208  train : 0.026636822149157524\n",
      "208  eval : 0.02548283338546753\n",
      "209  train : 0.02662631683051586\n",
      "209  eval : 0.02549225091934204\n"
     ]
    }
   ],
   "source": [
    "#软参数\n",
    "model = soft_net(n,4,3,1)\n",
    "optim = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_f = loss_funcation(model,punish)\n",
    "train_model(model,dataloader_dict,optim,210,loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  train : 0.03721237927675247\n",
      "0  eval : 0.03058800660073757\n",
      "1  train : 0.02920999564230442\n",
      "1  eval : 0.025306904688477516\n",
      "2  train : 0.02538616955280304\n",
      "2  eval : 0.02346804179251194\n",
      "3  train : 0.024313537403941154\n",
      "3  eval : 0.023175787180662155\n",
      "4  train : 0.024239614605903625\n",
      "4  eval : 0.023217806592583656\n",
      "5  train : 0.02426314912736416\n",
      "5  eval : 0.02316111885011196\n",
      "6  train : 0.024244174361228943\n",
      "6  eval : 0.023143885657191277\n",
      "7  train : 0.024236038327217102\n",
      "7  eval : 0.023142872378230095\n",
      "8  train : 0.02424433082342148\n",
      "8  eval : 0.023138392716646194\n",
      "9  train : 0.024238890036940575\n",
      "9  eval : 0.023130517452955246\n",
      "10  train : 0.024245744571089745\n",
      "10  eval : 0.023165525868535042\n",
      "11  train : 0.024238677695393562\n",
      "11  eval : 0.02313639596104622\n",
      "12  train : 0.024238277226686478\n",
      "12  eval : 0.02313639409840107\n",
      "13  train : 0.02423916570842266\n",
      "13  eval : 0.023153994232416153\n",
      "14  train : 0.024237580597400665\n",
      "14  eval : 0.023138584569096565\n",
      "15  train : 0.024243220686912537\n",
      "15  eval : 0.02313903532922268\n",
      "16  train : 0.0242460947483778\n",
      "16  eval : 0.02317303605377674\n",
      "17  train : 0.02423996664583683\n",
      "17  eval : 0.023133892565965652\n",
      "18  train : 0.02424870803952217\n",
      "18  eval : 0.023141171783208847\n",
      "19  train : 0.024252429604530334\n",
      "19  eval : 0.02312159724533558\n",
      "20  train : 0.02423287183046341\n",
      "20  eval : 0.023163095116615295\n",
      "21  train : 0.02424187958240509\n",
      "21  eval : 0.023166391998529434\n",
      "22  train : 0.02424025349318981\n",
      "22  eval : 0.02312549576163292\n",
      "23  train : 0.02424509823322296\n",
      "23  eval : 0.023149453103542328\n",
      "24  train : 0.024251963943243027\n",
      "24  eval : 0.02314910851418972\n",
      "25  train : 0.024235038086771965\n",
      "25  eval : 0.023138321936130524\n",
      "26  train : 0.0242376159876585\n",
      "26  eval : 0.02312765084207058\n",
      "27  train : 0.024247094988822937\n",
      "27  eval : 0.02315809763967991\n",
      "28  train : 0.02423914149403572\n",
      "28  eval : 0.023166071623563766\n",
      "29  train : 0.024230631068348885\n",
      "29  eval : 0.023129036650061607\n",
      "30  train : 0.02424141950905323\n",
      "30  eval : 0.0231421310454607\n",
      "31  train : 0.02424716390669346\n",
      "31  eval : 0.02311358042061329\n",
      "32  train : 0.02423488162457943\n",
      "32  eval : 0.023126762360334396\n",
      "33  train : 0.024235421791672707\n",
      "33  eval : 0.023175019770860672\n",
      "34  train : 0.024268867447972298\n",
      "34  eval : 0.02313276380300522\n",
      "35  train : 0.024235103279352188\n",
      "35  eval : 0.023157816380262375\n",
      "36  train : 0.024245787411928177\n",
      "36  eval : 0.023148752748966217\n",
      "37  train : 0.0242532417178154\n",
      "37  eval : 0.0231699887663126\n",
      "38  train : 0.024233128875494003\n",
      "38  eval : 0.02314016968011856\n",
      "39  train : 0.024234900251030922\n",
      "39  eval : 0.02314763516187668\n",
      "40  train : 0.024264324456453323\n",
      "40  eval : 0.023143531754612923\n",
      "41  train : 0.024238307029008865\n",
      "41  eval : 0.02312607504427433\n",
      "42  train : 0.024237103760242462\n",
      "42  eval : 0.02315407805144787\n",
      "43  train : 0.024271268397569656\n",
      "43  eval : 0.023111891001462936\n",
      "44  train : 0.024235863238573074\n",
      "44  eval : 0.023181406781077385\n",
      "45  train : 0.02427670545876026\n",
      "45  eval : 0.02312438003718853\n",
      "46  train : 0.02423485554754734\n",
      "46  eval : 0.023158149793744087\n",
      "47  train : 0.024251332506537437\n",
      "47  eval : 0.0231617521494627\n",
      "48  train : 0.024233169853687286\n",
      "48  eval : 0.023138370364904404\n",
      "49  train : 0.024243082851171494\n",
      "49  eval : 0.02312573604285717\n",
      "50  train : 0.0242433100938797\n",
      "50  eval : 0.02315538190305233\n",
      "51  train : 0.024238305166363716\n",
      "51  eval : 0.023139402270317078\n",
      "52  train : 0.024237537756562233\n",
      "52  eval : 0.023134902119636536\n",
      "53  train : 0.024235257878899574\n",
      "53  eval : 0.023131437599658966\n",
      "54  train : 0.024240244179964066\n",
      "54  eval : 0.023114483803510666\n",
      "55  train : 0.02424263022840023\n",
      "55  eval : 0.023169707506895065\n",
      "56  train : 0.024237852543592453\n",
      "56  eval : 0.023121822625398636\n",
      "57  train : 0.024235451593995094\n",
      "57  eval : 0.023138726130127907\n",
      "58  train : 0.024246610701084137\n",
      "58  eval : 0.023121215403079987\n",
      "59  train : 0.02423340268433094\n",
      "59  eval : 0.02318163588643074\n",
      "60  train : 0.024260837584733963\n",
      "60  eval : 0.023159224539995193\n",
      "61  train : 0.024235650897026062\n",
      "61  eval : 0.023121723905205727\n",
      "62  train : 0.0242372564971447\n",
      "62  eval : 0.023134566843509674\n",
      "63  train : 0.024262530729174614\n",
      "63  eval : 0.02311566285789013\n",
      "64  train : 0.0242606233805418\n",
      "64  eval : 0.023175276815891266\n",
      "65  train : 0.024265998974442482\n",
      "65  eval : 0.02311643771827221\n",
      "66  train : 0.024235691875219345\n",
      "66  eval : 0.02314506284892559\n",
      "67  train : 0.024232180789113045\n",
      "67  eval : 0.023136354982852936\n",
      "68  train : 0.02424156852066517\n",
      "68  eval : 0.023135429248213768\n",
      "69  train : 0.024236610159277916\n",
      "69  eval : 0.023139119148254395\n",
      "70  train : 0.02423185668885708\n",
      "70  eval : 0.023132367059588432\n",
      "71  train : 0.024252882227301598\n",
      "71  eval : 0.02313772588968277\n",
      "72  train : 0.024245332926511765\n",
      "72  eval : 0.023143839091062546\n",
      "73  train : 0.024231508374214172\n",
      "73  eval : 0.023146476596593857\n",
      "74  train : 0.02422979660332203\n",
      "74  eval : 0.02315683662891388\n",
      "75  train : 0.024257633835077286\n",
      "75  eval : 0.023146091029047966\n",
      "76  train : 0.024223431944847107\n",
      "76  eval : 0.023128483444452286\n",
      "77  train : 0.024230685085058212\n",
      "77  eval : 0.0231198500841856\n",
      "78  train : 0.024224359542131424\n",
      "78  eval : 0.02313859388232231\n",
      "79  train : 0.024292323738336563\n",
      "79  eval : 0.02310018613934517\n",
      "80  train : 0.024281533434987068\n",
      "80  eval : 0.023189198225736618\n",
      "81  train : 0.024238748475909233\n",
      "81  eval : 0.02315252460539341\n",
      "82  train : 0.024231785908341408\n",
      "82  eval : 0.02314109541475773\n",
      "83  train : 0.0242912694811821\n",
      "83  eval : 0.023110762238502502\n",
      "84  train : 0.02423955127596855\n",
      "84  eval : 0.0231610219925642\n",
      "85  train : 0.024236733093857765\n",
      "85  eval : 0.023161746561527252\n",
      "86  train : 0.024225987493991852\n",
      "86  eval : 0.023108884692192078\n",
      "87  train : 0.024232788011431694\n",
      "87  eval : 0.023126134648919106\n",
      "88  train : 0.02422737330198288\n",
      "88  eval : 0.02314479649066925\n",
      "89  train : 0.0242304690182209\n",
      "89  eval : 0.02316438965499401\n",
      "90  train : 0.024237852543592453\n",
      "90  eval : 0.02315349131822586\n",
      "91  train : 0.024230491369962692\n",
      "91  eval : 0.023113582283258438\n",
      "92  train : 0.02423105202615261\n",
      "92  eval : 0.023123307153582573\n",
      "93  train : 0.024226844310760498\n",
      "93  eval : 0.023132093250751495\n",
      "94  train : 0.024239573627710342\n",
      "94  eval : 0.02311733551323414\n",
      "95  train : 0.024249086156487465\n",
      "95  eval : 0.02315978705883026\n",
      "96  train : 0.024227088317275047\n",
      "96  eval : 0.023138005286455154\n",
      "97  train : 0.024232976138591766\n",
      "97  eval : 0.023156926035881042\n",
      "98  train : 0.024231137707829475\n",
      "98  eval : 0.023119401186704636\n",
      "99  train : 0.0242574755102396\n",
      "99  eval : 0.023155538365244865\n",
      "100  train : 0.024229759350419044\n",
      "100  eval : 0.02310352772474289\n",
      "101  train : 0.024240588769316673\n",
      "101  eval : 0.02314775064587593\n",
      "102  train : 0.024226617068052292\n",
      "102  eval : 0.02311362698674202\n",
      "103  train : 0.02423001080751419\n",
      "103  eval : 0.023111233487725258\n",
      "104  train : 0.024238307029008865\n",
      "104  eval : 0.023151466622948647\n",
      "105  train : 0.024247542023658752\n",
      "105  eval : 0.023128338158130646\n",
      "106  train : 0.02422361634671688\n",
      "106  eval : 0.02312091365456581\n",
      "107  train : 0.02422480098903179\n",
      "107  eval : 0.0231479499489069\n",
      "108  train : 0.024225108325481415\n",
      "108  eval : 0.023145323619246483\n",
      "109  train : 0.02423546276986599\n",
      "109  eval : 0.023131830617785454\n",
      "110  train : 0.024221889674663544\n",
      "110  eval : 0.02310873754322529\n",
      "111  train : 0.024226319044828415\n",
      "111  eval : 0.02312272973358631\n",
      "112  train : 0.0242330152541399\n",
      "112  eval : 0.023122422397136688\n",
      "113  train : 0.02423436939716339\n",
      "113  eval : 0.023132843896746635\n",
      "114  train : 0.024228816851973534\n",
      "114  eval : 0.023125778883695602\n",
      "115  train : 0.02424953505396843\n",
      "115  eval : 0.02309909276664257\n",
      "116  train : 0.024217698723077774\n",
      "116  eval : 0.02314036525785923\n",
      "117  train : 0.024245748296380043\n",
      "117  eval : 0.023152964189648628\n",
      "118  train : 0.02422187849879265\n",
      "118  eval : 0.023169852793216705\n",
      "119  train : 0.024239206686615944\n",
      "119  eval : 0.02313203364610672\n",
      "120  train : 0.02422606199979782\n",
      "120  eval : 0.023138830438256264\n",
      "121  train : 0.024237019941210747\n",
      "121  eval : 0.023128150030970573\n",
      "122  train : 0.024226121604442596\n",
      "122  eval : 0.02313718944787979\n",
      "123  train : 0.024246670305728912\n",
      "123  eval : 0.023142216727137566\n",
      "124  train : 0.024226732552051544\n",
      "124  eval : 0.0231220293790102\n",
      "125  train : 0.024232707917690277\n",
      "125  eval : 0.023152459412813187\n",
      "126  train : 0.024250859394669533\n",
      "126  eval : 0.02315448969602585\n",
      "127  train : 0.024224620312452316\n",
      "127  eval : 0.023123322054743767\n",
      "128  train : 0.024221576750278473\n",
      "128  eval : 0.023104656487703323\n",
      "129  train : 0.02423531748354435\n",
      "129  eval : 0.023155968636274338\n",
      "130  train : 0.024218643084168434\n",
      "130  eval : 0.0231256652623415\n",
      "131  train : 0.024219222366809845\n",
      "131  eval : 0.02312568947672844\n",
      "132  train : 0.024258572608232498\n",
      "132  eval : 0.023145608603954315\n",
      "133  train : 0.024232644587755203\n",
      "133  eval : 0.023113034665584564\n",
      "134  train : 0.02422131970524788\n",
      "134  eval : 0.02312028408050537\n",
      "135  train : 0.02422402799129486\n",
      "135  eval : 0.023122739046812057\n",
      "136  train : 0.0242299921810627\n",
      "136  eval : 0.023121822625398636\n",
      "137  train : 0.0242140032351017\n",
      "137  eval : 0.023140424862504005\n",
      "138  train : 0.024224136024713516\n",
      "138  eval : 0.023143453523516655\n",
      "139  train : 0.024222509935498238\n",
      "139  eval : 0.023107709363102913\n",
      "140  train : 0.024232521653175354\n",
      "140  eval : 0.02312680333852768\n",
      "141  train : 0.024223458021879196\n",
      "141  eval : 0.02311611734330654\n",
      "142  train : 0.024219820275902748\n",
      "142  eval : 0.023135753348469734\n",
      "143  train : 0.024222001433372498\n",
      "143  eval : 0.023127064108848572\n",
      "144  train : 0.024231329560279846\n",
      "144  eval : 0.023150265216827393\n",
      "145  train : 0.024226468056440353\n",
      "145  eval : 0.023137738928198814\n",
      "146  train : 0.0242226030677557\n",
      "146  eval : 0.02310767024755478\n",
      "147  train : 0.024222364649176598\n",
      "147  eval : 0.02314574271440506\n",
      "148  train : 0.02422008477151394\n",
      "148  eval : 0.023143161088228226\n",
      "149  train : 0.024219203740358353\n",
      "149  eval : 0.023100700229406357\n",
      "150  train : 0.024217842146754265\n",
      "150  eval : 0.023105135187506676\n",
      "151  train : 0.02422415092587471\n",
      "151  eval : 0.023115942254662514\n",
      "152  train : 0.024227343499660492\n",
      "152  eval : 0.023133939132094383\n",
      "153  train : 0.024215195327997208\n",
      "153  eval : 0.023123640567064285\n",
      "154  train : 0.024229906499385834\n",
      "154  eval : 0.023099131882190704\n",
      "155  train : 0.02421516366302967\n",
      "155  eval : 0.02312139980494976\n",
      "156  train : 0.02423613704741001\n",
      "156  eval : 0.02314210683107376\n",
      "157  train : 0.024218199774622917\n",
      "157  eval : 0.02310235984623432\n",
      "158  train : 0.024242177605628967\n",
      "158  eval : 0.023094339296221733\n",
      "159  train : 0.024229710921645164\n",
      "159  eval : 0.023173954337835312\n",
      "160  train : 0.024219851940870285\n",
      "160  eval : 0.02313241921365261\n",
      "161  train : 0.024226276203989983\n",
      "161  eval : 0.023111678659915924\n",
      "162  train : 0.02422897331416607\n",
      "162  eval : 0.023143233731389046\n",
      "163  train : 0.024214666336774826\n",
      "163  eval : 0.023111701011657715\n",
      "164  train : 0.024247342720627785\n",
      "164  eval : 0.02310914732515812\n",
      "165  train : 0.02420576848089695\n",
      "165  eval : 0.023136848583817482\n",
      "166  train : 0.024253562092781067\n",
      "166  eval : 0.023160312324762344\n",
      "167  train : 0.02423238754272461\n",
      "167  eval : 0.023077571764588356\n",
      "168  train : 0.024220915511250496\n",
      "168  eval : 0.023102471604943275\n",
      "169  train : 0.024227911606431007\n",
      "169  eval : 0.023151982575654984\n",
      "170  train : 0.024243662133812904\n",
      "170  eval : 0.023165803402662277\n",
      "171  train : 0.024213043972849846\n",
      "171  eval : 0.023091603070497513\n",
      "172  train : 0.024220986291766167\n",
      "172  eval : 0.02310939133167267\n",
      "173  train : 0.024254750460386276\n",
      "173  eval : 0.023167328909039497\n",
      "174  train : 0.02421092428267002\n",
      "174  eval : 0.02309284918010235\n",
      "175  train : 0.02422005496919155\n",
      "175  eval : 0.023109955713152885\n",
      "176  train : 0.02420954219996929\n",
      "176  eval : 0.02313540130853653\n",
      "177  train : 0.024220673367381096\n",
      "177  eval : 0.023116223514080048\n",
      "178  train : 0.024231288582086563\n",
      "178  eval : 0.023125827312469482\n",
      "179  train : 0.024233002215623856\n",
      "179  eval : 0.023114236071705818\n",
      "180  train : 0.024249086156487465\n",
      "180  eval : 0.023173706606030464\n",
      "181  train : 0.024216141551733017\n",
      "181  eval : 0.023096153512597084\n",
      "182  train : 0.02421838790178299\n",
      "182  eval : 0.023121511563658714\n",
      "183  train : 0.024211667478084564\n",
      "183  eval : 0.02312486805021763\n",
      "184  train : 0.024217557162046432\n",
      "184  eval : 0.02309846319258213\n",
      "185  train : 0.024208538234233856\n",
      "185  eval : 0.023122377693653107\n",
      "186  train : 0.024216609075665474\n",
      "186  eval : 0.02313687838613987\n",
      "187  train : 0.024205785244703293\n",
      "187  eval : 0.023117272183299065\n",
      "188  train : 0.024221502244472504\n",
      "188  eval : 0.023121315985918045\n",
      "189  train : 0.024220699444413185\n",
      "189  eval : 0.023100707679986954\n",
      "190  train : 0.024212809279561043\n",
      "190  eval : 0.023141484707593918\n",
      "191  train : 0.02423395961523056\n",
      "191  eval : 0.023120170459151268\n",
      "192  train : 0.02421235479414463\n",
      "192  eval : 0.02315855212509632\n",
      "193  train : 0.024218693375587463\n",
      "193  eval : 0.023109517991542816\n",
      "194  train : 0.02420823648571968\n",
      "194  eval : 0.023127034306526184\n",
      "195  train : 0.024228932335972786\n",
      "195  eval : 0.023157840594649315\n",
      "196  train : 0.02421201951801777\n",
      "196  eval : 0.02308660000562668\n",
      "197  train : 0.024217598140239716\n",
      "197  eval : 0.02311389520764351\n",
      "198  train : 0.024221209809184074\n",
      "198  eval : 0.02309168316423893\n",
      "199  train : 0.02420303411781788\n",
      "199  eval : 0.02313775010406971\n",
      "200  train : 0.02422323264181614\n",
      "200  eval : 0.023154621943831444\n",
      "201  train : 0.02426617406308651\n",
      "201  eval : 0.023091401904821396\n",
      "202  train : 0.024200396612286568\n",
      "202  eval : 0.023146167397499084\n",
      "203  train : 0.024251634255051613\n",
      "203  eval : 0.023116983473300934\n",
      "204  train : 0.0242657158523798\n",
      "204  eval : 0.023077640682458878\n",
      "205  train : 0.02423497848212719\n",
      "205  eval : 0.023196188732981682\n",
      "206  train : 0.024325208738446236\n",
      "206  eval : 0.02307993173599243\n",
      "207  train : 0.024243714287877083\n",
      "207  eval : 0.023196332156658173\n",
      "208  train : 0.02421438694000244\n",
      "208  eval : 0.023142583668231964\n",
      "209  train : 0.024204585701227188\n",
      "209  eval : 0.02311754785478115\n"
     ]
    }
   ],
   "source": [
    "#不加惩罚项\n",
    "model = soft_net(n,4,3,1)\n",
    "optim = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_f = loss_funcation(model,punish=0)\n",
    "train_model(model,dataloader_dict,optim,210,loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  train : 0.02547510713338852\n",
      "0  eval : 0.024396026507019997\n",
      "0  train : 0.02512911707162857\n",
      "0  eval : 0.024001827463507652\n",
      "0  train : 0.024912415072321892\n",
      "0  eval : 0.023794636130332947\n",
      "0  train : 0.02476642094552517\n",
      "0  eval : 0.02361229807138443\n",
      "0  train : 0.02466581016778946\n",
      "0  eval : 0.02357923798263073\n",
      "0  train : 0.024541519582271576\n",
      "0  eval : 0.02340228483080864\n",
      "0  train : 0.024481646716594696\n",
      "0  eval : 0.023395007476210594\n",
      "0  train : 0.024365447461605072\n",
      "0  eval : 0.023264586925506592\n",
      "0  train : 0.024285711348056793\n",
      "0  eval : 0.023152507841587067\n",
      "0  train : 0.024272069334983826\n",
      "0  eval : 0.023187315091490746\n",
      "0  train : 0.02424723654985428\n",
      "0  eval : 0.02313636615872383\n"
     ]
    }
   ],
   "source": [
    "#自适应参数惩罚项\n",
    "model = soft_net(n,4,3,1)\n",
    "optim = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_f = loss_funcation(model,punish)\n",
    "train_model(model,dataloader_dict,optim,1,loss_f)\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    index = param_index(model,i)\n",
    "    loss_f = loss_funcation(model,punish)\n",
    "    train_model(model,dataloader_dict,optim,1,loss_f,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = soft_net(n,4,3,1)\n",
    "optim = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_f = loss_funcation(model,punish=punish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  train : 0.0406477153301239\n",
      "0  eval : 0.03538883104920387\n",
      "1  train : 0.03131786361336708\n",
      "1  eval : 0.027192501351237297\n",
      "2  train : 0.02622252143919468\n",
      "2  eval : 0.0227962676435709\n",
      "3  train : 0.023968806490302086\n",
      "3  eval : 0.020996563136577606\n",
      "4  train : 0.02342749759554863\n",
      "4  eval : 0.02031157724559307\n",
      "5  train : 0.023259872570633888\n",
      "5  eval : 0.020106781274080276\n",
      "6  train : 0.023235242813825607\n",
      "6  eval : 0.020126914605498314\n",
      "7  train : 0.02323061227798462\n",
      "7  eval : 0.020112695172429085\n",
      "8  train : 0.023234773427248\n",
      "8  eval : 0.020079219713807106\n",
      "9  train : 0.023223914206027985\n",
      "9  eval : 0.020111367106437683\n",
      "10  train : 0.02322867140173912\n",
      "10  eval : 0.02015507221221924\n",
      "11  train : 0.023236894980072975\n",
      "11  eval : 0.02012210339307785\n",
      "12  train : 0.02322879247367382\n",
      "12  eval : 0.020094886422157288\n",
      "13  train : 0.023239493370056152\n",
      "13  eval : 0.020174257457256317\n",
      "14  train : 0.02323741838335991\n",
      "14  eval : 0.02012120559811592\n",
      "15  train : 0.023236529901623726\n",
      "15  eval : 0.020104825496673584\n",
      "16  train : 0.023225417360663414\n",
      "16  eval : 0.020146751776337624\n",
      "17  train : 0.023229874670505524\n",
      "17  eval : 0.020133063197135925\n",
      "18  train : 0.023232534527778625\n",
      "18  eval : 0.020177442580461502\n",
      "19  train : 0.023252462968230247\n",
      "19  eval : 0.020082756876945496\n",
      "20  train : 0.0232298094779253\n",
      "20  eval : 0.020157964900135994\n",
      "21  train : 0.023233499377965927\n",
      "21  eval : 0.020167220383882523\n",
      "22  train : 0.023220999166369438\n",
      "22  eval : 0.02013568952679634\n",
      "23  train : 0.023237766698002815\n",
      "23  eval : 0.020100779831409454\n",
      "24  train : 0.0232250839471817\n",
      "24  eval : 0.020086441189050674\n",
      "25  train : 0.023228485137224197\n",
      "25  eval : 0.020164089277386665\n",
      "26  train : 0.023225313052535057\n",
      "26  eval : 0.02016012743115425\n",
      "27  train : 0.02323194593191147\n",
      "27  eval : 0.020095689222216606\n",
      "28  train : 0.02324104867875576\n",
      "28  eval : 0.02013731189072132\n",
      "29  train : 0.023231592029333115\n",
      "29  eval : 0.02016715705394745\n",
      "30  train : 0.023230629041790962\n",
      "30  eval : 0.02009192481637001\n",
      "31  train : 0.023227566853165627\n",
      "31  eval : 0.02005857415497303\n",
      "32  train : 0.023228293284773827\n",
      "32  eval : 0.020124919712543488\n",
      "33  train : 0.023240137845277786\n",
      "33  eval : 0.020176399499177933\n",
      "34  train : 0.023230433464050293\n",
      "34  eval : 0.020129486918449402\n",
      "35  train : 0.023234253749251366\n",
      "35  eval : 0.02016347087919712\n",
      "36  train : 0.023235000669956207\n",
      "36  eval : 0.020155107602477074\n",
      "37  train : 0.023238761350512505\n",
      "37  eval : 0.020107397809624672\n",
      "38  train : 0.023251907899975777\n",
      "38  eval : 0.020151745527982712\n",
      "39  train : 0.023255929350852966\n",
      "39  eval : 0.02013816311955452\n",
      "40  train : 0.02332109771668911\n",
      "40  eval : 0.020004982128739357\n",
      "41  train : 0.0232271496206522\n",
      "41  eval : 0.02023462764918804\n",
      "42  train : 0.023240918293595314\n",
      "42  eval : 0.020213305950164795\n",
      "43  train : 0.02323843166232109\n",
      "43  eval : 0.020086193457245827\n",
      "44  train : 0.023249538615345955\n",
      "44  eval : 0.02010486274957657\n",
      "45  train : 0.023243818432092667\n",
      "45  eval : 0.020080821588635445\n",
      "46  train : 0.02323434129357338\n",
      "46  eval : 0.02021377719938755\n",
      "47  train : 0.02323102205991745\n",
      "47  eval : 0.020153017714619637\n",
      "48  train : 0.023286903277039528\n",
      "48  eval : 0.02006802335381508\n",
      "49  train : 0.023258594796061516\n",
      "49  eval : 0.02019362710416317\n",
      "50  train : 0.023226765915751457\n",
      "50  eval : 0.02016167901456356\n",
      "51  train : 0.023257765918970108\n",
      "51  eval : 0.020193150267004967\n",
      "52  train : 0.023232802748680115\n",
      "52  eval : 0.020082565024495125\n",
      "53  train : 0.023269126191735268\n",
      "53  eval : 0.020052824169397354\n",
      "54  train : 0.023265518248081207\n",
      "54  eval : 0.020187105983495712\n",
      "55  train : 0.023230236023664474\n",
      "55  eval : 0.020108794793486595\n",
      "56  train : 0.023234762251377106\n",
      "56  eval : 0.02018435299396515\n",
      "57  train : 0.023235468193888664\n",
      "57  eval : 0.020126694813370705\n",
      "58  train : 0.023231294006109238\n",
      "58  eval : 0.020135683938860893\n",
      "59  train : 0.023237578570842743\n",
      "59  eval : 0.020098406821489334\n",
      "60  train : 0.023235999047756195\n",
      "60  eval : 0.020137224346399307\n",
      "61  train : 0.02324535883963108\n",
      "61  eval : 0.02009781450033188\n",
      "62  train : 0.023255150765180588\n",
      "62  eval : 0.02025354839861393\n",
      "63  train : 0.023240722715854645\n",
      "63  eval : 0.020103221759200096\n",
      "64  train : 0.023242346942424774\n",
      "64  eval : 0.020113220438361168\n",
      "65  train : 0.023232968524098396\n",
      "65  eval : 0.020138904452323914\n",
      "66  train : 0.023239558562636375\n",
      "66  eval : 0.020129388198256493\n",
      "67  train : 0.02324231155216694\n",
      "67  eval : 0.020130448043346405\n",
      "68  train : 0.023277347907423973\n",
      "68  eval : 0.020257771015167236\n",
      "69  train : 0.023236103355884552\n",
      "69  eval : 0.020124640315771103\n",
      "70  train : 0.023236971348524094\n",
      "70  eval : 0.020103856921195984\n",
      "71  train : 0.02324093133211136\n",
      "71  eval : 0.02013017237186432\n",
      "72  train : 0.023274557664990425\n",
      "72  eval : 0.020266879349946976\n",
      "73  train : 0.0232294462621212\n",
      "73  eval : 0.020117484033107758\n",
      "74  train : 0.023237669840455055\n",
      "74  eval : 0.020110366865992546\n",
      "75  train : 0.0232571791857481\n",
      "75  eval : 0.02015724964439869\n",
      "76  train : 0.02323962189257145\n",
      "76  eval : 0.020142139866948128\n",
      "77  train : 0.02323843166232109\n",
      "77  eval : 0.020118093118071556\n",
      "78  train : 0.023249350488185883\n",
      "78  eval : 0.020168518647551537\n",
      "79  train : 0.023263856768608093\n",
      "79  eval : 0.020141584798693657\n",
      "80  train : 0.023241888731718063\n",
      "80  eval : 0.020206797868013382\n",
      "81  train : 0.023249614983797073\n",
      "81  eval : 0.02015232853591442\n",
      "82  train : 0.02324359491467476\n",
      "82  eval : 0.02016482874751091\n",
      "83  train : 0.023249492049217224\n",
      "83  eval : 0.02012266218662262\n",
      "84  train : 0.023243047297000885\n",
      "84  eval : 0.020142052322626114\n",
      "85  train : 0.023252001032233238\n",
      "85  eval : 0.020145095884799957\n",
      "86  train : 0.023256637156009674\n",
      "86  eval : 0.020280232653021812\n",
      "87  train : 0.02326289191842079\n",
      "87  eval : 0.020093344151973724\n",
      "88  train : 0.023252293467521667\n",
      "88  eval : 0.020129572600126266\n",
      "89  train : 0.023236911743879318\n",
      "89  eval : 0.020266935229301453\n",
      "90  train : 0.023282133042812347\n",
      "90  eval : 0.020281337201595306\n",
      "91  train : 0.02325565367937088\n",
      "91  eval : 0.02009613811969757\n",
      "92  train : 0.023252420127391815\n",
      "92  eval : 0.02012738771736622\n",
      "93  train : 0.023246174678206444\n",
      "93  eval : 0.020163994282484055\n",
      "94  train : 0.023262716829776764\n",
      "94  eval : 0.020186977460980415\n",
      "95  train : 0.02324756607413292\n",
      "95  eval : 0.020169474184513092\n",
      "96  train : 0.02324497513473034\n",
      "96  eval : 0.020220644772052765\n",
      "97  train : 0.02325725555419922\n",
      "97  eval : 0.02014932408928871\n",
      "98  train : 0.023258084431290627\n",
      "98  eval : 0.02021377719938755\n",
      "99  train : 0.02327798120677471\n",
      "99  eval : 0.02026747725903988\n",
      "100  train : 0.0232560858130455\n",
      "100  eval : 0.020113041624426842\n",
      "101  train : 0.023253951221704483\n",
      "101  eval : 0.02017888054251671\n",
      "102  train : 0.023270396515727043\n",
      "102  eval : 0.020169302821159363\n",
      "103  train : 0.023303747177124023\n",
      "103  eval : 0.020269589498639107\n",
      "104  train : 0.023279737681150436\n",
      "104  eval : 0.020129695534706116\n",
      "105  train : 0.023254171013832092\n",
      "105  eval : 0.020217468962073326\n",
      "106  train : 0.02327592298388481\n",
      "106  eval : 0.020230934023857117\n",
      "107  train : 0.023253200575709343\n",
      "107  eval : 0.020135153084993362\n",
      "108  train : 0.023253154009580612\n",
      "108  eval : 0.020184284076094627\n",
      "109  train : 0.023281138390302658\n",
      "109  eval : 0.020127123221755028\n",
      "110  train : 0.023294944316148758\n",
      "110  eval : 0.020277775824069977\n",
      "111  train : 0.02329193986952305\n",
      "111  eval : 0.02016373910009861\n",
      "112  train : 0.02329789660871029\n",
      "112  eval : 0.020263027399778366\n",
      "113  train : 0.023263990879058838\n",
      "113  eval : 0.02016926370561123\n",
      "114  train : 0.023266218602657318\n",
      "114  eval : 0.020229682326316833\n",
      "115  train : 0.023257635533809662\n",
      "115  eval : 0.020163146778941154\n",
      "116  train : 0.02325989305973053\n",
      "116  eval : 0.020222149789333344\n",
      "117  train : 0.023275800049304962\n",
      "117  eval : 0.020263470709323883\n",
      "118  train : 0.023254981264472008\n",
      "118  eval : 0.020123735070228577\n",
      "119  train : 0.023263316601514816\n",
      "119  eval : 0.02014743536710739\n",
      "120  train : 0.023253872990608215\n",
      "120  eval : 0.02022341638803482\n",
      "121  train : 0.023263869807124138\n",
      "121  eval : 0.020296843722462654\n",
      "122  train : 0.023265372961759567\n",
      "122  eval : 0.02015969529747963\n",
      "123  train : 0.023270511999726295\n",
      "123  eval : 0.0202405396848917\n",
      "124  train : 0.023265980184078217\n",
      "124  eval : 0.02019750140607357\n",
      "125  train : 0.023291654884815216\n",
      "125  eval : 0.020184380933642387\n",
      "126  train : 0.0232625100761652\n",
      "126  eval : 0.020248206332325935\n",
      "127  train : 0.023269489407539368\n",
      "127  eval : 0.02026638574898243\n",
      "128  train : 0.02325911819934845\n",
      "128  eval : 0.020198965445160866\n",
      "129  train : 0.02327835001051426\n",
      "129  eval : 0.020254969596862793\n",
      "130  train : 0.023258820176124573\n",
      "130  eval : 0.02016294002532959\n",
      "131  train : 0.023264750838279724\n",
      "131  eval : 0.02016379125416279\n",
      "132  train : 0.023307783529162407\n",
      "132  eval : 0.020320696756243706\n",
      "133  train : 0.023290393874049187\n",
      "133  eval : 0.02014845795929432\n",
      "134  train : 0.02327641472220421\n",
      "134  eval : 0.020322903990745544\n",
      "135  train : 0.023277971893548965\n",
      "135  eval : 0.02018534578382969\n",
      "136  train : 0.023271901533007622\n",
      "136  eval : 0.02027267962694168\n",
      "137  train : 0.02327156253159046\n",
      "137  eval : 0.020284578204154968\n",
      "138  train : 0.023287609219551086\n",
      "138  eval : 0.020193548873066902\n",
      "139  train : 0.023292073979973793\n",
      "139  eval : 0.0202452652156353\n",
      "140  train : 0.023279981687664986\n",
      "140  eval : 0.020162127912044525\n",
      "141  train : 0.023301564157009125\n",
      "141  eval : 0.020351428538560867\n",
      "142  train : 0.023267770186066628\n",
      "142  eval : 0.02022530883550644\n",
      "143  train : 0.02327529713511467\n",
      "143  eval : 0.020220361649990082\n",
      "144  train : 0.023262953385710716\n",
      "144  eval : 0.020275942981243134\n",
      "145  train : 0.023272454738616943\n",
      "145  eval : 0.020304176956415176\n",
      "146  train : 0.02328338846564293\n",
      "146  eval : 0.020219188183546066\n",
      "147  train : 0.023274149745702744\n",
      "147  eval : 0.020322980359196663\n",
      "148  train : 0.023279493674635887\n",
      "148  eval : 0.02023940719664097\n",
      "149  train : 0.023297181352972984\n",
      "149  eval : 0.020344793796539307\n",
      "150  train : 0.023277774453163147\n",
      "150  eval : 0.020181551575660706\n",
      "151  train : 0.02328680083155632\n",
      "151  eval : 0.020288100466132164\n",
      "152  train : 0.023282771930098534\n",
      "152  eval : 0.020312247797846794\n",
      "153  train : 0.02328692190349102\n",
      "153  eval : 0.020224595442414284\n",
      "154  train : 0.02328997105360031\n",
      "154  eval : 0.02029401622712612\n",
      "155  train : 0.023294586688280106\n",
      "155  eval : 0.020199408754706383\n",
      "156  train : 0.023291077464818954\n",
      "156  eval : 0.0203914362937212\n",
      "157  train : 0.02328842133283615\n",
      "157  eval : 0.020216645672917366\n",
      "158  train : 0.023305527865886688\n",
      "158  eval : 0.020198721438646317\n",
      "159  train : 0.023272497579455376\n",
      "159  eval : 0.02034018747508526\n",
      "160  train : 0.02331489883363247\n",
      "160  eval : 0.02044067531824112\n",
      "161  train : 0.02328263781964779\n",
      "161  eval : 0.0202851053327322\n",
      "162  train : 0.02341240644454956\n",
      "162  eval : 0.02016492933034897\n",
      "163  train : 0.023280804976820946\n",
      "163  eval : 0.02043309435248375\n",
      "164  train : 0.023291952908039093\n",
      "164  eval : 0.02031339332461357\n",
      "165  train : 0.02327706106007099\n",
      "165  eval : 0.02025168016552925\n",
      "166  train : 0.023326965048909187\n",
      "166  eval : 0.020189888775348663\n",
      "167  train : 0.023329371586441994\n",
      "167  eval : 0.02040376141667366\n",
      "168  train : 0.02330940030515194\n",
      "168  eval : 0.02037440612912178\n",
      "169  train : 0.023303357884287834\n",
      "169  eval : 0.020200539380311966\n",
      "170  train : 0.023298723623156548\n",
      "170  eval : 0.020289670675992966\n",
      "171  train : 0.023306112736463547\n",
      "171  eval : 0.02034866064786911\n",
      "172  train : 0.023283585906028748\n",
      "172  eval : 0.020313365384936333\n",
      "173  train : 0.023307356983423233\n",
      "173  eval : 0.0202516820281744\n",
      "174  train : 0.02330164983868599\n",
      "174  eval : 0.020404145121574402\n",
      "175  train : 0.023310786113142967\n",
      "175  eval : 0.020288784056901932\n",
      "176  train : 0.023316223174333572\n",
      "176  eval : 0.02035653591156006\n",
      "177  train : 0.023298971354961395\n",
      "177  eval : 0.020278817042708397\n",
      "178  train : 0.02330644428730011\n",
      "178  eval : 0.02033545821905136\n",
      "179  train : 0.02329377830028534\n",
      "179  eval : 0.02034747786819935\n",
      "180  train : 0.023313965648412704\n",
      "180  eval : 0.020366869866847992\n",
      "181  train : 0.023301079869270325\n",
      "181  eval : 0.020285429432988167\n",
      "182  train : 0.023317959159612656\n",
      "182  eval : 0.020331865176558495\n",
      "183  train : 0.023298317566514015\n",
      "183  eval : 0.020343098789453506\n",
      "184  train : 0.023301271721720695\n",
      "184  eval : 0.020390043035149574\n",
      "185  train : 0.023296039551496506\n",
      "185  eval : 0.020339813083410263\n",
      "186  train : 0.02329735830426216\n",
      "186  eval : 0.02033204771578312\n",
      "187  train : 0.023324012756347656\n",
      "187  eval : 0.02033246122300625\n",
      "188  train : 0.023293940350413322\n",
      "188  eval : 0.020461268723011017\n",
      "189  train : 0.02330154925584793\n",
      "189  eval : 0.02040688320994377\n",
      "190  train : 0.023305892944335938\n",
      "190  eval : 0.02028491348028183\n",
      "191  train : 0.023317888379096985\n",
      "191  eval : 0.020403973758220673\n",
      "192  train : 0.023324718698859215\n",
      "192  eval : 0.02030971832573414\n",
      "193  train : 0.023300718516111374\n",
      "193  eval : 0.020439142361283302\n",
      "194  train : 0.02329913154244423\n",
      "194  eval : 0.020383629947900772\n",
      "195  train : 0.023324446752667427\n",
      "195  eval : 0.02036110684275627\n",
      "196  train : 0.023306531831622124\n",
      "196  eval : 0.020378880202770233\n",
      "197  train : 0.023319147527217865\n",
      "197  eval : 0.02034638822078705\n",
      "198  train : 0.023316534236073494\n",
      "198  eval : 0.020345356315374374\n",
      "199  train : 0.023306338116526604\n",
      "199  eval : 0.020403383299708366\n"
     ]
    }
   ],
   "source": [
    "# for _ in range(100):\n",
    "train_model(model,dataloader_dict,optim,200,loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = param_index(model,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  train : 0.02287113666534424\n",
      "0  eval : 0.019984418526291847\n",
      "1  train : 0.022902486845850945\n",
      "1  eval : 0.020153814926743507\n",
      "2  train : 0.022845495492219925\n",
      "2  eval : 0.020038355141878128\n",
      "3  train : 0.02285665087401867\n",
      "3  eval : 0.01998898573219776\n",
      "4  train : 0.02285449579358101\n",
      "4  eval : 0.020066820085048676\n",
      "5  train : 0.022846484556794167\n",
      "5  eval : 0.020126650109887123\n",
      "6  train : 0.022850114852190018\n",
      "6  eval : 0.02014332078397274\n",
      "7  train : 0.02287016436457634\n",
      "7  eval : 0.020136702805757523\n",
      "8  train : 0.022860372439026833\n",
      "8  eval : 0.020159833133220673\n",
      "9  train : 0.02287468872964382\n",
      "9  eval : 0.01996036432683468\n"
     ]
    }
   ],
   "source": [
    "# for _ in range(20):\n",
    "train_model(model,dataloader_dict,optim,10,loss_f,index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b89859744d7c350b12eb8b3275b0a21a12a464476cbd0474ddca5e429d7bc1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
