\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
% ---------------------------------------------
% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}[theorem]{Lemma}
% 定义定理环境，编号独立
\newtheorem{theorem}{Theorem}
% 定义引理环境，编号独立
\newtheorem{lemma}{Lemma}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{tabularx} % Allows tables to have adjustable widths
% ---------------------------------------------

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{A Robust Data Synthesis Method based on Feature Subspace 
Interpolation to Optimize Samples with Unknown Noise}

% ----------------------------------------------------------------------

% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% % The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% % Remember, if you use this you must call \IEEEpubidadjcol in the second
% % column for its text to clear the IEEEpubid mark.

% ----------------------------------------------------------------------

\maketitle

\begin{abstract}
Sample noise, characterized by complex and elusive distribution 
patterns, poses a significant challenge to the efficacy of data 
analysis and machine learning models, especially in scenarios of 
data scarcity and high dimensionality. Traditional methods for 
managing datasets with unknown noise often result in the loss of 
original information and lead to overgeneralization. Most existing 
data synthesis methods do not adequately address noisy dataset 
conditions because they were not specifically designed to handle 
sample noise. This paper proposes a novel data synthesis approach, 
the Robust Subspace Interpolation Synthesis (RSIS), which is based on 
feature subspace linear and equidistant interpolation. This technique 
synthesizes samples with reduced noise without sacrificing the 
original sample information and adaptively interpolates between 
feature subspaces to enhance data diversity and representativeness. 
Contrary to the common assumption that sample noise is identically 
distributed, we theoretically prove that the RSIS method maintains 
optimization capabilities across varying noise distributions, which 
is further validated in simulation experiments. Benchmark dataset 
experiments reveal that RSIS is applicable across various models, 
significantly enhancing their predictive performance. RSIS exhibits 
excellent robustness, even when processing samples with 
high-variance noise and under practical conditions that deviate 
from RSIS assumptions. Additionally, by integrating RSIS with 
convolutional neural networks, our experiments demonstrate its 
effectiveness in improving image prediction accuracy.
\end{abstract}

\begin{IEEEkeywords}
Data Synthesis, Feature Subspace, Interpolation, Optimize Samples, Robust.
\end{IEEEkeywords}

% ------------------------------------------------------------

\section{Introduction}
\IEEEPARstart{D}{ata} noise, obscuring true information, severely 
undermines the reliability and accuracy of analytical and predictive 
models. It refers to any inaccurate or misleading 
information within the data, which may stem from various reasons such 
as instrument errors, transmission issues, or even 
systematic biases in the data collection process \cite{bib1,bib2,bib3}. In many practical 
scenarios, the distribution of noise is intricate and not well-defined, 
leading to substantial reductions in the efficacy of data analysis 
and machine learning models. This problem is critical 
in fields that demand highly accurate predictions and analyses, 
such as medical image analysis, quantitative finance, and 
environmental surveillance \cite{bib4,bib5,bib6}.

To address the problem of datasets with unknown noise, researchers 
have developed a variety of methods, including data cleaning \cite{bib7,bib8}, 
data smoothing \cite{bib9,bib10,bib11}, and outlier detection \cite{bib12,bib13}. These methods 
primarily focus on mitigating or removing noise from the data, but 
their effectiveness is limited and can result in information loss \cite{bib14}. 
For instance, outliers may in some cases be an important information 
in data analysis, while data cleaning might mistakenly identify them 
as noise and remove them \cite{bib15}. Similarly, data smoothing techniques, 
such as moving averages or Gaussian smoothing, can reduce random 
fluctuations in the data but may also lead to overgeneralization. 
This overgeneralization can obscure important features and details 
within the data, thereby affecting the accuracy of data analysis. 

While traditional methods offer some effectiveness under specific conditions, 
they often fall short in real-world applications characterized by complex noise 
patterns, particularly in datasets with a limited number of samples. In addition, 
existing data synthesis methods, which aim to perform data augmentation, sample expansion, 
and simulation by generating non-real yet statistically representative data, do not 
exhibit robust performance on noisy datasets, primarily because most were not designed to 
address sample noise issues \cite{bib20,bib25,bib32,bib36,bib39}.
Against this backdrop, we introduce 
a novel data synthesis method for datasets with unknown noise, based 
on the concept of feature subspace interpolation, named Robust 
Subspace Interpolation Synthesis (RSIS). This method can optimize 
the original dataset in two ways. Firstly, the RSIS method can 
synthesize samples with less noise without losing the information 
of the original samples, not only reducing the average error of the 
samples but also decreasing the proportion of high-noise samples. 
Secondly, the RSIS method can adaptively interpolate samples between 
feature subspaces to increase data diversity and representativeness, 
better highlighting the true functional relationships between 
variables.  Compared to traditional methods, RSIS is more suitable 
for sample noise processing in the field of machine learning, 
effectively enhancing the generalization ability of models. 

The RSIS method includes several key steps. First, using the 
unsupervised clustering algorithm we proposed, the original dataset 
is divided into multiple subsets with approximately equal sample 
sizes, each corresponding to a feature subspace. Then, utilizing the 
clustering results, the RSIS method introduces the concept of the 
Traveling Salesman Problem (TSP) to order the feature subspaces. 
Subsequently, we integrate the mechanism of soft 
parameter sharing, conducting a linear fitting between 
adjacent subspaces considering global information based 
on the ordering results. Finally, we propose an 
innovative multi-stage minimum weight matching method, 
through which an suboptimal interpolation matching strategy 
is obtained, allowing for the generation of samples with 
smaller errors through linear and equidistant 
interpolation of adjacent subspaces.

Currently, most studies assume that sample noise is identically 
distributed, which is rare in practical applications. We theoretically 
prove that the RSIS method can still exhibit good optimization 
effects even when faced with noise of different distributions. We 
also simulated samples with unknown and differently distributed 
complex noise, where the RSIS method demonstrated good robustness 
and significantly optimized the samples. In addition, experimental 
results of benchmark datasets demonstrate exceptional adaptability 
with different models, generally improving their prediction accuracy 
after optimization by RSIS, and even if there are violations of the 
RSIS assumptions in practical applications, this method may still 
achieve good optimization results. Furthermore, we apply the RSIS 
method in conjunction with convolutional neural networks (CNN) to 
the field of computer vision, where experimental results show that 
the RSIS method can also effectively improve the generalization 
ability of image classification models. 

% -----------------------------------------------------------------



\section{Related Work}
Existing data synthesis methods aim to perform data augmentation, 
sample expansion, and simulation by generating non-real yet 
statistically representative data. These methods are widely used in 
scenarios of data scarcity, privacy protection, or model training. 
Currently, the main methods of data 
synthesis are as follows.

Rule-based methods depend on predefined rules or algorithms to modify 
existing data or generate new data instances. Classic techniques such 
as linear interpolation and polynomial interpolation utilize known 
data points to estimate unknown points \cite{bib19,bib20}. Data augmentation 
techniques enhance the diversity of image datasets through predefined 
image transformations, including rotation, scaling, flipping, and 
cropping. Data augmentation techniques such as MixUp and CutMix play 
a pivotal role in enhancing model generalization by synthesizing new 
samples through the proportional mixing of random image samples. These 
methods, by introducing a controlled level of variation within the 
training data, significantly contribute to improving model robustness 
across diverse domains \cite{bib21,bib22}. Furthermore, simulation methods 
such as Monte Carlo simulations and surrogate models employ random 
sampling or simplified representations to accurately emulate data 
distributions. This approach facilitates the analysis of complex 
system behaviors, enables rapid assessment of system responses, and 
significantly enhances the quality and accessibility of data \cite{bib23,bib25}.

Statistical-based methods can be subdivided into parametric and 
non-parametric, utilizing statistical models to estimate data 
distributions and thereby generate data. Within parametric approaches, 
the synthesis of data is contingent upon predefined assumptions about 
distributions, with typical methods including Gaussian mixture models 
and Bayesian networks \cite{bib26,bib27}. These models assume that the data 
follows certain recognized probability distributions, and then utilize these 
distributions to generate new data points, maintaining correlations 
within the data \cite{bib28,bib30}. Non-parametric methods, 
such as histogram approaches, kernel density estimation, and SMOTE 
methods along with their variants (SVM-SMOTE, SMOTE-ENC), do not rely 
on fixed distribution models but infer the shape of distributions 
directly from the data, thereby affording greater flexibility in 
accommodating various types and configurations of data distributions 
\cite{bib31,bib32,bib33,bib34,bib35}.

Data synthesis methods based on machine learning have seen rapid 
development in recent years, particularly with the emergence of 
technologies like Generative Adversarial Networks (GANs), Variational 
Autoencoders (VAEs), and Conditional Generative Adversarial Networks 
(cGANs). GANs introduced an innovative adversarial training mechanism, 
where the generator and discriminator engage in competition, enabling 
the generator to produce increasingly realistic data \cite{bib36,bib37,bib38}. VAEs 
model high-dimensional data distributions by optimizing the 
variational lower bound of the latent space, providing a robust 
mathematical framework for the analysis of complex data 
distributions \cite{bib39,bib40}. cGANs, by incorporating conditional variables, 
allow for the addition of specific conditions during the data 
generation process, thus providing enhanced flexibility and control 
in data synthesis \cite{bib41,bib42}. Moreover, autoregressive models such as 
PixelRNN and PixelCNN have demonstrated strong capabilities in 
sequentially generating image and text data, by learning the 
sequential dependency relationships within the data \cite{bib43,bib44}.

Hybrid methods aim to enhance the quality, diversity, and 
practicality of synthetic data by integrating various data synthesis 
techniques, such as Bayesian Gaussian Mixture Models (Bayesian GMM), 
Dirichlet Process Mixture Models (DPMM), and Hierarchical Mixture 
Models (HMM). The Bayesian GMM utilizes Bayesian approaches to 
estimate the parameters of Gaussian Mixture Models, providing a 
posterior distribution of parameters that more accurately reflects 
their uncertainty \cite{bib46,bib47}. The DPMM, a non-parametric Bayesian 
approach, adapts automatically to data, enabling a flexible number 
of infinite latent categories or components for accurately modeling 
data distributions, thereby producing synthetic data that mirrors 
the complex structure of the original data \cite{bib48,bib49}. The HMM adds a 
hierarchical structure to mixture models, allowing for variability 
at different levels and facilitating the creation of synthetic data 
that more closely resembles the inherent hierarchical organization of 
the data \cite{bib51}.

Existing data synthesis methods do not exhibit robust performance 
on noisy datasets, primarily because most were not designed to 
address sample noise issues. For instance, the SMOTE algorithm is 
intended for balancing sample data in classification 
tasks, and the objective of GANs and VAEs is to learn 
the distribution of data for generating new samples that 
are indistinguishable from the original dataset. RSIS, a 
novel data synthesis approach derived 
from hybrid methods, effectively addresses the challenges of noisy 
datasets. It showcases the potential of advanced synthesis techniques 
to optimize samples, thereby improving the performance and 
generalization capacity of machine learning models when dealing 
with complex data.

% --------------------------------------------------------------------

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.3in]{Fig1.png}
  \caption{The overall flowchart of the RSIS method. (a) The 
  original feature space is divided into several feature 
  subspaces with approximately equal sample sizes through an 
  unsupervised clustering algorithm. (b) The TSP problem is 
  solved for the case of a two-dimensional feature space, 
  resulting in a subspace interpolation sorting scheme. (c) 
  Performing linear fitting between subspaces with adjacent 
  sorting. (d) Based on the results of linear fitting, four 
  sample interpolation matching strategies between adjacent 
  subspaces is obtained through the multi-stage minimum weight 
  matching method, and interpolation is performed to 
  synthesize samples with smaller errors.}
  \label{Fig1}
\end{figure}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=4.5in]{Fig2.png}
  \caption{The hierarchical clustering process of RSIS.}
  \label{Fig2}
\end{figure*}

\section{Proposed Method}
The RSIS method is divided into four steps: The first step involves 
dividing the original feature space into multiple subspaces by 
unsupervised clustering method, with the RSIS method requiring 
that these subspaces have nearly equal sample sizes. Subsequently, the subspaces are ranked in order, with this step of 
the process transforming the subspace interpolation sorting issue 
into TSP to be solved with the goal of 
minimizing the total distance. We integrate the principle of soft parameter sharing mechanisms, 
executing linear fitting on samples across adjacent subspaces. Lastly, we introduce a multi-stage minimum weight matching algorithm 
that can quickly obtain effective linear interpolation matching 
strategies for samples between adjacent subspaces. The overall 
flowchart of the RSIS method is shown in Fig. \ref{Fig1}.

\subsection{Dividing of Feature Spaces}
We propose an unsupervised clustering algorithm that can 
partition the feature space into multiple subspaces with 
similar sample sizes. This algorithm consists of two stages: 
hierarchical clustering and the K-Nearest Neighbors 
optimization.

\subsubsection{Hierarchical Clustering}
The design of the hierarchical clustering process is 
primarily based on the concept of a greedy strategy, 
implemented through an unsupervised algorithm that 
aggregates clusters via multiple iterative cycles. 
An initial hyperparameter $k$ is required, where $k$ 
intuitively represents the number of subspaces into which 
the original feature space is partitioned. 

In the initial iteration process, an ordered set of sample 
features is given as $C_1=\left\{\boldsymbol{c}_i \right\}_{i=1}^n
=\left\{\boldsymbol{x}_i \right\}_{i=1}^n$, where the 
$\boldsymbol{c}_i$ is the $i\text{-th}$ element in set 
$C_1$. Subsequently, the closest pair of elements 
$\boldsymbol{c}_p$ and $\boldsymbol{c}_q$ within $C_1$ is 
identified, where $p,q=\mathop{\text{argmin}}_{\substack{p, q; p\neq {q}}} 
\text{dist}(\boldsymbol{c}_p,\boldsymbol{c}_q)$. Based on this, 
the set $D_{1}=\{\boldsymbol{c}_p,\boldsymbol{c}_q\}$, 
encompassing the two nearest elements, is defined as the first 
cluster, and calculate the centroid according to Formula \eqref{eq1}:
\begin{equation}
\label{eq1}
\bar{\boldsymbol{x}}^s = \frac{\sum_{\boldsymbol{x}^s \in D_s} 
\boldsymbol{x}^s}{\text{num}(D_s)}, 
\end{equation}
where $\text{num}(D_s)$ is the number of samples in set $D_s$. At 
the end of the first iteration, $\boldsymbol{c}_p$ and $\boldsymbol{c}_q$ are removed from the set $C_1$, and add the 
centroid of $D_1$ to $C_1$. i.e., $C_{2}=\{\boldsymbol{c}_i \in C_{1} | i \neq 
p, q\} \cup \{\bar{\boldsymbol{x}}^1\}$.

\begin{algorithm}[b!]
  \caption{Hierarchical Clustering.}\label{alg:alg1}
  \begin{algorithmic}
  \STATE 
  \STATE $\mathbf{Input}$: the set of sample features $C$, hyperparameter $k$
  \STATE $\mathbf{Output}$: $D=\{D_s\}_{s=1}^k$
  \vspace{5pt} %在这里调整距离以达到期望的空间大小
  \STATE While: $C\neq\emptyset$:
  \STATE \hspace{0.5cm}Obtain $\boldsymbol{c}_p$ and $\boldsymbol{c}_q$ by $\mathop{\text{argmin}}_{\substack{p, q ; p \neq  {q}}}\text{dist}(\boldsymbol{c}_p,\boldsymbol{c}_q)$
  \STATE \hspace{0.5cm}Judging the situation of $\boldsymbol{c}_p$ and $\boldsymbol{c}_q$:
  \STATE \hspace{1.0cm}If both $\boldsymbol{c}_p$ and $\boldsymbol{c}_q$ are samples:
  \STATE \hspace{1.5cm}Define the new cluster $D_s=\{\boldsymbol{c}_p,\boldsymbol{c}_q\}$
  \STATE \hspace{1.5cm}Calculate the centroid of $D_s$ and add it to $C$
  \STATE \hspace{1.5cm}Remove $\boldsymbol{c}_p$ and $\boldsymbol{c}_q$ from $C$
  \STATE \hspace{1.0cm}Else if $\boldsymbol{c}_p:\bar{\boldsymbol{x}}^s$ or $\boldsymbol{c}_q:{\bar{\boldsymbol{x}}^s}$:
  \STATE \hspace{1.5cm}$D_s\leftarrow D_s {\cup} \{{\boldsymbol{c}}_q\}$ or $D_s\leftarrow D_s {\cup} \{{\boldsymbol{c}}_p\}$
  \STATE \hspace{1.5cm}Recalculate the centroid of $D_s$ and renew it
  \STATE \hspace{1.5cm}in $C$
  \STATE \hspace{1.5cm}Remove $\boldsymbol{c}_q$ or $\boldsymbol{c}_p$ from $C$
  \STATE \hspace{1.0cm}Else if both $\boldsymbol{c}_p:{\bar{\boldsymbol{x}}^s}$ and $\boldsymbol{c}_q:{\bar{\boldsymbol{x}}^l}$ are centroids:
  \STATE \hspace{1.5cm}$D_s \leftarrow D_s  {\cup}  D_l$
  \STATE \hspace{1.5cm}Recalculate the centroid of $D_s$ and add it
  \STATE \hspace{1.5cm}to $C$
  \STATE \hspace{1.5cm}Remove $\boldsymbol{c}_p$ and $\boldsymbol{c}_q$ from $C$
  \STATE \hspace{0.5cm}If $\text{num}(D_s)\ge \left\lceil n/k\right\rceil$:
  \STATE \hspace{1.0cm}Remove $\bar{\boldsymbol{x}}^s$ from $C$
  \STATE \hspace{1.0cm}Detect the redundant samples in $D_s$ by LOF
  \STATE \hspace{1.0cm}algorithm
  \STATE \hspace{1.0cm}Remove $\{\boldsymbol{x}^s_i\}_{i=\left\lceil n/k\right\rceil+1}^{\text{num}(D_s)}$ from $D_s$ and add them to
  \STATE \hspace{1.0cm}$C$
  \STATE \hspace{1.0cm}$D \leftarrow D  {\cup}  D_s$
  \end{algorithmic}
  \label{Alg1}
\end{algorithm}

In subsequent iterations, it is required to determine the two 
elements $\boldsymbol{c}_p,\boldsymbol{c}_q$ with the closest 
distance in the set $C_t$. Three scenarios are considered 
for $\boldsymbol{c}_p$ and $\boldsymbol{c}_q$: both are 
original samples; one is a sample and the other is a cluster 
centroid; both are cluster centroids.

In the scenario where both $\boldsymbol{c}_p$ and $
\boldsymbol{c}_q$ are original samples, the set $D_s=\{
  \boldsymbol{c}_p,\boldsymbol{c}_q\}$ is defined as a new cluster 
  and its centroid $\bar{\boldsymbol{x}}^s$ is computed. 
  $\boldsymbol{c}_p$ and $\boldsymbol{c}_q$ are then removed 
  from set $C_t$ and $\bar{\boldsymbol{x}}^s$ is added to $C_t$, i.e.,
   $C_{t+1}=\{\boldsymbol{c}_i \in C_{t} | i \neq p, q\} \cup 
   \{\bar{\boldsymbol{x}}^s\}$.

In the scenario where $\boldsymbol{c}_p$ is an original sample and 
$\boldsymbol{c}_q:{\bar{\boldsymbol{x}}^s}$ is a cluster centroid, 
add $\boldsymbol{c}_p$ to $D_s$. Then, $\boldsymbol{c}_p$ is removed 
from set $C_t$, and the centroid of $D_s$ within $C_t$ is 
recalculated and updated according to Formula \eqref{eq1}, resulting in 
$C_{t+1}=\{\boldsymbol{c}_i \in C_{t} | i \neq p, q\}\cup \{ 
  \bar{\boldsymbol{x}}^{s^\prime} \}$.

If both $\boldsymbol{c}_p:{\bar{\boldsymbol{x}}^s}$ and $
\boldsymbol{c}_q:{\bar{\boldsymbol{x}}^l}$ are centroids, then 
the two clusters are merged by $D_s \leftarrow D_s  
{\cup}  D_l$, and the centroid of $D_s$ is recalculated. We can 
obtain $C_{t+1}=\{\boldsymbol{c}_i \in C_{t} | i \neq p, q\} \cup \{
  \bar{\boldsymbol{x}}^{s^\prime}\}$.

At the end of each iteration, if $\text{num}(D_s){\ge}\left\lceil 
n/k\right\rceil$, remove $\bar{\boldsymbol{x}}^s$ from $C_{t+1}$. 
Subsequently, the Local Outlier Factor (LOF) algorithm \cite{bib53} is 
employed to detect the outliers in $D_s$, and eliminate the 
superfluous samples $\{\boldsymbol{x}^s_i\}_{i=\left\lceil n/k
\right\rceil+1}^{\text{num}(D_s)}$ from $D_s$. $D_s$ is then 
considered a complete cluster and excluded from further iterations.
 Lastly, the redundant samples are 
 reinserted into $C_{t+1}$. The iterative process ends when 
$C_{t+1}=\emptyset$. The detailed procedural flow for this 
 segment can be referenced in Fig. \ref{Fig2}, and Algorithm \ref{Alg1}.

Consequently, the iterative process can partition the original 
dataset into $k-1$ subsets, each containing an equal number 
of samples; and one additional subset consisting of no more 
than $\left\lceil n/k\right\rceil$ samples. Formally, we 
define $D={\cup}^k_{s=1}D_s$, $D_s=\{\boldsymbol{x}_i\}_{i=1}^l$, 
where $1 {\le} l {\le} \left\lceil n/k\right\rceil$. Moreover, the 
iterative clustering process of RSIS results in the division of 
the original feature space into $k$ feature subspaces, represented 
as $\mathcal{X} = \cup_{s=1}^{k} \mathcal{X}^s$.

\subsubsection{K-Nearest Neighbors Optimization}
The hierarchical clustering process, being based on the concept 
of a greedy strategy, may yield irrational outcomes in later 
iterations, which affects the overall clustering performance. 
Consequently, an optimization process utilizing K-Nearest 
Neighbors (KNN) has been developed to fine-tune the clustering 
results. This approach may affect the balance of the number of 
samples in each cluster, but it will significantly improve the 
performance of the clustering.

Upon completing the hierarchical clustering phase, each sample 
is endowed with a label reflecting its cluster membership. 
During the KNN optimization phase, the algorithm identifies 
the $\left\lceil n/k\right\rceil$ nearest neighbors for each 
sample in the feature space. Each sample's cluster label is then 
updated to reflect the most frequently occurring cluster label 
among its neighbors, as it can be seen in Fig. \ref{Fig3}. It is important to note that this process 
may induce changes in the cluster affiliation of some samples, 
thereby causing slight fluctuations in the number of samples 
within clusters. Such alterations could result in the dissolution 
of clusters with a more dispersed sample distribution. By 
integrating the hierarchical clustering process with KNN 
optimization, we arrive at the final clustering results 
$\{D_s\}_{s=1}^{k'}$, where $k'$ denotes the number of clusters 
post-optimization.

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.45in]{Fig3.png}
  \caption{K-Nearest Neighbors optimization.}
  \label{Fig3}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.45in]{Fig4.png}
  \caption{Greedy algorithm and 3-opt solved for TSP in 
  two-dimensional feature space.}
  \label{Fig4}
\end{figure}

\subsection{Subspace Interpolation Path Sorting}
The design philosophy of the interpolation path involves starting 
from a subspace, traversing each subspace exactly once, and 
ultimately returning to the original point. The aim is to minimize 
the total distance of the path. This challenge can be formulated 
as TSP, which is then addressed 
by depicting the issue as a weighted graph. Bridging this concept 
to our ultimate goal, the objective function is defined as:
\begin{equation}
\label{eq2}
\min{\sum\limits_{i=1}^{k'-1}\text{dist}(\boldsymbol{\bar{x}}^i,
\boldsymbol{\bar{x}}^{i+1})}+\text{dist}(\boldsymbol{\bar{x}}^{k'},
\boldsymbol{\bar{x}}^1). 
\end{equation}

The TSP is a classic NP-hard problem 
in combinatorial optimization, posing significant challenges in 
finding polynomial-time solutions as graph complexity increases. 
Widely applicable in logistics, planning, and manufacturing, 
the TSP's direct solution is complex, prompting the development 
of heuristic and approximation algorithms to seek near-optimal 
solutions efficiently \cite{bib56,bib57}. To ensure that a sufficiently good 
solution can be obtained within an acceptable timeframe, we first 
employ a greedy algorithm to quickly find an initial solution, 
which is then optimized using the 3-opt method to arrive at the 
final solution \cite{bib61,bib64}, as is shown 
in Fig. \ref{Fig4}. Ultimately, a sorted set 
$D_\text{sorted}=\{D_{(s)}\}_{s=1}^{k'}$ is obtained. Corresponydingly, $\mathcal{X}_{\text{sorted}}=\cup_{s=1}^{k^\prime}\mathcal{X}^{(s)}$. In the subsequent sections of this paper, the feature 
subspace corresponding to two subsets with adjacent indices will be 
defined as adjacent feature subspaces.

The above discussion applies to multidimensional feature spaces. 
However, in the case of a one-dimensional feature space, given 
the unique characteristics of one-dimensional spaces, the design 
of the interpolation path still requires passing through all 
subspaces exactly once, but it does not necessitate returning to 
the starting point. A greedy strategy can be directly employed to 
solve this, and in the one-dimensional case, the solution obtained 
by this method is guaranteed to be optimal. First, the starting 
subspace of the interpolation path is identified:
\begin{equation}
\label{eq3}
D_{(1)}=\mathop{\text{argmin}}_{\substack{D_{(s)}\in D_{\text{sorted}}}}
\text{dist}({\bar{x}}^s,{x}_0), 
\end{equation}
where $x_0$ is the minimum value in feature set. The definition 
for subsequent clusters is as follows:
\begin{equation}
\label{eq4}
D_{(d)} = \mathop{\text{argmin}}_{\substack{D_{(s)} \in D_{\text{sorted}}; \\ 
D_{(s)} \neq D_{(1)}, \ldots, D_{(d-1)}}}\text{dist}\left(\bar{x}^s, 
\bar{x}^{s-1}\right).
\end{equation}

\subsection{Adjacent Subspace Linear Regression Fitting}
Given a dataset with noise $D=\{\boldsymbol{x}_i,y_i\}_{i=1}^n$, 
where $\boldsymbol{x}_i\in \mathcal{X}=\mathbb{R}^d$ and 
$y_i\in\mathcal{Y}=\mathbb{R}$. Assume that 
$\dot{y}_i=f(\dot{\boldsymbol{x}}_i)$, where 
$\dot{\boldsymbol{x}}_i$ represents the denoised true value of 
$\boldsymbol{x}_i$, and $\dot{y}_i$ is the true value of 
$y_i$. The RIRS method presupposes that the function $f(\cdot)$ 
is continuous. And we have:
\begin{align} 
\label{eq5}
\dot{y}_i+\varepsilon_{i,y}=f(\dot{\boldsymbol{x}}_i+
\boldsymbol{\varepsilon}_{i,x})+\varepsilon_i, 
\end{align}
where $\boldsymbol{\varepsilon}_{ix}$ and $\varepsilon_{i,y}$ 
respectively represent the noise contained in 
$\dot{\boldsymbol{x}}_i$ and $\dot{y}_i$, $\varepsilon_i$ is the 
error term of the equation. Equation \eqref{eq5} can be transformed into:
\begin{equation} 
\label{eq6}
y_i=f(\boldsymbol{x}_i)+\varepsilon_i. 
\end{equation}

Through the aforementioned clustering and subspace sorting 
algorithms, the dataset $D$ can be partitioned into multiple 
ordered subsets $D_{\text{sorted}}=\{D_{(s)}\}_{s=1}^{k^\prime}$, 
where $D_{(s)}=\{ \boldsymbol{x}_i^s,y_i^s \}_{i=1}^{\text{num}
(D_{(s)})}$, and the feature space can be divided into multiple 
subspaces $\mathcal{X}_{\text{sorted}}=\cup_{s=1}^{k^\prime}\mathcal{X}^{(s)}$. For 
two adjacent feature subspaces, given that the RSIS method assumes 
the relation $f(\cdot)$ to be a continuous function, it allows 
$f(\cdot)$ to be approximately fitted as a linear function 
$g(\cdot)$, hence Equation \eqref{eq6} can be transformed into: 
\begin{equation} 
\label{eq7}
y_i^{s,s+1}=g_{s,s+1}(\boldsymbol{x}_i^{s,s+1})+\varepsilon_i+
\varepsilon_i^\prime, 
\end{equation}
where $g_{s,s+1}(\cdot)$ and $\varepsilon_i^\prime$ respectively 
denote the linear fitting function between adjacent feature 
subspaces $\mathcal{X}^{(s)}$ and $\mathcal{X}^{(s+1)}$, and the linear 
fitting error, $\{\boldsymbol{x}_i^{s,s+1},y_i^{s,s+1}\}\in D_{(s)}\cup 
D_{(s+1)}$. If the distance between two subspaces is sufficiently 
close and the measure of the subspaces tends towards $0$, then the 
linear fitting error $\varepsilon_i^\prime\rightarrow0$.

One may consider employing methods such as Ordinary Least Squares 
(OLS), Lasso regression, or locally weighted linear regression to 
estimate $g_{s,s+1}(\cdot)$. 
However, given the assumption that $f(\cdot)$ is a continuous 
function, merely conducting linear fitting based on samples from 
two subspaces does not adequately take into account global 
information. Therefore, we integrate the principle of soft 
parameter sharing mechanisms \cite{bib70,bib71}, estimating linear functions 
among multiple subspaces simultaneously, and incorporates the number of $k^\prime$ 
$L_1$ regularization term in the design of the loss function. This approach ensures that the estimation of each linear function 
$\hat{g}_{s,s+1}(\cdot)$ incorporates global information to the 
greatest extent possible:
\begin{equation} 
\label{eq8}
\begin{aligned}
&L(\boldsymbol{\beta})= \\
&\sum\limits_{s=2}^{k^\prime-1}
\left(L_2(\boldsymbol{\beta}^{s,s+1})+\dfrac{\lambda}
{\text{dist}(\overline{\boldsymbol{x}}^{s},
\overline{\boldsymbol{x}}^{s-1})+1} 
L_1(\boldsymbol
{\beta}^{s,s+1},\boldsymbol{\beta}^{s-1,s}) \right) \\
&\qquad+L_2(\boldsymbol{\beta}^{1,2})+L_2(\boldsymbol{\beta}
^{k^\prime,1}) \\
&\qquad+\dfrac{\lambda}{\text{dist}(
  \overline{\boldsymbol{x}}^{k^\prime},\overline
  {\boldsymbol{x}}^{k^\prime-1})+1}L_1(\boldsymbol{\beta}^{k^\prime,1},
  \boldsymbol{\beta}^{k^\prime-1,k^\prime})\\
&\qquad+\dfrac{\lambda}{\text{dist}(
\overline{\boldsymbol{x}}^{k^\prime},\overline
{\boldsymbol{x}}^{1})+1}L_1(\boldsymbol{\beta}^{1,2},
\boldsymbol{\beta}^{k^\prime,1}), 
\end{aligned}
\end{equation}
where $\boldsymbol{\beta}=\left(\boldsymbol{\beta}
^{1,2},\boldsymbol{\beta}^{2,3},\ldots,\boldsymbol
{\beta}^{k^\prime-1,k^\prime},\boldsymbol{\beta}^
{k^\prime,1}\right)^\prime$, $L_2(\boldsymbol{\beta}
^{p,q})=\left\|\boldsymbol{y}^{p, q}-\boldsymbol{X}^{p, q} 
\boldsymbol{\beta}^{p, q}\right\|_{2}^{2}$, 
$L_1(\boldsymbol{\beta}^{q,r},\boldsymbol{\beta}^
{p,q})=\|\boldsymbol{\beta}^{q,r}-\boldsymbol{\beta}
^{p,q}\|_1$ and $\dfrac{\lambda}{\text{dist}(
\overline{\boldsymbol{x}}^{s},\overline{\boldsymbol{x}}
^{s-1})+1}$ serves as the coefficient for the 
regularization term, adaptively adjusted based on 
the distance between subspaces. In the subsequent 
simulation experiments and case analyses, $\lambda$ 
is set to $1$.



It is evident that the loss function is convex, 
ensuring the attainment of the global minimum 
and facilitating the application of optimization algorithms 
with guaranteed convergence properties.
% Theorem \ref{the1}, presented later, builds directly upon the 
% foundational results established by Lemma \ref{lem1} and 
% Lemma \ref{lem2}. By proving the convexity of the loss 
% function through Theorem \ref{the1}, we ensure the attainment 
% of the global minimum and facilitate the application 
% of optimization algorithms with guaranteed convergence 
% properties. Detailed proofs are provided in the 
% Appendix.

% \begin{lemma}
% \label{lem1}
% For the function $f(\boldsymbol{\beta})=\|\boldsymbol{y}-
% \boldsymbol{X}\boldsymbol{\beta}\|_2^2$, where 
% $\boldsymbol{y}\in\mathbb{R}^{n\times1}$, $\boldsymbol{X}\in
% \mathbb{R}^{n\times d}$, and $\boldsymbol{\beta}\in\mathbb{R}
% ^{d\times1}$. For $\forall\omega\in[0,1]$, and $\forall\boldsymbol
% {\beta}_1,\boldsymbol{\beta}_2\in\mathbb{R}^{d\times1}$, it can 
% be obtained:
% \begin{equation}
% \label{eq9}
% f(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_2)
% \le\omega f(\boldsymbol{\beta}_1)+(1-\omega)f(\boldsymbol{\beta}_2).
% \end{equation}
% \end{lemma}

% \begin{lemma}
% \label{lem2}
% For the function $f(\boldsymbol{\beta})=\|\boldsymbol{\beta}^1-
% \boldsymbol{\beta}^2\|_1$, where $\boldsymbol{\beta}^1,\boldsymbol
% {\beta}^2\in\mathbb{R}^{d\times1}$, $\boldsymbol{\beta}=(
% (\boldsymbol{\beta}^1)^\prime,(\boldsymbol{\beta}^2)^\prime)
% ^\prime$. For $\forall\omega\in[0,1]$, and $\forall\boldsymbol
% {\beta}_1,\boldsymbol{\beta}_2\in\mathbb{R}^{2d\times1}$, one 
% can infer:
% \begin{equation}
% \label{eq10}
% f(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_2)\le
% \omega f(\boldsymbol{\beta}_1)+(1-\omega)f(\boldsymbol{\beta}_2).
% \end{equation}
% \end{lemma}

% \begin{theorem}
% \label{the1}
% For the Equation \eqref{eq8}, $\forall\omega\in[0,1]$, 
% and $\forall\boldsymbol{\beta}_1,\boldsymbol{\beta}_2\in
% \mathbb{R}^{d\times k^\prime}$, it can be derived:
% \begin{equation}
% \label{eq11}
% L(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_2)
% \le\omega L(\boldsymbol{\beta}_1)+(1-\omega)L(\boldsymbol{\beta}_2).
% \end{equation}
% \end{theorem}

\subsection{Multi-Stage Minimum Weight Matching}
In the subsequent steps, we will synthesize new samples by 
performing linear and equidistant interpolation between adjacent 
subspaces. The samples synthesized by the RSIS take into account 
all sample information comprehensively, and the interpolation 
rules are designed as follows:

\begin{itemize}
  \item The two samples used for linear interpolation should 
  originate from distinct and adjacent subspaces.
  \item Each sample must participate in the interpolation 
  process at least once.
  \item The interpolation will be performed a total of 
  $\max(\text{num}{(D_{(s)}),\text{num}(D_{(s+1)})})$ times.
  \item The participation frequency in interpolation for all 
  samples must be uniform. Specifically, each sample is allowed 
  to participate in the interpolation process up to 
  $\left\lceil \frac{\max(\text{num}{(D_{(s)}),
  \text{num}(D_{(s+1)})})}{\min(\text{num}{(D_{(s)}),
  \text{num}(D_{(s+1)})})}\right\rceil$ times.
\end{itemize}

Without loss of generality, we 
assume that $\text{num}(D_{(s)})=m$, $\text{num}(D_{(s+1)})=n$, 
and $n/2\le m \le n$. According to the rules of interpolation, 
there are $C_{n-2k}^{2}\prod_{k=0}^{n-m-1} C_{m}^{n-m}(2m-n)!$ 
interpolation matching strategies available for selection. 
Seeking a suboptimal matching strategy is a crucial problem 
that we need to tackle.

Similarly, without loss of generality, assuming that the feature 
space is one-dimensional. The average error of samples synthesized 
through linear and equidistant interpolation between two original 
samples is measured by:
\begin{equation} 
\label{eq12}
\bar{S}(x^s,x^{s+1})=\frac{\int_{x^s}^{x^{s+1}}|f(x)-l(x)|  dx}
{|x^{s+1}-x^s|}, 
\end{equation}
where $l(x)$ denotes the linear function passing through points 
$(x^s,y^s)$ and $(x^{s+1},y^{s+1})$, and $f(\cdot)$ represents
 the real function of $x$ and $y$. 

\begin{theorem}
\label{the2}
For two adjacent subspaces $\mathcal{X}^{(s)}$ and $\mathcal{X}^{(s+1)}$, 
consider $(x^s,y^s)\in D_{(s)}\;\text{and}\; (x^{s+1},y^{s+1})
\in D_{(s+1)}$, with $y=f(x)+\varepsilon$. According to \eqref{eq7}, 
assuming the linear fitting error $\varepsilon^\prime\rightarrow0$. 
It can be obtained that:
\begin{equation}
\label{eq13}
\mathbb{E} (\bar{S}(x^s,x^{s+1}))< \mathbb{E}(\frac{|\varepsilon^s|
+|\varepsilon^{s+1}|}{2}).
\end{equation}
\end{theorem}

From Theorem \ref{the2}, under the assumption of the linear fitting error 
$\varepsilon_i^\prime\rightarrow0$, linear interpolation between 
samples from any two adjacent subspaces will result in a lower expected 
average error for the interpolated synthetic samples compared to 
the original samples, even in the scenario where noise with different 
distributions. 

\begin{theorem}
\label{the3}
Extending the groundwork established in Theorem \ref{the2}, let 
$y^s=f(x^s)+\varepsilon^s$, $y^{s+1}=f(x^{s+1})+\varepsilon^{s+1}$.
 Assuming the linear fitting error $\varepsilon^\prime
 \rightarrow0$. We have: 
\begin{equation}
\label{eq14}
\begin{aligned}
&\bar{S}(x^s,x^{s+1})\\
&=
\begin{cases} 
\dfrac{|\frac{\varepsilon^s}{\varepsilon^s-\varepsilon^{s+1}}|
\cdot |\varepsilon^{s}|+|\frac{\varepsilon^{s+1}}{\varepsilon^s-
\varepsilon^{s+1}}|\cdot|\varepsilon^{s+1}|}{2},\quad\varepsilon^s
\cdot \varepsilon^{s+1}<0,
\\
\dfrac{|\varepsilon^s|+|\varepsilon^{s+1}|}{2}, 
\qquad\qquad\qquad\qquad\qquad\varepsilon^s\cdot 
\varepsilon^{s+1}\ge0.
\end{cases}
\end{aligned}
\end{equation}
\end{theorem}

According to Theorem \ref{the3}, Formula \eqref{eq12} can be transformed into 
Formula \eqref{eq14}. We can first estimate the error of samples, 
and then determine the matching strategy based on the error 
information by employing the method of bipartite graph minimum 
weight matching. The noise of samples can be estimated by the 
linear fitting function between adjacent subspaces, under the 
assumption conditions, as can be derived from Formula \eqref{eq7}:
\begin{equation} 
\label{eq15}
\varepsilon_i^{s,s+1}=y_i^{s,s+1}-g_{s,s+1}(\boldsymbol{x}_i^
{s,s+1}).
\end{equation} 

According to interpolation rules, each sample is mandated to 
undergo at least one interpolation, and the total number of 
interpolations is constrained. After optimization with the KNN, 
the number of samples between subspaces is not always exactly 
equal, thus this cannot be considered a classical minimum weight perfect 
matching problem. This problem represents a variant of the 
weighted matching problem, incorporating features of both 
balanced matching and minimum weight matching, as it necessitates 
minimizing the total weight of matching while also ensuring the 
uniformity of sample matching occurrences. Currently, there is 
no standard algorithm readily available for this specific 
problem, necessitating modifications to the traditional minimum 
weight matching approach to accommodate this target scenario. 
Based on this, we propose a heuristic multi-stage minimum weight 
matching algorithm. Note that this method does not guarantee an 
optimal solution, but it can obtain a reasonable suboptimal 
solution in a short time.

Continuing with the previous assumption $\text{num}(D_{(s)})=m$, $\text{num}(D_{(s+1)})=n$, $n/2\le m\le n$. In this scenario, each 
sample in $D_{(s)}$ is considered for either one or two
 matches, and the solution is approached in two stages. Given a 
 weighted bipartite graph $G=(D_{(s)},D_{(s+1)},E)$ with a weight 
 function $\omega:E\rightarrow\mathbb{R}$, where $D_{(s)}$, $D_{(s+1)}$ 
 are the sets of vertices on the two sides and $E\subseteq D_{(s)}
 \times D_{(s+1)}$ is the set of edges. According to Formula 
 \eqref{eq14}, let the weight function $\omega(\boldsymbol{x}^{s},
 \boldsymbol{x}^{s+1})=\bar{S}(\boldsymbol{x}^{s},\boldsymbol{x}
 ^{s+1})$. Consider the objective function:
\begin{equation*}
\begin{aligned}
&\text{min}\textstyle\sum\limits_{(i,j)\in{E}}\omega(\boldsymbol{x}^{s}_i,\boldsymbol{x}_j^{s+1})v_{i,j},\\
&s.t. \;\;
\textstyle\sum\limits_{j=1}^nv_{i,j} \in\{1,2\}\;\text{for}\;\forall i, \\
&\quad\;\;\;\textstyle\sum\limits_{i=1}^mv_{i,j} =1\;\text{for}\;\forall j,
\end{aligned}
\end{equation*}
where $v_{i,j}$ is a binary decision variable. In the first stage, 
without considering any constraints, the Hungarian algorithm is 
applied directly to the bipartite graph $G$ to obtain a solution, 
resulting in a maximum matching $M_1$ for $G$. At this point, 
each vertex $\boldsymbol{x}^s\in D_{(s)}$ is matched once, $m$ 
vertices from $D_{(s+1)}$ are also matched once, and the 
remaining $n-m$ vertices in $D_{(s+1)}$ remain unmatched, which 
are denoted as $D^\prime_{(s+1)}$.

In the second stage, define $G'=(D_{(s)},D'_{(s+1)},E')$ with 
$\omega$, where $E'\subseteq D_{(s)}\times D'_{(s+1)}$. Still 
employing the Hungarian algorithm for $G'$ to obtain $M_2$. Since 
$n/2\le m\le n$, each vertex in $D'_{(s+1)}$ is involved in one 
matching in $M_2$ and $n-m$ vertices from $D_{(s)}$ participate 
in a matching again. The final matching is obtained $M=M_1\cup
{M_2}$.

The above describes a two-stage minimum weight matching method 
for a specific assumed situation, which is also the most common 
scenario in sample matching between subspaces. If there is a 
significant difference in the number of samples between the two 
subspaces, the problem needs to be divided into 
$\left\lceil \frac{\max(\text{num}{(D_{(s)}),\text{num}
(D_{(s+1)})})}{\min(\text{num}{(D_{(s)}),\text{num}(D_{(s+1)})})}
\right\rceil$ stages for resolution, with a more general form 
presented in Algorithm \ref{Alg2}.

\begin{algorithm}[H]
  \caption{Multi-Stage Minimum Weight Matching.}\label{alg:alg2}
  \begin{algorithmic}
  \STATE 
  \STATE $\mathbf{Input}$: $D_{(s)}=\{\boldsymbol{x}_i^{s},y_i^s\}_{i=1}^m$, 
  \STATE $\qquad\quad\; D_{(s+1)}=\{\boldsymbol{x}_i^{s+1},y_i^{s+1}\}_{i=1}^n\;(n\ge m)$, $\hat{g}_{s,s+1}$
  \STATE $\mathbf{Output}$: Interpolation matching strategy $M$
  \vspace{5pt} %在这里调整距离以达到期望的空间大小
  \STATE Estimate the error of $D_{(s)}$ and $D_{(s+1)}$ by \eqref{eq15}
  \STATE Define $G_1=(D_{(s)},D_{(s+1)},E)$ with 
  \STATE $\omega(\boldsymbol{x}^{s},\boldsymbol{x}^{s+1})=\bar{S}(\boldsymbol{x}^{s},\boldsymbol{x}^{s+1})$
  \STATE For $t=1,2,\ldots,\left\lceil\frac{n}{m}\right\rceil$ do:
  \STATE \hspace{0.5cm}Utilizing the Hungarian algorithm on bipartite graph 
  \STATE \hspace{0.5cm}$G_t$ to obtain a maximum matching $M_t$
  \STATE \hspace{0.5cm}Define $D_{\text{matched}}\subset{D_{(s+1)}}$ which have been matched 
  \STATE \hspace{0.5cm}in $M_t$
  \STATE \hspace{0.5cm}$D^\prime_{(s+1)}\leftarrow D_{(s+1)}-D_{\text{matched}}$
  \STATE \hspace{0.5cm}Define $G_{t+1}=(D_{(s)},D'_{(s+1)},E')$, where 
  \STATE \hspace{0.5cm}$E'\subseteq D_{(s)}\times D'_{(s+1)}$
  \STATE \hspace{0.5cm}$M\leftarrow{M}\cup{M_t}$
  \end{algorithmic}
  \label{Alg2}
\end{algorithm}

\subsection{RSIS Overall Analysis}
Given that the number of subspaces is $k^\prime$, it is necessary 
to perform $k^\prime$ times of multi-stage minimum weight matching 
to obtain $k^\prime$ matching strategies, ultimately yielding 
the total distance of interpolation paths between all samples, 
denoted as $\text{dist}_{\text{sum}}$. An additional 
hyperparameter $\eta$ must be specified, which intuitively 
represents the ratio of the number of synthetic samples to the 
original sample size. For two samples from adjacent subspaces, 
$\{\boldsymbol{x}^s,y^s\}$ and $\{\boldsymbol{x}^{s+1},y^{s+1}\}$, 
the synthesized samples are given by $\{\boldsymbol{x}_{(d)}^
{s,s+1},y_{(d)}^{s,s+1} \}_{d=1}^{\left\lceil \frac{n\eta }
{\text{dist}_{\text{sum}}}\text{dist}(\boldsymbol{x}^s,
\boldsymbol{x}^{s+1})\right\rceil}$, where $n$ is the original 
sample size, and the term $\frac{n\eta}{\text{dist}_{\text{sum}}}$
 is intuitively the number of samples inserted per unit distance, 
 while $\left\lceil \frac{n\eta}{\text{dist}_{\text{sum}}}
 \text{dist}(\boldsymbol{x}^s,\boldsymbol{x}^{s+1})\right\rceil$ 
 is the total number of inserted samples between two original 
 samples. Assuming all variables are continuous, 
 the linear interpolation formula is designed as follows:
\begin{equation}
\label{eq16}
\boldsymbol{x}^{s,s+1}_{(d)}=\boldsymbol{x}^s+d\dfrac
{\boldsymbol{x}^{s+1}-\boldsymbol{x}^{s}}{\left\lceil 
\frac{n\eta}{\text{dist}_{\text{sum}}}\text{dist}
(\boldsymbol{x}^s,\boldsymbol{x}^{s+1})\right\rceil+1},
\end{equation}

\begin{equation}
\label{eq17}
y^{s,s+1}_{(d)}=y^s+d\dfrac{y^{s+1}-y^{s}}{\left\lceil 
\frac{n\eta}{\text{dist}_{\text{sum}}}\text{dist}
(\boldsymbol{x}^s,\boldsymbol{x}^{s+1})\right\rceil+1}.
\end{equation}

\begin{algorithm}[!b]
  \caption{RSIS Method.}\label{alg:alg3}
  \begin{algorithmic}
  \STATE 
  \STATE $\mathbf{Input}$: Dataset $D=\{\boldsymbol{x}_i,y_i \}_{i=1}^n$,
  Hyperparameters $k,\eta$
  \STATE $\mathbf{Output}$: Synthesized dataset $D^\prime$
  \vspace{5pt} %在这里调整距离以达到期望的空间大小
  \STATE Construct feature dataset: $C=\{\boldsymbol{x}_i\}_{i=1}^n$
  \STATE Apply the hierarchical clustering algorithm to samples to obtain clusters $D=\{D_s\}_{s=1}^{k}$
  \STATE After KNN optimization, obtain: $D=\{D_s\}_{s=1}^{k^\prime}$
  \STATE Utilize greedy algorithm and 3-opt to obtain the sorted solution: $D_{\text{sorted}}=\{D_{(s)}\}_{s=1}^{k^\prime}$
  \STATE Perform linear fitting between adjacent subspaces to derive $\{\hat{g}_{s,s+1}\}_{s=1}^{k^\prime-1}\cup\{\hat{g}_{k^\prime,1}\}$
  \STATE For $s=1,2,\ldots,k^\prime$ do:
  \STATE \hspace{0.5cm}Estimate the error in adjacent subspaces samples 
  \STATE \hspace{0.5cm}from $D_{(s)},D_{(s+1)}$ by \eqref{eq15}
  \STATE \hspace{0.5cm}Employ the multi-stage minimum weight matching 
  \STATE \hspace{0.5cm}method to determine the interpolation matching 
  \STATE \hspace{0.5cm}strategy
  \STATE \hspace{0.5cm}According to the matching strategy, interpolate 
  \STATE \hspace{0.5cm}between adjacent subspaces samples to synthesize 
  \STATE \hspace{0.5cm}new data
  \STATE \hspace{0.5cm}Add the newly synthesized data to $D^\prime$
  \end{algorithmic}
  \label{Alg3}
\end{algorithm}

This approach allows for the adaptive control of the quantity of 
generated samples based on the distance between samples in the 
feature space, ensuring that inserted samples are linear and 
equidistant. Theorems \ref{the2} and \ref{the3} 
have essentially proved the effectiveness of the RSIS method. 
To ensure thoroughness and academic rigor, we additionally 
present a proof in Theorem \ref{the4} adhering to the interpolation 
format specified in Equations \eqref{eq16} and \eqref{eq17}.

\begin{theorem}
\label{the4}
For two adjacent subspaces $\mathcal{X}^{(s)}$ and 
$\mathcal{X}^{(s+1)}$, consider $(\boldsymbol{x}^s,{y}^s)\in D_{(s)}
\;\text{and}\; (\boldsymbol{x}^{s+1},{y}^{s+1})\in D_{(s+1)}$, 
with ${y}=f(\boldsymbol{x})+\varepsilon$. $\{\boldsymbol{x}_{(d)}
^{s,s+1},y_{(d)}^{s,s+1} \}_{d=1}^{\left\lceil \frac{n\eta }
{\text{dist}_{\text{sum}}}\text{dist}(\boldsymbol{x}^s,\boldsymbol
{x}^{s+1})\right\rceil}$ are the synthesized samples according to 
\eqref{eq16} and \eqref{eq17} with $\varepsilon^{s,s+1}_{(d)}=
y_{(d)}^{s,s+1}-f(\boldsymbol{x}_{(d)}^{s,s+1})$. Assuming the 
linear fitting error $\varepsilon^\prime\rightarrow0$, it can be 
obtained:
\begin{equation*}
\mathbb{E}(\frac{\textstyle\sum_{d=1}^{{\left\lceil \frac{n\eta }{\text{dist}_{\text{sum}}}\text{dist}(\boldsymbol{x}^s,\boldsymbol{x}^{s+1})\right\rceil}}{|\varepsilon^{s,s+1}_{(d)}|}}{\left\lceil \frac{n\eta }{\text{dist}_{\text{sum}}}\text{dist}(\boldsymbol{x}^s,\boldsymbol{x}^{s+1})\right\rceil})<\mathbb{E}(\frac{|\varepsilon^s|+|\varepsilon^{s+1}|}{2}).
\end{equation*}
\end{theorem}

According to Theorems \ref{the2}, \ref{the3}, and \ref{the4}, 
the RSIS method can synthesize samples with less noise compared 
to the original samples. From Equations \eqref{eq5} 
and \eqref{eq6}, it follows that the samples synthesized by RSIS 
will have smaller $\varepsilon_i$ thereby highlighting the true 
functional relationships between variables. Moreover, RSIS performs 
linear interpolation between samples, which does not compromise the 
information of the original samples. By synthesizing a large number 
of samples with smaller errors, it extracts more information from the original 
data. Consequently, RSIS can increase data diversity and representativeness.
In summary, the RSIS method is based on three assumptions. 
The function $f(\cdot)$ is continuous, the linear fitting 
error $\varepsilon^\prime\rightarrow0$ and the features 
and label variables are continuous. This method is 
delineated into multiple steps, with the overall 
operational procedure presented in Algorithm \ref{Alg3}.

% ------------------------------------------------------------------

\section{Experiments}

\begin{table*}[!t]
  \caption{DEFINITIONS OF EVALUATION METRICS\label{tab1}}
  \centering
  \begin{tabular}{cc}
  \toprule
  Metric & Expression \\
  \midrule
  MAE & \multirow{2}{*}{$\dfrac{1}{n} \sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|$} \\
  (Mean absolute error) & \\
  \\
  WIA & \multirow{2}{*}{$1 - \dfrac{\sum_{i=1}^{n}(\hat{y}_i - y_i)^2}{\sum_{i=1}^{n}(\left|\hat{y}_i - \bar{y}\right| + \left|y_i - \bar{y}\right|)^2}$} \\
  (Willmott's index of agreement) & \\
  \\
  MAPE & \multirow{2}{*}{$\dfrac{1}{n} \sum_{i=1}^{n} \left| \dfrac{y_i - \hat{y}_i}{y_i+0.01} \right|$} \\
  (Mean absolute percent error) & \\
  \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}[!t]
  \caption{SIMULATION DATASETS\label{tab2}}
  \centering
  \begin{tabular}{ccccc}
  \toprule
  Simulation Datasets & $\varepsilon_{i,x},\;\varepsilon_{i,y}$ Distribution & Samples Size & Feature Dimension & Feature Distribution \\
  \midrule
  $D_1$ & $\begin{aligned}&20\%\sim N(0,64)\\&30\%\sim U(-8,8)\\&50\%\sim N(0,1)\end{aligned}$ & 500 & 10 & $N(0,10)$ \\
  \midrule
  $D_2$ & $\begin{aligned}&20\%\sim N(0,64)\\&30\%\sim U(-8,8)\\&50\%\sim N(0,1)\end{aligned}$ & 500 & 10 & $T(0,10)$ \\
  \midrule
  $D_3$ & $\begin{aligned}&20\%\sim N(0,64)\\&30\%\sim U(-8,8)\\&50\%\sim N(0,1)\end{aligned}$ & 1500 & 10 & $N(0,10)$ \\
  \midrule
  $D_4$ & $\begin{aligned}&20\%\sim N(0,64)\\&30\%\sim U(-8,8)\\&50\%\sim N(0,1)\end{aligned}$ & 500 & 30 & $N(0,10)$ \\
  \midrule
  $D_5$ & $\begin{aligned}&50\%\sim N(0,64)\\&30\%\sim U(-8,8)\\&20\%\sim N(0,1)\end{aligned}$ & 500 & 10 & $N(0,10)$ \\
  \midrule
  $D_6$ & $\begin{aligned}&20\%\sim N(0,64)\\&30\%\sim U(-8,8)\\&50\%\sim N(0,1)\end{aligned}$ & 200 & 10 & $N(0,10)$ \\
  \bottomrule
  \end{tabular}
\end{table*}

For the simulated datasets, which are generated by a known 
function $f(\cdot)$, we thus use them to test the optimization 
performance of RSIS; for benchmark datasets, our research focuses 
on the improvement in model predictive capability after 
optimization by RSIS.

As a novel data synthesis method, RSIS lacks comparable existing 
counterparts, primarily because most methods are not tailored to 
tackle issues related to sample noise and to strengthen the 
relationships between variables. Consequently, comparative analysis between RSIS and 
these methods is not presented in this section.

\subsection{Simulation Datasets}

\begin{table*}[]
  \caption{COMPUTATIONAL RESULTS OF OPTIMIZATION EFFECTS}\label{tab3}
  \begin{tabularx}{\textwidth}{cccXXXXXXXXX}
  \toprule
  \multirow{3}{*}{Datasets} & \multirow{3}{*}{Before RSIS} & \multicolumn{10}{c}{After RSIS} \\ 
  \cmidrule{3-12}
    &  & $k$: & 5 & 10 & 15 & 5 & 10 & 15 & 5 & 10 & 15 \\ 
    &  & $\eta$: & 1 & 1 & 1 & 5 & 5 & 5 & 10 & 10 & 10 \\
  \hline
  \multirow{3}{*}{$D_1$} & 17.79 &  & 13.82+ & 14.47+ & 15.48+ & 13.14+ & 14.42+ & 15.75+ & 13.01+ & 14.48+ & 15.89+ \\
    & 0.290 &  & 0.319+ & 0.315+ & 0.313+ & 0.341+ & 0.320+ & 0.315+ & 0.344+ & 0.319+ & 0.314+ \\
    & 5.160 &  & 3.908+ & 3.947+ & 4.210+ & 3.919+ & 3.967+ & 4.207+ & 3.974+ & 4.317+ & 4.191+ \\
  \hline
  \multirow{3}{*}{$D_2$} & 19.97 &  & 16.01+ & 16.34+ & 16.97+ & 15.11+ & 16.06+ & 16.71+ & 14.95+ & 16.05+ & 16.78+ \\
    & 0.241 &  & 0.249+ & 0.259+ & 0.259+ & 0.259+ & 0.271+ & 0.265+ & 0.260+ & 0.273+ & 0.265+ \\
    & 6.179 &  & 4.484+ & 4.793+ & 5.095+ & 4.671+ & 4.813+ & 4.893+ & 4.603+ & 4.925+ & 4.913+ \\
  \hline
  \multirow{3}{*}{$D_3$} & 18.37 &  & 13.77+ & 14.35+ & 14.95+ & 13.21+ & 14.34+ & 14.89+ & 13.12+ & 14.36+ & 14.99+ \\
    & 0.313 &  & 0.333+ & 0.339+ & 0.338+ & 0.345+ & 0.345+ & 0.339+ & 0.345+ & 0.344+ & 0.336+ \\
    & 5.143 &  & 3.446+ & 3.466+ & 4.318+ & 3.187+ & 3.284+ & 3.880+ & 3.185+ & 3.225+ & 6.409+ \\
  \hline
  \multirow{3}{*}{$D_4$} & 19.29 &  & 19.29$\approx$ & 15.71+ & 15.07+ & 19.29$\approx$ & 14.26+ & 14.15+ & 19.29$\approx$ & 13.95+ & 14.01+ \\
    & 0.280 &  & 0.280$\approx$ & 0.302+ & 0.312+ & 0.280$\approx$ & 0.325+ & 0.340+ & 0.280$\approx$ & 0.331+ & 0.345+ \\
    & 6.643 &  & 6.643$\approx$ & 4.973+ & 4.578+ & 6.643$\approx$ & 4.303+ & 4.097+ & 6.643$\approx$ & 4.112+ & 3.991+ \\
  \hline
  \multirow{3}{*}{$D_5$} & 31.88 &  & 22.15+ & 22.34+ & 23.00+ & 21.17+ & 21.94+ & 22.88+ & 21.01+ & 21.90+ & 22.87+ \\
    & 0.190 &  & 0.211+ & 0.216+ & 0.220+ & 0.237+ & 0.239+ & 0.236+ & 0.240+ & 0.239+ & 0.237+ \\
    & 9.508 &  & 6.221+ & 6.557+ & 6.607+ & 6.149+ & 6.641+ & 6.679+ & 6.102+ & 6.769+ & 6.714+ \\
  \hline
  \multirow{3}{*}{$D_6$} & 18.64 &  & 15.12+ & 16.09+ & 16.89+ & 14.61+ & 14.04+ & 16.91+ & 14.53+ & 16.05+ & 16.99+ \\
    & 0.307 &  & 0.328+ & 0.329+ & 0.330+ & 0.333+ & 0.332+ & 0.332+ & 0.334+ & 0.332+ & 0.331+ \\
    & 3.958 &  & 2.943+ & 3.347+ & 3.505+ & 2.841+ & 3.271+ & 3.596+ & 2.795+ & 3.302+ & 3.577+ \\
  \hline
  $+$ &  &  & (5,5,5) & (6,6,6) & (6,6,6) & (5,5,5) & (6,6,6) & (6,6,6) & (5,5,5) & (6,6,6) & (6,6,6) \\
  $-$ &  &  & (0,0,0) & (0,0,0) & (0,0,0) & (0,0,0) & (0,0,0) & (0,0,0) & (0,0,0) & (0,0,0) & (0,0,0) \\
  $\approx$ &  &  & (1,1,1) & (0,0,0) & (0,0,0) & (1,1,1) & (0,0,0) & (0,0,0) & (1,1,1) & (0,0,0) & (0,0,0) \\

  \bottomrule
  \end{tabularx}
\end{table*}

\subsubsection{Experimental Settings}
Multiple error criteria have been used to assess optimization 
performance. The performance metrics are summarized in Table \ref{tab1}, 
where $y_i$ denotes the real value of original or synthesized 
samples, while $\hat{y}_i$ represents the observed value from the 
original dataset or the value of synthesized data obtained through 
Equation \eqref{eq17}.

Due to the RSIS method assumes that all variables and their 
functional relationships are continuous, we generate relationship 
matrices $\boldsymbol{M}_1\in\mathbb{R}^{d\times{d'}}$, and 
$\boldsymbol{M}_2\in\mathbb{R}^{d'\times{1}}$, and the model 
utilized is $y_i = f(\boldsymbol{x}_i) = \tanh(\boldsymbol{x}_i 
\boldsymbol{M}_1)\boldsymbol{M}_2$. 

\begin{equation}
\label{eq18}
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}.
\end{equation}

According to Formula \eqref{eq5}, add noise to all variables in 
the dataset. It is often difficult to determine the distribution 
of noise, and in most cases, the noise is not identically 
distributed. We designs three different distributions of noise, 
added to the simulated dataset in certain proportions, to mimic 
the actual noise conditions of real datasets \cite{bib82}. We designed 
six different simulated datasets, and each experiment was 
repeated 25 times with the average value taken as the result. 
Detailed information on the simulated datasets is shown in 
Table \ref{tab2}.

\subsubsection{Analysis of Optimization Effects}
Table \ref{tab3} presents the results for the MAE (first row), WIA 
(second row), and MAPE (third row) for each simulated experiment's 
original datasets and for the datasets processed using the RSIS 
method under varying hyperparameters. To determine whether the 
datasets were optimized, the Wilcoxon Signed-Rank Test was performed 
\cite{bib83}. Signs $'+',\;{'-'}\;\text{and}\;{'}{\approx}' $ 
respectively indicate a significant improvement, a significant 
decline, or no significant change in the metrics after processing 
with the RSIS method. The last three rows of the table detail the 
outcomes of the sign rank test, with each tuple representing the 
count of significant results for the respective metric.

% \begin{table*}[]
%   \caption{Experimental Results on Benchmark Data Sets}\label{tab2}
%   \begin{tabular*}{\textwidth}{@{\extracolsep\fill}cccccccccccc}
%   \toprule
%   \multirow{2}{*}{Data sets} & \multirow{2}{*}{Before RSIS} & \multicolumn{10}{c}{After RSIS} \\ 
%   \cmidrule{3-7}
%     &  & KNN & RF & MLP & SVR & GBDT \\ 
%   % \hline
%   % \multirow{2}{*}{Bike Sharing Demand} & - & 7620 & $2.20$ & $\boldsymbol{0.12}$ & $0.54$ & $4.26$ & $0.23$ \\
%   % & $k=150,\eta=10$ & 40380 & $2.00$ & $\boldsymbol{0.06}$ & $0.25$ & $4.12$ & $0.19$ \\
%   \bottomrule
%   \end{tabular*}
% \end{table*}

Based on the results of the Wilcoxon Signed-Rank Test, it is 
evident that the RSIS method has improved performance across 
nearly all metrics for given datasets, with no instances of 
significant decline observed. For dataset $D_4$ under the 
conditions of $k=5,\eta=1,5,10$, no significant changes were 
noted. This is primarily attributed to the fact that as feature 
dimensionality increases, the samples of most clusters become too 
sparsely distributed in the feature space. During the KNN 
optimization phase, all samples were merged into a single cluster, 
omitting the subsequent phase of RSIS. However, this issue was 
mitigated with an increase in $k$. 

Compared to $\eta$, the optimization effect of the RSIS method 
is more sensitive to the hyperparameter $k$, which will be 
further substantiated in the subsequent section. With an 
appropriately selected value for $k$, elevating the value of 
$\eta$ may amplify the method's optimization efficacy.

Experimental results also show that RSIS's performance continues 
to optimize effectively, as the content of noise with large 
variances increases. Hence, it can deal with more complex noise 
and has good robustness. Moreover, the RSIS method is capable of 
significantly optimizing samples even in datasets with 
heavy-tailed distributions or smaller sample sizes, demonstrating 
its excellent adaptability. 

\subsubsection{Hyperparameter Analysis}
To more clearly demonstrate the optimization effect of RSIS and 
the impact of hyperparameter value, we subtracted the 
pre-optimization metrics from the post-optimization metrics 
and calculated the performance of the simulated datasets under 
various hyperparameters, as shown in Fig. \ref{Fig5}. (a), (b) and (c) 
present the performance of different metrics at various $k$ values 
when $\eta=1$, while (d), (e) and (f) in Fig. \ref{Fig5} display the 
performance across different $\eta$ values at the $k$ value 
where MAE is minimized. 

In the vast majority of cases, all metrics have been optimized. 
Compared to MAE and WIA, MAPE exhibits greater fluctuations. 
The primary reason is the RSIS method synthesizing a substantial 
number of values close to zero. Given that MAPE is particularly 
sensitive to near-zero true values, especially for the simulated 
dataset $D_3$, this leads to larger variations in MAPE.

The performance of each metric under different hyperparameter 
$\eta$ values is more stable compared to $k$, and in most cases, 
as the value of $\eta$ increases, the optimization effect is 
further enhanced, subsequently leading to a gradual stabilization. 
Under different values of $k$, the optimization effect of MAE 
presents a U-shape; MAPE is optimized in most scenarios; and 
WIA initially increases and then fluctuates slightly within a 
stable range.

(g) and (h) in Fig. \ref{Fig5} represent the proportion of errors 
of different magnitudes to the total sample size before and after 
optimization by the RSIS method, with the left part of the graph 
illustrating the proportion of negative errors and the right part 
showing the proportion of positive errors. It is evident that the 
proportion of samples with larger errors has significantly 
decreased following optimization by the RSIS method, while the 
proportion of samples with smaller errors has significantly 
increased. 

In summary, our experimental results further validate that the 
RSIS method can effectively increase data diversity and representativeness, 
and highlight the true functional relationships between variables by synthesizing 
a large number of samples with smaller errors.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=5.0in]{Fig5.png}
  \caption{Optimization effect of the RSIS method with different 
  hyperparameters.}
  \label{Fig5}
\end{figure*}

\subsection{Benchmark Datasets}
To assess the optimization performance in prediction of the RSIS 
method, four benchmark datasets were utilized \cite{bib84,bib85,bib86,bib87}. Samples 
were randomly selected in varying quantities from each dataset 
and divided into training and testing sets. Following 
processing of the training data with the RSIS approach, a 
suite of machine learning models was subsequently trained on 
this dataset, which included the original training data as well 
as the data synthesized by RSIS. The models' predictive performance were subsequently 
assessed on the testing set. For each set of experiments, we 
repeated the process 25 times and took the average value as the 
experimental result, and a Wilcoxon signed-rank test was 
conducted thereafter. In addition, we removed features that 
cannot be directly used, such as “Datetime” for Bike Sharing 
Demand, “Month” for Forest Fires, and so on. We selected a range 
of machine learning models for prediction, including KNN, 
Gradient Boosting Decision Tree (GBDT), Multilayer Perceptron 
(MLP), and Support Vector Regression (SVR), with SVR employing a 
Radial Basis Function (RBF) kernel. 

\begin{table*}[!t]
  \caption{EXPERIMENTAL RESULTS FOR BENCHMARK DATASETS.}\label{tab4}
  \begin{tabularx}{\textwidth}{cccXXXX}
  \toprule
  \multirow{2}{*}{Datasets} & \multirow{2}{*}{Optimizing} & \multirow{2}{*}{$\begin{aligned}&\text{Training\;(Testing)}\\&\qquad\quad\text{set\;size}\end{aligned}$} & \multicolumn{4}{c}{Models} \\ 
  \cmidrule{4-7}
    &  &  & KNN & MLP & SVR & GBDT \\
  \cmidrule{1-7}
  \multirow{6}{*}{$\begin{aligned}&\text{Bike Sharing}\\&\quad\text{Demand}\end{aligned}$} & \multirow{3}{*}{-} & 500 (1000) & 5.99 (0.16) & 2.31 (0.14) & 64.33 (2.34) & 5.25 (0.05) \\
    &  & 1000 (1000) & 4.44 (0.12) & 1.65 (0.12) & 42.98 (1.44) & 4.45 (0.04) \\
    &  & 2000 (1000) & 3.66 (0.11) & 0.47 (0.02) & 26.85 (0.99) & 3.37 (0.04) \\
  \cmidrule{2-7}
    & \multirow{3}{*}{RSIS} & 500 (1000) & 3.66$+$ (0.14$+$) & 0.29$+$ (0.02$+$) & 12.27$+$ (1.44$+$) & 2.92$+$ (0.05$\approx$) \\
    &  & 1000 (1000) & 2.94$+$ (0.12$\approx$) & 0.18$+$ (0.01$+$) & 8.32$+$ (0.83$+$) & 2.94$+$ (0.05$\approx$) \\
    &  & 2000 (1000) & 2.66$+$ (0.09$+$) & 0.15$+$ (0.006$+$) & 4.74$+$ (0.34$+$) & 2.45$+$ (0.04$\approx$) \\
  \cmidrule{1-7}
  \multirow{6}{*}{Air Quality} & \multirow{3}{*}{-} & 500 (1000) & 105.38 (0.12) & 104.23 (0.12) & 327.52 (0.52) & 95.38 (0.11) \\
    &  & 1000 (1000) & 101.86 (0.11) & 98.97 (0.11) & 306.72 (0.49) & 89.29 (0.10) \\
    &  & 2000 (1000) & 95.76 (0.11) & 92.80 (0.10) & 268 (0.44) & 89.24 (0.10) \\
  \cmidrule{2-7}
    & \multirow{3}{*}{RSIS} & 500 (1000) & 101.43$+$ (0.11$+$) & 89.85$+$ (0.10$+$) & 218.93$+$ (0.31$+$) & 89.92$+$ (0.10$+$) \\
    &  & 1000 (1000) & 96.52$+$ (0.11$\approx$) & 88.77$+$ (0.10$+$) & 174.02$+$ (0.30$+$) & 87.52$+$ (0.10$\approx$) \\
    &  & 2000 (1000) & 90.24$+$ (0.10$+$) & 86.18$+$ (0.10$\approx$) & 144.60$+$ (0.20$+$) & 84.18$+$ (0.09$+$) \\
  \cmidrule{1-7}
  \multirow{6}{*}{$\begin{aligned}&\text{Facebook\;Metrics}\\&\;\;\;\text{(Normalized)} \end{aligned}$} & \multirow{3}{*}{-} & 100 (200) & 0.079 (0.189) & 0.072 (0.162) & 0.101 (0.187) & 0.004 (0.013) \\
    &  & 200 (200) & 0.065 (0.152) & 0.070 (0.112) & 0.084 (0.148) & 0.002 (0.006) \\
    &  & 300 (200) & 0.051 (0.124) & 0.055 (0.098) & 0.077 (0.141) & 0.001 (0.004) \\
  \cmidrule{2-7}
    & \multirow{3}{*}{RSIS} & 100 (200) & 0.078$\approx$ (0.181$+$) & 0.061$+$ (0.102$+$) & 0.105$-$ (0.198$-$) & 0.002$+$ (0.007$+$) \\
    &  & 200 (200) & 0.067$-$ (0.151$\approx$) & 0.048$+$ (0.093$+$) & 0.086$-$ (0.153$-$) & 0.001$+$ (0.004$+$) \\
    &  & 300 (200) & 0.056$-$ (0.125$\approx$) & 0.030$+$ (0.053$+$) & 0.076$\approx$ (0.136$+$) & 0.001$\approx$ (0.003$\approx$) \\
  \cmidrule{1-7}
  \multirow{6}{*}{$\begin{aligned}&\text{Forest\;Fires}\\&\text{(Normalized)}\end{aligned}$} & \multirow{3}{*}{-} & 50 (200) & 0.064 (0.540) & 0.177 (0.169) & 0.093 (0.083) & 0.063 (0.053) \\
    &  & 150 (200) & 0.060 (0.055) & 0.161 (0.158) & 0.080 (0.076) & 0.058 (0.054) \\
    &  & 300 (200) & 0.050 (0.039) & 0.093 (0.081) & 0.076 (0.067) & 0.055 (0.044) \\
  \cmidrule{2-7} 
    & \multirow{3}{*}{RSIS} & 50 (200) & 0.062$+$ (0.052$+$) & 0.144$+$ (0.104$+$) & 0.124$-$ (0.114$-$) & 0.077$-$ (0.066$-$) \\
    &  & 150 (200) & 0.059$\approx$ (0.054$\approx$) & 0.058$+$ (0.054$+$) & 0.082$\approx$ (0.078$\approx$) & 0.056$+$ (0.052$+$) \\
    &  & 300 (200) & 0.048$+$ (0.037$+$) & 0.068$+$ (0.057$+$) & 0.073$+$ (0.063$+$) & 0.054$\approx$ (0.043$\approx$) \\
  \cmidrule{1-7}
    & $+$ &  & 8 (7) & 12 (11) & 7 (8) & 9 (5) \\
    & $-$ &  & 2 (0) & 0 (0) & 3 (3) & 1 (1) \\
    & $\approx$ &  & 2 (5) & 0 (1) & 2 (1) & 2 (6) \\
  \bottomrule
  \end{tabularx}
\end{table*}

The computational results for the four models applied to the 
benchmark datasets are presented in Table \ref{tab4}, including the MAE 
and MAPE (in brackets) for each testing set. RSIS consistently 
delivers strong performance across various benchmarks and 
demonstrates exceptional adaptability with different models, 
generally improving their prediction accuracy. It can also be 
seen from Table \ref{tab4} that the effects of normalization 
and the varied sizes of training sets do not impact the 
optimization efficacy of RSIS. Fig. \ref{Fig6} demonstrates 
that after processing with the RSIS method, the prediction outcomes 
of each model are closer to the actual values, and even poorly 
performing prediction models (such as SVR) still achieve a 
significant improvement in predictive performance. 


\begin{figure}[!t]
  \centering
  \includegraphics[width=3.5in]{Fig6.png}
  \caption{Comparison of prediction effects on the partly testing 
  set of Bike Sharing Demand.}
  \label{Fig6}
\end{figure}

It is worth mentioning that the dataset contains many variables 
which are not continuous. Moreover, for sparse datasets 
(Facebook Metrics and Forest Fires), it is difficult to guarantee 
that the linear fitting error $\varepsilon'\rightarrow0$ when 
interpolating between subspaces. This indicates that even if there 
are violations of the RSIS assumptions in practical applications, 
this method may still achieve good optimization results. 
% --------------------------------------------------------------------

\section{Further Research on CNN Integration}
The preceding research primarily focused on synthesizing data 
using RSIS for feature vector data. This chapter delves into the 
application of the RSIS method in combination with the CNN 
framework in the field of image prediction.

\subsection{Method Overview}
Consider a dataset of RGB images $D=\{\boldsymbol{I}_i,\boldsymbol{L}_i \}_
{i=1}^n$, where $\boldsymbol{L}_i$ is the one-hot vector representing the category of $\boldsymbol{I}_i$. Let the feature extraction function of a CNN be 
$f:\mathbb{R}^{H\times W\times C}\rightarrow \mathbb{R}^d$, 
where $H\times W\times C$ denotes the dimensions and channel 
number of the image, and $d$ represents the dimension of the 
extracted features. Through the feature extraction operation of 
CNN, an image $\boldsymbol{I}_i$ is transformed into a feature 
vector $\boldsymbol{v}_i=f(\boldsymbol{I}_i)$. Using the RSIS 
method, we obtain $\{\boldsymbol{v}_i,\boldsymbol{L}_i\}_{i=1}^{n^\prime}$, 
where $n^\prime$ is the sample size after adding synthesized 
feature vectors. Finally, all samples are placed into a fully 
connected network for iterative training. 

The combination of the RSIS method with the CNN framework 
necessitates changes to traditional model training approaches. 
Consequently, we have designed a two-stage training strategy, as 
illustrated in Fig. \ref{Fig7}.

In the first stage, following traditional training protocols, 
the model performs forward propagation and calculates loss using 
input images. It then uses backpropagation to calculate gradients 
for updating model parameters, continuing this process until 
convergence.

In the second stage, original images are processed by the feature 
extractor, trained in the initial stage, to generate a dataset of 
feature vectors for each image. Subsequently, the RSIS method is 
employed on this dataset to synthesize new data, which is then 
exclusively retrained in the fully connected layer until 
convergence is reached. Notably, this stage updates only the 
fully connected layer's parameters, leaving the feature extraction 
layers unchanged during the training process.

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.0in]{Fig7.png}
  \caption{RSIS method combined with CNN framework.}
  \label{Fig7}
\end{figure}

\subsection{Analysis of Optimization Effects}
To assess the optimization performance in prediction, four 
image datasets (MNIST \cite{bib88}, FashionMNIST \cite{bib89}, CIFAR-10 \cite{bib90}, and 
SVHN \cite{bib91}) and CNN frameworks were utilized, including ResNet50, 
VGG16, DenseNet121, and MobileNet-v2. All models were initialized 
with pre-trained parameters. We randomly selected 200 and 1,000 
samples as the training and testing sets, respectively. In the 
second stage, new synthesized samples were added to the training 
set, expanding the original dataset to 500 (first row), 1,000 (second row), and 3,000 (third row) 
samples to explore the optimization effects at different scales. 
The process was repeated 25 times, with the average value taken 
as the experimental result, followed by a Wilcoxon signed-rank 
test. The optimization effect of the RSIS method was measured by 
the variation in the test error rate $E_{\text{var}}$:
\begin{equation}
\label{eq19}
  E=\dfrac{1}{n}\sum\limits_{i=1}^n{I}(\hat{L}_i\neq L_i),
\end{equation}
\begin{equation}
\label{eq20}
  E_{\text{var}}=E_{\text{stage1}}-E_{\text{stage2}},
\end{equation}
where $E_{\text{stage1}}$ represents the error rate on the testing 
set at the end of the first stage, and $E_{\text{stage2}}$ is 
the error rate post the second stage. It can be seen from Table \ref{tab5} 
that the RSIS significantly enhances the model's generalization 
capability in most cases. However, the benefits derived from the 
RSIS method vary according to the model's adaptability and 
compatibility with specific datasets. For instance, DenseNet121 
demonstrates significant generalization improvement and a 
pronounced reduction in prediction errors on the SVHN dataset.

\begin{table}[!t]
  \centering
  \caption{EXPERIMENTAL RESULTS FOR IMAGE DATASETS.}\label{tab5}
  \begin{tabular}{ccccc}
  \toprule
  \multirow{2}{*}{Models} & \multicolumn{4}{c}{Datesets}  \\ 
  \cmidrule{2-5}
   & MNIST & FashionMNIST & CIFAR-10 & SVHN \\
  \cmidrule{1-5}
  \multirow{3}{*}{ResNet} & 1.70\%$+$ & 0.30\%$+$ & 0.50\%$+$ & 3.10\%$+$ \\
   & 1.90\%$+$ & 1.30\%$+$ & 1.60\%$+$ & 3.60\%$+$ \\
   & 2.70\%$+$ & 1.50\%$+$ & 2.10\%$+$ & 3.60\%$+$ \\
  \cmidrule{2-5}
  \multirow{3}{*}{VGG} & 0.20\%$+$ & 0.10\%$-$ & 0.30\%$-$ & 0.80\%$-$ \\
   & 0.50\%$+$ & 0.90\%$+$ & 0.40\%$+$ & 0.00\%$\approx$ \\
   & 0.70\%$+$ & 1.10\%$+$ & 0.40\%$+$ & 0.00\%$\approx$ \\
  \cmidrule{2-5}
  \multirow{3}{*}{DenseNet} & 0.70\%$+$ & 0.20\%$+$ & 0.00\%$\approx$ & 4.70\%$+$ \\
   & 1.00\%$+$ & 0.30\%$+$ & 0.50\%$+$ & 5.10\%$+$ \\
   & 1.70\%$+$ & 0.60\%$+$ & 1.00\%$+$ & 5.40\%$+$ \\
  \cmidrule{2-5}
  \multirow{3}{*}{MobileNet} & 1.30\%$+$ & 0.70\%$-$ & 0.20\%$+$ & 3.30\%$+$ \\
   & 1.50\%$+$ & 0.40\%$+$ & 1.80\%$+$ & 5.20\%$+$ \\
   & 1.70\%$+$ & 1.20\%$+$ & 2.50\%$+$ & 5.70\%$+$ \\
  \cmidrule{1-5}
  $+$ & 12 & 10 & 10 & 9 \\
  $-$ & 0  & 2  & 1  & 1 \\
  $\approx$ & 0 & 0 & 1 &2 \\
  \bottomrule
  \end{tabular}
\end{table}

We selected the MNIST dataset, and examined the iterative changes 
in $E_{\text{stage2}}$, as illustrated in Fig \ref{Fig8}. The section 
right of the red dashed line illustrates the subsequent iterative 
changes in $E_{\text{stage2}}$ for three distinct synthesized 
sample sizes, following optimization with RSIS. It is evident 
that in most scenarios, the error significantly decreases in the 
second stage.

\begin{figure}[!b]
  \centering
  \includegraphics[width=3.3in]{Fig8.png}
  \caption{Iterative comparison of prediction errors.}
  \label{Fig8}
\end{figure}

Fig. \ref{Fig9} presents the predictive performance of four 
models across metrics $\rho_1$ (Misclassification Improvement 
Ratio) and $\rho_2$ (Negative Conversion Rate) on the 
FashionMNIST dataset.
\begin{equation}
\label{eq21}
\rho_1=\dfrac{\text{num}(C_{\text{stage2}}\cap  
M_{\text{stage1}})}{\text{num}(M_{\text{stage1}})},
\end{equation}
\begin{equation}
\label{eq22}
\rho_2=\dfrac{\text{num}(M_{\text{stage2}}\cap C_{\text{stage1}})}
{\text{num}(C_{\text{stage1}})},
\end{equation}
where $M_{\text{stage1}}$ and $M_{\text{stage2}}$ denote the 
sample misclassified sets before and after optimization, while 
$C_{\text{stage1}}$ and $C_{\text{stage2}}$ denote the sample 
correctly sets classified before and after optimization, 
respectively.  In the majority of scenarios, $\rho_1$ tends to 
increase with the growth of synthesized samples in the training 
set, a trend that is particularly pronounced in VGG16 and 
DenseNet121. Conversely, $\rho_2$ generally decreases as the 
training set expands, with this effect being especially notable 
in VGG16. 

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.3in]{Fig9.png}
  \caption{The predictive performance of four models across metrics $\rho_1$ and $\rho_2$.}
  \label{Fig9}
\end{figure}

% --------------------------------------------------------------------

\section{Conclusion}
This paper introduces a data synthesis method based on linear and 
equidistant interpolation within feature subspaces, which 
synthesizes samples with reduced noise without sacrificing 
the original sample information and adaptively interpolates 
between feature subspaces to enhance data diversity and 
representativeness. Furthermore, our experimental results reveal 
that the RSIS method significantly optimizes samples by 
synthesizing samples with reduced noise and simultaneously 
decreasing the proportion of high-noise samples. On benchmark 
datasets, it notably enhances the models' generalization 
capabilities. By innovatively integrating the RSIS method with 
the convolutional neural network framework, we have further 
improved the model's predictive performance.

However, RSIS is not 
suitable for studies on sample distribution because it alters 
the distribution of variables. Theoretically, if the 
hyperparameter $\eta$ is set to infinity, variables will 
tend to a uniform distribution. Exploring how to significantly 
optimize samples without disrupting their distribution is worth 
pursuing.


% ------------------------------------------------------------------



% {\appendix[Proof of Lemma and Theorem]
% Use $\backslash${\tt{appendix}} if you have a single appendix:
% Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
%  starts a section numbered zero.)}

% \appendices[]
% \section{Proof of Lemma 1 and Lemma 2}


\appendix

{\appendices
% \section*{Proof of Lemma 1}
% Consider the function $f(\boldsymbol{\beta})=\|
% \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\|_2^2$, 
% where $\|\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}
% \|_2$ denotes the $L_2$ norm of $\boldsymbol{y}-
% \boldsymbol{X}\boldsymbol{\beta}$. Then, for 
% $\forall\omega\in[0,1]$, and $\forall\boldsymbol{\beta}_1,
% \boldsymbol{\beta}_2\in\mathbb{R}^{d\times1}$, we have:
% \begin{equation*}
% \begin{aligned}
%   &f(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta_2})- \omega f(\boldsymbol{\beta_1})-(1-\omega)f(\boldsymbol{\beta_2}) \\
%   &=\omega^2||(y-X\boldsymbol{\beta}_1)-(y-X\boldsymbol{\beta}_2)||_2^2\\ &-\omega||(y-X\boldsymbol{\beta}_1)-(y-X\boldsymbol{\beta}_2)||_2^2 \\
%   &=\omega(\omega-1)||(y-X\boldsymbol{\beta}_1)-(y-X\boldsymbol{\beta}_2)||_2^2 \le0.
% \end{aligned}
% \end{equation*}

% From inequality $f(\omega\boldsymbol{\beta}_1+(1-\omega)
% \boldsymbol{\beta}_2)- \omega f(\boldsymbol{\beta}_1)-
% (1-\omega)f(\boldsymbol{\beta}_2)\le0$, simplification 
% yields the convexity condition:
% \begin{equation*}
% \begin{aligned}
% f(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_
% 2)\le \omega f(\boldsymbol{\beta}_1)+(1-\omega)
% f(\boldsymbol{\beta}_2).
% \end{aligned}
% \end{equation*}

% \section*{Proof of Lemma 2}
% Considering the linear structure of $f(\boldsymbol
% {\beta})$ with respect to the $L_1$ norm, we commence 
% the proof by examining the function at a convex 
% combination of $\boldsymbol{\beta}_1$ and $\boldsymbol
% {\beta}_2$, and we have:
% \begin{equation*}
%   \begin{aligned}
%     &f(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_2)\\
%     &= ||\omega(\boldsymbol{\beta}_1^1-\boldsymbol{\beta}_1^2)+(1-\omega)(\boldsymbol{\beta}_2^1-\boldsymbol{\beta}_2^2)||_1 \\
%     &\le \omega||\boldsymbol{\beta}_1^1-\boldsymbol{\beta}_1^2||_1+(1-\omega)||\boldsymbol{\beta}_2^1-\boldsymbol{\beta}_2^2||_1.
%   \end{aligned}
% \end{equation*}

% \section*{Proof of Theorem 1}
% Let $\lambda_{p,q}=\dfrac{\lambda}{\text{dist}(\overline
% {\boldsymbol{x}}^{p},\overline{\boldsymbol{x}}^{q})+1}$, 
% $L(\boldsymbol{\beta})=\gamma_1(\boldsymbol{\beta})+
% \gamma_2(\boldsymbol{\beta})$, where
% \begin{equation*}
% \begin{aligned}
% \gamma_1(\boldsymbol{\beta})=&\sum\limits_{s=2}^
% {k^\prime-1}\lambda_{s,s-1}L_1(\boldsymbol{\beta}^{s,s+1},
% \boldsymbol{\beta}^{s-1,s}) +\lambda_{k^\prime,1}L_1
% (\boldsymbol{\beta}^{1,2},\boldsymbol{\beta}^{k^\prime,1}) \\
% &+\lambda_{k^\prime-1,k^\prime}L_1(\boldsymbol{\beta}^{k^\prime,1},
%   \boldsymbol{\beta}^{k^\prime-1,k^\prime}),
% \end{aligned}
% \end{equation*}
% \begin{equation*}
% \gamma_2(\boldsymbol{\beta})=\sum\limits_{s=1}^
% {k^\prime-1}L_2(\boldsymbol{\beta}^{s,s+1})+
% L_2(\boldsymbol{\beta}^{k^\prime,1}),
% \end{equation*}
% we have: 
% \begin{equation*}
% \begin{aligned}
% &L(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_2)\\
% &=\gamma_1(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol
% {\beta}_2)+\gamma_2(\omega\boldsymbol{\beta}_1+(1-\omega)
% \boldsymbol{\beta}_2).
% \end{aligned}
% \end{equation*}

% Based on Lemma 1 and Lemma 2, we obtain:
% \begin{equation*}
% \begin{aligned}
% &L_1(\omega\boldsymbol{\beta}^{q,r}_1+(1-\omega)
% \boldsymbol{\beta}^{q,r}_2,\omega\boldsymbol{\beta}^
% {p,q}_1+(1-\omega)\boldsymbol{\beta}^{p,q}_2)\\
% &\le\omega L_1(\boldsymbol{\beta}^{q,r}_1,\boldsymbol
% {\beta}^{p,q}_1)+(1-\omega)L_1(\boldsymbol{\beta}_2^{q,r},
% \boldsymbol{\beta}^{p,q}_2),
% \end{aligned}
% \end{equation*}
% \begin{equation*}
% L_2(\omega\boldsymbol{\beta}^{p,q}_1+(1-\omega)
% \boldsymbol{\beta}^{p,q}_2)\le\omega L_2
% (\boldsymbol{\beta}^{p,q}_1)+(1-\omega)L_2
% (\boldsymbol{\beta}^{p,q}_2),
% \end{equation*}
% which further implies:
% \begin{equation*}
% \begin{aligned}
% &\gamma_1(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol
% {\beta}_2) \\
% &\le \sum\limits_{s=2}^{k^\prime-1}\lambda_{s,s-1}
% (\omega L_1(\boldsymbol{\beta}^{s,s+1}_1,\boldsymbol
% {\beta}^{s-1,s}_1)+\\
% &(1-\omega)L_1(\boldsymbol{\beta}_2^
% {s,s+1},\boldsymbol{\beta}^{s-1,s}_2))+\lambda_{k^\prime,1}(\omega L_1(\boldsymbol{\beta}^{1,2}
% _1,\boldsymbol{\beta}^{k^\prime,1}_1)\\
% &+(1-\omega)L_1(\boldsymbol{\beta}_2^{1,2},\boldsymbol{\beta}^
% {k^\prime,1}_2)) \\
% &=\omega\gamma_1(\boldsymbol{\beta}_1)+(1-\omega)\gamma_1
% (\boldsymbol{\beta}_2),
% \end{aligned}
% \end{equation*}
% \begin{equation*}
% \begin{aligned}
% &\gamma_2(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_2)\\
% &\le\sum\limits_{s=1}^{k^\prime-1}(\omega L_2(\boldsymbol{\beta}^{s,s+1}_1)+(1-\omega)L_2(\boldsymbol{\beta}^{s,s+1}_2))\\
% &+\omega L_2(\boldsymbol{\beta}^{k^\prime,1}_1)+(1-\omega)L_2(\boldsymbol{\beta}^{k^\prime,1}_2) \\
% &=\omega\gamma_2(\boldsymbol{\beta}_1)+(1-\omega)\gamma_2(\boldsymbol{\beta}_2).
% \end{aligned}
% \end{equation*}

% Hence, we have:
% \begin{equation*}
% \begin{aligned} 
% &L(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_2) \\
% &=\gamma_1(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_2)+\gamma_2(\omega\boldsymbol{\beta}_1+(1-\omega)\boldsymbol{\beta}_2) \\
% &\le \omega\gamma_1(\boldsymbol{\beta}_1)+(1-\omega)\gamma_1(\boldsymbol{\beta}_2)+\omega\gamma_2(\boldsymbol{\beta}_1)+(1-\omega)\gamma_2(\boldsymbol{\beta}_2) \\
% &=\omega L(\boldsymbol{\beta}_1)+(1-\omega)L(\boldsymbol {\beta}_2). \end{aligned}
% \end{equation*}

\section*{Proof of Theorem 1}
Since $\varepsilon^\prime\rightarrow0$, it follows from 
Equation \eqref{eq7} that:
\begin{equation*}
y^{s,s+1}=g_{s,s+1}(x^{s,s+1})+\varepsilon.
\end{equation*}

Transform Equation \eqref{eq12} to:
\begin{equation*}
\bar{S}(x^s,x^{s+1})=\frac{\int_{x^s}^{x^{s+1}}|g(x)-l(x)|
 dx}{|x^{s+1}-x^s|}.
\end{equation*}

According to the Law of Iterated Expectations(LIE):
\begin{equation*}
\begin{aligned}
&\mathbb{E}(\bar{S}(x^s,x^{s+1})) \\
&=\mathbb{E}(\bar{S}(x^s,x^{s+1})|\varepsilon^s\cdot\varepsilon^{s+1}<0)P(\varepsilon^s\cdot\varepsilon^{s+1}<0)\\
&\quad\; +\mathbb{E}(\bar{S}(x^s,x^{s+1})|\varepsilon^s\cdot\varepsilon^{s+1}\ge0)P(\varepsilon^s\cdot\varepsilon^{s+1}\ge0).
\end{aligned}
\end{equation*}

We can simplify $\bar{S}(x^s,x^{s+1})$ using basic 
geometric area calculations. If $\varepsilon^s\cdot
\varepsilon^{s+1}<0$, then $\exists\; x^\prime\in
(x^s,x^{s+1})$, such that $g(x^\prime)=l(x\prime)$. 
It follows that:
\begin{equation*}
\begin{aligned}
&\mathbb{E}(\bar{S}(x^s,x^{s+1})|\varepsilon^s\cdot
\varepsilon^{s+1}<0)\\
&=\frac{\mathbb{E}(|\varepsilon^s|)
\cdot|x^s-x'|+\mathbb{E}(|\varepsilon^{s+1}|)\cdot
|x^{s+1}-x'|}{2|x^{s+1}-x^s|}.
\end{aligned}
\end{equation*}

If $\varepsilon^s\cdot\varepsilon^{s+1}\ge0$, then
\begin{equation*}
\begin{aligned}
&\mathbb{E}(\bar{S}(x^s,x^{s+1})|\varepsilon^s\cdot
\varepsilon^{s+1}\ge0)\\
&=\frac{\mathbb{E}(|\varepsilon^s|+
|\varepsilon^{s+1}|)\cdot|x^{s+1}-x^{s}|}
{2|x^{s+1}-x^s|}\\
&=\frac{\mathbb{E}(|\varepsilon^s|+|\varepsilon^{s+1}|)}
{2}.
\end{aligned}
\end{equation*}

Substituting into the original formula, it can be derived:
\begin{equation*}
\begin{aligned}
&\mathbb{E}({\bar{S}(x^{s}, x^{(s+1)})})\\
&=\frac{{\mathbb{E}(|\varepsilon^s|)\cdot|x^s-x'|+
\mathbb{E}(|\varepsilon^{s+1}|)\cdot|x^{s+1}-x'|}}
{2|x^{s+1}-x^s|}P(\varepsilon^s\cdot\varepsilon^{s+1}<0)\\
&\;\quad+\frac{\mathbb{E}(|\varepsilon^s|+|\varepsilon^
{s+1}|)}{2}P(\varepsilon^s\cdot\varepsilon^{s+1}\ge0).
\end{aligned}
\end{equation*}

Note that $P(\varepsilon^s\cdot\varepsilon^{s+1}<0)+
P(\varepsilon^s\cdot\varepsilon^{s+1}\ge0)=1$, and
\begin{equation*}
\begin{aligned}
&\frac{{\mathbb{E}(|\varepsilon^s|)\cdot|x^s-x'|+
\mathbb{E}(|\varepsilon^{s+1}|)\cdot|x^{s+1}-x'|}}
{2|x^{s+1}-x^s|} \\
&=\frac{{\mathbb{E}(|\varepsilon^s|)\cdot|\frac{x^s-x'}
{x^{s+1}-x^s}|+\mathbb{E}(|\varepsilon^{s+1}|)\cdot|
\frac{x^{s+1}-x'}{x^{s+1}-x^s}|}}{2}\\
&<\frac{{\mathbb{E}(|\varepsilon^s|)+\mathbb{E}(|
\varepsilon^{s+1}|)}}{2},
\end{aligned}
\end{equation*}
imply that:
\begin{equation*}
\mathbb{E}(\bar{S}(x^s,x^{s+1}))<\mathbb{E}(\frac{|
\varepsilon^s|+|\varepsilon^{s+1}|}{2}).
\end{equation*}

\section*{Proof of Theorem 2}
If $\varepsilon^s\cdot\varepsilon^{s+1}<0$, then $\exists
\;x'\in(x^s,x^{s+1})$, such that $g(x')=l(x')$. It follows 
that:
\begin{equation*}
\begin{aligned}
\bar{S}(x^s,x^{s+1})
&=\frac{|\varepsilon^{s}|\cdot|\frac{x^s-x'}{x^{s}-x^
{s+1}}|+|\varepsilon^{s+1}|\cdot|\frac{x^{s+1}-x'}{x^{s}-
x^{s+1}}|}{2}.
\end{aligned}
\end{equation*}

Based on the properties of similar triangles, we can 
infer that:
\begin{equation*}
|\frac{x^s-x'}{x^{s}-x^{s+1}}|=|\frac{\varepsilon^s}
{\varepsilon^s-\varepsilon^{s+1}}|,
\end{equation*}
\begin{equation*}
|\frac{x^{s+1}-x'}{x^{s}-x^{s+1}}|=|\frac{\varepsilon^
{s+1}}{\varepsilon^s-\varepsilon^{s+1}}|.
\end{equation*}

Substituting into the original formula, it can be derived:
\begin{equation*}
\begin{aligned}
\bar{S}(x^s,x^{s+1})
&=\frac{|\frac{\varepsilon^s}{\varepsilon^s-\varepsilon^
{s+1}}|\cdot |\varepsilon^{s}|+|\frac{\varepsilon^{s+1}}
{\varepsilon^s-\varepsilon^{s+1}}|\cdot|\varepsilon^
{s+1}|}{2}\\
&<\frac{|\varepsilon^s|+|\varepsilon^{s+1}|}{2}.
\end{aligned}
\end{equation*}

If $\varepsilon^s\cdot\varepsilon^{s+1}\ge0$, then
\begin{equation*}
\begin{aligned}
\bar{S}(x^s,x^{s+1})
&=\frac{(|\varepsilon^s|+|\varepsilon^{s+1}|)\cdot|x^
{s+1}-x^{s}|}{2|x^{s+1}-x^{s}|}\\
&=\frac{|\varepsilon^s|+|\varepsilon^{s+1}|}{2}.
\end{aligned}
\end{equation*}

\section*{Proof of Theorem 3}
Since $\varepsilon^\prime\rightarrow0$, it follows from 
Equation \eqref{eq7} that:
\begin{equation*}
y^{s,s+1}=g_{s,s+1}(\boldsymbol{x}^{s,s+1})+\varepsilon.
\end{equation*}

According to (\ref{eq16}), (\ref{eq17}):
\begin{equation*}
\begin{aligned}
\varepsilon^{s,s+1}_{(d)}
&=y_{(d)}^{s,s+1}-f(\boldsymbol{x}_{(d)}^{s,s+1})\\
&=y^s+d\dfrac{y^{s+1}-y^{s}}{\left\lceil \frac{n\eta}
{\text{dist}_{\text{sum}}}\text{dist}(\boldsymbol{x}^s,
\boldsymbol{x}^{s+1})\right\rceil+1}\\
&-g{(\boldsymbol{x}^s+
d\dfrac{\boldsymbol{x}^{s+1}-\boldsymbol{x}^{s}}{\left
\lceil \frac{n\eta}{\text{dist}_{\text{sum}}}\text{dist}
(\boldsymbol{x}^s,\boldsymbol{x}^{s+1})\right\rceil+1})}.
\end{aligned}
\end{equation*}

Let $\dfrac{d}{\left\lceil \frac{n\eta}{\text{dist}_
{\text{sum}}}\text{dist}(\boldsymbol{x}^s,\boldsymbol{x}^
{s+1})\right\rceil+1}=m$, we have:
\begin{equation*}
\begin{aligned}
\varepsilon^{s,s+1}_{(d)}
&=y^s-g(\boldsymbol{x}^s)+m\cdot(y^{s+1}-g(\boldsymbol
{x}^{s+1})-y^s+g(\boldsymbol{x}^{s}))\\
&=(1-m)\cdot\varepsilon^s+m\cdot\varepsilon^{s+1}.
\end{aligned}
\end{equation*}

Since $0<m<1$, we can infer that:
\begin{equation*}
\begin{aligned}
&\mathbb{E}(\frac{\textstyle\sum_{d=1}^{{\left\lceil 
\frac{n\eta }{\text{dist}_{\text{sum}}}\text{dist}
(\boldsymbol{x}^s,\boldsymbol{x}^{s+1})\right\rceil}}
{|\varepsilon^{s,s+1}_{(d)}|}}{\left\lceil \frac{n\eta }
{\text{dist}_{\text{sum}}}\text{dist}(\boldsymbol{x}^s,
\boldsymbol{x}^{s+1})\right\rceil}) \\
&=\frac{\textstyle\sum_{d=1}^{{\left\lceil \frac{n\eta }
{\text{dist}_{\text{sum}}}\text{dist}(\boldsymbol{x}^s,
\boldsymbol{x}^{s+1})\right\rceil}}{\mathbb{E}|(1-m)
\cdot\varepsilon^s+m\cdot\varepsilon^{s+1}|}}{\left\lceil 
\frac{n\eta }{\text{dist}_{\text{sum}}}\text{dist}
(\boldsymbol{x}^s,\boldsymbol{x}^{s+1})\right\rceil}\\
&<\frac{\textstyle\sum_{d=1}^{{\left\lceil \frac{n\eta }
{\text{dist}_{\text{sum}}}\text{dist}(\boldsymbol{x}^s,
\boldsymbol{x}^{s+1})\right\rceil}}{\mathbb{E}(|(1-m)
\cdot\varepsilon^s|+|m\cdot\varepsilon^{s+1}|)}}
{\left\lceil \frac{n\eta }{\text{dist}_{\text{sum}}}
\text{dist}(\boldsymbol{x}^s,\boldsymbol{x}^{s+1})\right
\rceil}\\
&=\mathbb{E}(\frac{|\varepsilon^s|+|\varepsilon^{s+1}|}
{2}).
\end{aligned}
\end{equation*}

}


\bibliography{ref}
\bibliographystyle{IEEEtran}




\vfill

\end{document}


