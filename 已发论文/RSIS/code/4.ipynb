{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "# 忽略所有警告\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy.spatial import KDTree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance_matrix\n",
    "from tsp_solver.greedy import solve_tsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_min_distance_method(samples,p):\n",
    "    tree = KDTree(samples)\n",
    "    min_distance = np.inf\n",
    "    closest_pair = (0, 0)\n",
    "\n",
    "    for i in range(samples.shape[0]):\n",
    "        distances, indices = tree.query(samples[i], k=2, p=p)  # k=2 返回最近的两个点（包括自身）\n",
    "        if distances[1] < min_distance:  # distances[1] 是第二近的点（即除了自身外最近的点）\n",
    "            min_distance = distances[1]\n",
    "            closest_pair = (i, indices[1])\n",
    "\n",
    "    return closest_pair\n",
    "\n",
    "def K_Space(cluster_nums, samples, p=2):\n",
    "\n",
    "    n = samples.shape[0]\n",
    "    num_each_cluster = n // cluster_nums\n",
    "    clusters = [[] for _ in range(cluster_nums+1)]\n",
    "    cluster_centers = []\n",
    "    samples_remaining = samples.copy()\n",
    "    add_cluster_index = 0\n",
    "    clusters_ = []\n",
    "    time = 0\n",
    "\n",
    "    while samples_remaining.shape[0] + len(cluster_centers)>1:\n",
    "        #打印每次迭代的效果\n",
    "        \n",
    "        # print(f\"第{time}次循环 samples_reminding:{samples_remaining.shape[0]} 临时类数量:{len(cluster_centers)} 产出类数量:{len(clusters_)}\")\n",
    "        \n",
    "        matrix = np.vstack([samples_remaining,[i for i in cluster_centers]]) if len(cluster_centers)>0 else samples_remaining\n",
    "        #首先计算合并的矩阵中距离最近的两个元素\n",
    "        index1, index2 = efficient_min_distance_method(matrix,p=p)\n",
    "        #求得两个元素是样本还是簇心\n",
    "        label1 = 0 if index1 < samples_remaining.shape[0] else 1 # 0表示样本,1表示簇心\n",
    "        label2 = 0 if index2 < samples_remaining.shape[0] else 1\n",
    "        \n",
    "        #样本点之间的合并\n",
    "        if add_cluster_index < len(clusters) and label1+label2 == 0:\n",
    "            clusters[add_cluster_index].extend([samples_remaining[index1], samples_remaining[index2]])\n",
    "            cluster_centers.append(np.mean(clusters[add_cluster_index], axis=0))\n",
    "            samples_remaining = np.delete(samples_remaining, [index1, index2], axis=0)\n",
    "            add_cluster_index += 1\n",
    "            # print(f\"样本合并创建新类 \")\n",
    "            \n",
    "        #当簇的数量足够时,如果还是两个样本,则将两个样本作为一个整体加入距离簇心最近的类中\n",
    "        elif add_cluster_index == len(clusters) and label1+label2 == 0:\n",
    "            two_samples_mean = np.mean([samples_remaining[index1], samples_remaining[index2]],axis=0)\n",
    "            # print(cluster_centers)\n",
    "            # print(clusters)\n",
    "            # print(clusters_)\n",
    "            if add_cluster_index == 0 and len(clusters) == 0:\n",
    "                clusters.append([])\n",
    "                clusters.append([])\n",
    "                clusters[add_cluster_index].extend([samples_remaining[index1], samples_remaining[index2]])\n",
    "                cluster_centers.append(np.mean(clusters[add_cluster_index], axis=0))\n",
    "                samples_remaining = np.delete(samples_remaining, [index1, index2], axis=0)\n",
    "                add_cluster_index += 1\n",
    "                # print(f\"类数已充足,增加类数\")\n",
    "            else:\n",
    "                cluster_centers_matrix = np.vstack([i for i in cluster_centers])\n",
    "                tree = KDTree(cluster_centers_matrix)\n",
    "                _, indices = tree.query(two_samples_mean, k=1, p=p)\n",
    "                clusters[indices].extend([samples_remaining[index1], samples_remaining[index2]])\n",
    "                cluster_centers[indices] = np.mean(clusters[indices], axis=0)\n",
    "                samples_remaining = np.delete(samples_remaining, [index1,index2], axis=0)\n",
    "                # print(f\"两个样本归纳新类\")\n",
    "            \n",
    "        #样本点添加到类中\n",
    "        elif label1+label2 == 1:\n",
    "            index_samples = index1 if label1 == 0 else index2\n",
    "            index_cluster = index1-samples_remaining.shape[0] if label1 == 1 else index2-samples_remaining.shape[0]\n",
    "            clusters[index_cluster].append(samples_remaining[index_samples])\n",
    "            cluster_centers[index_cluster] = np.mean(clusters[index_cluster], axis=0)\n",
    "            samples_remaining = np.delete(samples_remaining, index_samples, axis=0)\n",
    "            # print(f\"一个样本归纳新类\")\n",
    "           \n",
    "        #类与类之间的合并\n",
    "        else:\n",
    "            index1 = index1 - samples_remaining.shape[0]\n",
    "            index2 = index2 - samples_remaining.shape[0]\n",
    "            if len(clusters[index1])+len(clusters[index2]) <= num_each_cluster:#如果两个类中样本合并的数量不足饱和\n",
    "                clusters[index1] = clusters[index1]+clusters[index2]\n",
    "                cluster_centers[index1] = np.mean(clusters[index1], axis=0)\n",
    "                del clusters[index2]\n",
    "                del cluster_centers[index2]\n",
    "                add_cluster_index -= 1\n",
    "            #使用lof算法剔除多余样本点\n",
    "            else:\n",
    "                points = np.vstack([clusters[index1]+clusters[index2]])\n",
    "                del clusters[max(index1,index2)]\n",
    "                del clusters[min(index1,index2)]\n",
    "                del cluster_centers[max(index1,index2)]\n",
    "                del cluster_centers[min(index1,index2)]\n",
    "                lof_model = LocalOutlierFactor(n_neighbors=int(num_each_cluster/2),p=p)\n",
    "                lof_model.fit_predict(points)\n",
    "                score = abs(lof_model.negative_outlier_factor_)\n",
    "                clusters_.append(list(points[np.argsort(score)[:num_each_cluster]]))\n",
    "                samples_remaining = np.vstack([samples_remaining,points[np.argsort(score)[num_each_cluster:]]])\n",
    "                add_cluster_index -= 2\n",
    "            clusters.append([])\n",
    "            # print(f\"两个类合并\")\n",
    "            \n",
    "        time += 1\n",
    "        #剔除已经饱和的簇\n",
    "        for i in range(add_cluster_index):\n",
    "            if len(clusters[i]) >= num_each_cluster:\n",
    "                clusters_.append(clusters[i])\n",
    "                del clusters[i]\n",
    "                del cluster_centers[i]\n",
    "                add_cluster_index -= 1\n",
    "                break\n",
    "        #将最后剩余的样本点归纳为一个类\n",
    "        if samples_remaining.shape[0] + len(cluster_centers) == 1:\n",
    "            if len(clusters) == 0:\n",
    "                break\n",
    "            clusters_.append(clusters[0])\n",
    "            \n",
    "\n",
    "    return clusters_\n",
    "\n",
    "#计算聚类效果\n",
    "def calculate_indicator(samples,label):\n",
    "    clusters_center = []\n",
    "    for i in range(np.max(label+1)):\n",
    "        center = np.mean(samples[np.where(label==i)],axis=0)\n",
    "        clusters_center.append(center)\n",
    "    clusters_center = np.array(clusters_center)\n",
    "    m = 0\n",
    "    for i in range(samples.shape[0]):\n",
    "        distance = np.linalg.norm(samples[i,:]-clusters_center,axis=1)\n",
    "        if np.argmin(distance) != label[i]:\n",
    "            m += 1\n",
    "            \n",
    "    return m/samples.shape[0]\n",
    "    \n",
    "def list_transform_array(clusters):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    max_depth = int(len(clusters)/3)\n",
    "    #将原本聚类结果完全转化为列表,导出label\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(len(clusters[i])):\n",
    "            x_train.append(clusters[i][j])\n",
    "            y_train.append(i)\n",
    "    #将列表格式的数据转化为数组\n",
    "    x_train = np.vstack(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    return x_train,y_train\n",
    "    \n",
    "def fine_turning(x_train,y_train):\n",
    "    n,m = x_train.shape\n",
    "    nums_class = np.max(y_train)+1\n",
    "    #计算优化前的指标\n",
    "    indicator = calculate_indicator(x_train,y_train)\n",
    "    # print(f\"优化前聚类效果指标:{indicator}\")\n",
    "    #选择机器学习模型进行预测,调整聚类结果\n",
    "    knn = KNeighborsClassifier(n_neighbors=int(n/nums_class))\n",
    "    knn.fit(x_train, y_train)\n",
    "    predictions = knn.predict(x_train)\n",
    "    # print(f\"优化样本占比:{round((1-accuracy_score(y_train, predictions))*100,5)}%\\n优化后聚类效果指标:{calculate_indicator(x_train,predictions)}\")\n",
    "    \n",
    "    c = predictions\n",
    "    while next(x for x in range(len(c)+1) if x not in c) <= c.max():\n",
    "        missing_number = next(x for x in range(len(c)) if x not in c)\n",
    "        c = np.array([x - 1 if x > missing_number else x for x in c])\n",
    "    \n",
    "    return x_train,c\n",
    "\n",
    "def caculater_cluster_index(samples,cluster_label):\n",
    "    cluster_center = []\n",
    "    for i in range(max(cluster_label)+1):\n",
    "        cluster = samples[np.where(cluster_label == i)]\n",
    "        cluster_center.append(np.mean(cluster,axis=0))\n",
    "    distance_matrix_ = distance_matrix(cluster_center,cluster_center)\n",
    "    path = solve_tsp( distance_matrix_, endpoints = (0,0) )\n",
    "    return path\n",
    "\n",
    "def transform(b,c,index):\n",
    "    conclusion = []\n",
    "    for i in range(max(index)+1):\n",
    "        number = index[i]\n",
    "        conclusion.append(np.vstack(b[np.where(c==number)]))\n",
    "    \n",
    "    return conclusion\n",
    "\n",
    "def match1(data, regular=False):\n",
    "    # 输入的data需要时簇心排序好的list\n",
    "    ''' \n",
    "        input:  list data 里面是每一类的array（簇心已排序好）\n",
    "                torch: 是否考虑上一条线 默认为True\n",
    "        output: list Para 里面是相邻两类的拟合直线参数\n",
    "    '''\n",
    "    if not regular:\n",
    "        Para = []\n",
    "        for i in range(len(data)-1):\n",
    "            model = LinearRegression()\n",
    "            d = np.concatenate([data[i],data[i+1]])\n",
    "            model.fit(d[:,:-1], d[:,-1])\n",
    "            para = np.append(model.coef_,model.intercept_)\n",
    "            Para.append(para)\n",
    "    \n",
    "    else:\n",
    "        # 拟合第一条线\n",
    "        Para = []\n",
    "        model = LinearRegression()\n",
    "        d = np.concatenate([data[0],data[1]])\n",
    "        # model.fit(data[0][:,:-1], data[0][:,-1])\n",
    "        model.fit(d[:,:-1], d[:,-1])\n",
    "        para = np.append(model.coef_,model.intercept_)\n",
    "        Para.append(para)\n",
    "        # 簇心\n",
    "        center = []\n",
    "        for i in range(len(data)):\n",
    "            center.append(np.mean(data[i][:,:-1],axis=0))\n",
    "\n",
    "        for i in tqdm(range(1,len(data)-1)):\n",
    "            p = torch.randn(len(Para[0]), requires_grad=True)\n",
    "            p_ = torch.tensor(Para[i-1]).float()\n",
    "            d = torch.tensor(np.concatenate([data[i],data[i+1]])).float()\n",
    "            epoch_num = 3000\n",
    "            optimizer = torch.optim.Adam([p],lr=0.01)\n",
    "            # lam = 0.5 # 惩罚项系数\n",
    "            temp_min = ((np.array(center)[1:]-np.array(center)[:-1])**2).min()\n",
    "            temp_max = ((np.array(center)[1:]-np.array(center)[:-1])**2).max()\n",
    "            lam = (temp_max - (center[i]-center[i-1])**2) / (temp_max-temp_min) * 0.5\n",
    "            lam = lam[0]\n",
    "\n",
    "            for epoch in range(epoch_num):\n",
    "                optimizer.zero_grad()\n",
    "                L = ((torch.matmul(d[:,:-1],p[:-1].reshape(-1,1))+p[-1]-d[:,-1].reshape(-1,1))**2).mean() + lam*((p-p_)**2).mean()\n",
    "                L.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            Para.append(p.detach().numpy())\n",
    "    return Para\n",
    "\n",
    "def match2(A,B):\n",
    "    ''' \n",
    "        input:  list A 第一类每个点的误差\n",
    "                list B 第二类每个点的误差\n",
    "        output: array row_ind 第一类的匹配点索引\n",
    "                array col_ind 第二类的匹配点索引\n",
    "                total_cost  总成本\n",
    "    '''\n",
    "    # 提供的成本矩阵\n",
    "    cost_matrix = np.array([[abs(a + b) for b in B] for a in A])\n",
    "\n",
    "    # 获取原始矩阵的行数和列数\n",
    "    rows, cols = cost_matrix.shape\n",
    "\n",
    "    # 扩展成本矩阵为方阵\n",
    "    if rows == cols:\n",
    "        extended_matrix = cost_matrix\n",
    "    else:\n",
    "        size = max(rows, cols)\n",
    "        extended_matrix = np.full((size, size), np.max(cost_matrix) + 1) # 使用最大值+1来扩充成本矩阵\n",
    "        extended_matrix[:rows, :cols] = cost_matrix\n",
    "\n",
    "    # 应用匈牙利算法\n",
    "    row_ind, col_ind = linear_sum_assignment(extended_matrix)\n",
    "\n",
    "    # 初始化总成本\n",
    "    total_cost = 0\n",
    "\n",
    "    # 仅使用原始矩阵范围内的匹配来计算总成本\n",
    "    total_cost = extended_matrix[row_ind, col_ind].sum()\n",
    "    total_cost = total_cost - (np.max(cost_matrix) + 1)*np.abs(len(B)-len(A))\n",
    "    if len(A) > len(B):\n",
    "        for j in range(np.abs(len(B)-len(A))):\n",
    "            index = row_ind[np.where(col_ind==len(A)-1-j)[0][0]]\n",
    "            col_ind[index] = np.argmin(cost_matrix[index,:])\n",
    "            total_cost += np.min(cost_matrix[index,:])\n",
    "    elif len(A) < len(B):\n",
    "        for j in range(np.abs(len(B)-len(A))):\n",
    "            index = row_ind[np.where(row_ind==len(B)-1-j)[0][0]]\n",
    "            row_ind[index] = np.argmin(cost_matrix[:,col_ind[index]])\n",
    "            total_cost += np.min(cost_matrix[:,col_ind[index]])\n",
    "\n",
    "    # 输出结果\n",
    "    return row_ind, col_ind, total_cost\n",
    "\n",
    "def interpolation(samples, y, k, eta, regular=True):\n",
    "    a = K_Space(k,samples)\n",
    "    b,c = list_transform_array(a)\n",
    "    b,c = fine_turning(b,c)\n",
    "    index = caculater_cluster_index(b,c)\n",
    "    conclusion = transform(b,c,index)\n",
    "    \n",
    "    for i in range(len(conclusion)):\n",
    "        indices = np.array([np.where((samples == row).all(axis=1))[0][0] for row in conclusion[i]])\n",
    "        conclusion[i] = np.concatenate([conclusion[i], y[indices]],axis=1)\n",
    "\n",
    "    data = conclusion\n",
    "    Para = match1(data, regular)\n",
    "    Row_ind, Col_ind, Total_cost = [], [], []\n",
    "    for i in range(len(Para)):\n",
    "        A = list((np.matmul(data[i][:,:-1], Para[i][:-1].reshape(-1,1)) + Para[i][-1] - data[i][:,-1].reshape(-1,1)).reshape(-1))\n",
    "        B = list((np.matmul(data[i+1][:,:-1], Para[i][:-1].reshape(-1,1)) + Para[i][-1] - data[i+1][:,-1].reshape(-1,1)).reshape(-1))\n",
    "        row_ind, col_ind, total_cost = match2(A,B)\n",
    "        Row_ind.append(row_ind)\n",
    "        Col_ind.append(col_ind)\n",
    "        Total_cost.append(total_cost)\n",
    "\n",
    "    samples_add = np.concatenate([samples,y],axis=1)\n",
    "    for i in range(len(Row_ind)):\n",
    "        for j in range(len(Row_ind[i])):\n",
    "            point1 = data[i][Row_ind[i][j]]\n",
    "            point2 = data[i+1][Col_ind[i][j]]\n",
    "            # distance = np.sqrt(((point2-point1)**2).sum())\n",
    "            distance = np.linalg.norm(point2-point1,ord=2)\n",
    "            num_samples = int(np.floor(eta*distance))\n",
    "            sample_points = np.linspace(point1, point2, num_samples+ 2)[1:-1]\n",
    "            samples_add = np.concatenate([samples_add,sample_points])\n",
    "    \n",
    "    return data, Para, samples_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:02<00:00,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size:361\n",
      "after ASISO:3148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "add_noise = \"False\"\n",
    "nomarlization = \"True\"\n",
    "k =20#hyperparameters K\n",
    "eta = 10#hyperparameters eta\n",
    "train_data = pd.read_csv(r\"/Users/dyk/Desktop/paper/ASIDS/forestfires1.csv\")\n",
    "train_data = train_data.dropna(axis = 0)\n",
    "x = train_data.iloc[:,:-1].values\n",
    "y = train_data.iloc[:,-1].values\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=123)\n",
    "if add_noise == \"True\":\n",
    "    n = x_train.shape[0]\n",
    "    per = [0.2,0.3,0.5]\n",
    "    noise = np.hstack((np.random.normal(0,64,np.int(per[0]*n)+1),np.random.uniform(-8,8,np.int(per[1]*n)),np.random.normal(0,0.04,np.int(per[2]*n))))\n",
    "    y_train = y_train+ noise\n",
    "if nomarlization == \"True\":\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_train = min_max_scaler.fit_transform(x_train)\n",
    "    y_train = min_max_scaler.fit_transform(y_train.reshape(-1,1))\n",
    "    x_test = min_max_scaler.fit_transform(x_test)\n",
    "    y_test = min_max_scaler.fit_transform(y_test.reshape(-1,1))\n",
    "else:\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    y_test = y_test.reshape(-1,1)\n",
    "\n",
    "sort_cluster, Para, data = interpolation(x_train.astype(float), y_train.astype(float), k, eta)\n",
    "x_train_amli = data[:,:-1]\n",
    "y_train_amli = data[:,-1].reshape(-1,1)\n",
    "print(f\"sample size:{x_train.shape[0]}\\nafter ASISO:{x_train_amli.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before AMLI , k: 1 ,  MAE: 0.04062502395295469\n",
      "before AMLI , k: 6 ,  MAE: 0.04198464137854572\n",
      "before AMLI , k: 11 ,  MAE: 0.041986554825288847\n",
      "before AMLI , k: 16 ,  MAE: 0.04187174654620333\n",
      "before AMLI , k: 21 ,  MAE: 0.04279147440905207\n",
      "before AMLI , k: 26 ,  MAE: 0.042996307617371804\n",
      "before AMLI , k: 31 ,  MAE: 0.04354757112427605\n",
      "before AMLI , k: 36 ,  MAE: 0.04317423092498148\n",
      "before AMLI , k: 41 ,  MAE: 0.043282880858205\n",
      "before AMLI , k: 46 ,  MAE: 0.04315337895709288\n",
      "before AMLI , k: 51 ,  MAE: 0.04280052185020014\n",
      "before AMLI , k: 56 ,  MAE: 0.04273866165077797\n",
      "before AMLI , k: 61 ,  MAE: 0.04248537341452726\n",
      "before AMLI , k: 66 ,  MAE: 0.0423879914383797\n",
      "before AMLI , k: 71 ,  MAE: 0.042149177081940443\n",
      "before AMLI , k: 76 ,  MAE: 0.04187400057389325\n",
      "before AMLI , k: 81 ,  MAE: 0.04190963423230769\n",
      "before AMLI , k: 86 ,  MAE: 0.04174487148944698\n",
      "before AMLI , k: 91 ,  MAE: 0.04157512896414612\n",
      "before AMLI , k: 96 ,  MAE: 0.041309080395685975\n",
      "before AMLI , k: 101 ,  MAE: 0.04117035579629759\n",
      "before AMLI , k: 106 ,  MAE: 0.04095710174638743\n",
      "before AMLI , k: 111 ,  MAE: 0.04087987642829876\n",
      "before AMLI , k: 116 ,  MAE: 0.04078731720097769\n",
      "before AMLI , k: 121 ,  MAE: 0.040741563260497195\n",
      "before AMLI , k: 126 ,  MAE: 0.04065790932880599\n",
      "before AMLI , k: 131 ,  MAE: 0.040644637418049245\n",
      "before AMLI , k: 136 ,  MAE: 0.040673387121784775\n",
      "after AMLI , k: 1 ,  MAE: 0.0386438852495581\n",
      "after AMLI , k: 6 ,  MAE: 0.0400007780989076\n",
      "after AMLI , k: 11 ,  MAE: 0.041326865969924\n",
      "after AMLI , k: 16 ,  MAE: 0.04111484400114505\n",
      "after AMLI , k: 21 ,  MAE: 0.04129419869477847\n",
      "after AMLI , k: 26 ,  MAE: 0.040917938463298054\n",
      "after AMLI , k: 31 ,  MAE: 0.041203735436095824\n",
      "after AMLI , k: 36 ,  MAE: 0.041172001860641426\n",
      "after AMLI , k: 41 ,  MAE: 0.04088389248562356\n",
      "after AMLI , k: 46 ,  MAE: 0.04100955308712981\n",
      "after AMLI , k: 51 ,  MAE: 0.04120485399283717\n",
      "after AMLI , k: 56 ,  MAE: 0.041450760057140434\n",
      "after AMLI , k: 61 ,  MAE: 0.04134027030641176\n",
      "after AMLI , k: 66 ,  MAE: 0.04130602913103367\n",
      "after AMLI , k: 71 ,  MAE: 0.04133668284977809\n",
      "after AMLI , k: 76 ,  MAE: 0.041369862697919986\n",
      "after AMLI , k: 81 ,  MAE: 0.041426875447351436\n",
      "after AMLI , k: 86 ,  MAE: 0.041575249648750284\n",
      "after AMLI , k: 91 ,  MAE: 0.041615585020849485\n",
      "after AMLI , k: 96 ,  MAE: 0.04156278313429278\n",
      "after AMLI , k: 101 ,  MAE: 0.041621197234889246\n",
      "after AMLI , k: 106 ,  MAE: 0.041589684677885985\n",
      "after AMLI , k: 111 ,  MAE: 0.04161217696888178\n",
      "after AMLI , k: 116 ,  MAE: 0.041695813390972676\n"
     ]
    }
   ],
   "source": [
    "#knn\n",
    "from sklearn import neighbors\n",
    "for k in range(1,140,5):\n",
    "    modelKNN=neighbors.KNeighborsRegressor(n_neighbors=k,weights='distance')\n",
    "    modelKNN.fit(x_train,y_train)\n",
    "    y_pred = modelKNN.predict(x_test)\n",
    "    print(\"before AMLI , k:\",(k),\",  MAE:\",(mean_absolute_error(y_pred,y_test)))\n",
    "for k in range(1,120,5):\n",
    "    modelKNN=neighbors.KNeighborsRegressor(n_neighbors=k,weights='distance')\n",
    "    modelKNN.fit(x_train_amli,y_train_amli)\n",
    "    y_pred = modelKNN.predict(x_test)\n",
    "    print(\"after AMLI , k:\",(k),\",  MAE:\",(mean_absolute_error(y_pred,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before AMLI MAE:0.076799\n",
      "after AMLI MAE:0.047849\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "np.random.seed(124)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "model =MLPRegressor(hidden_layer_sizes=(12,10,8,4),activation=\"relu\",batch_size=100)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "mse1 = mean_absolute_error(y_pred,y_test)\n",
    "model =MLPRegressor()\n",
    "model.fit(x_train_amli,y_train_amli)\n",
    "y_pred = model.predict(x_test)\n",
    "mes2 = mean_absolute_error(y_pred,y_test)\n",
    "print(\"before AMLI MAE:%f\\nafter AMLI MAE:%f\"%(mse1,mes2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据集MAS:0.073671\n",
      "处理后数据集MAE:0.059040\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "model1 = SVR(kernel=\"rbf\")\n",
    "model1.fit(x_train,y_train)\n",
    "y_pred=model1.predict(x_test)\n",
    "mse1 = mean_absolute_error(y_test,y_pred)\n",
    "model1 = SVR(kernel=\"rbf\")\n",
    "model1.fit(x_train_amli,y_train_amli)\n",
    "y_pred=model1.predict(x_test)\n",
    "mse2 = mean_absolute_error(y_test,y_pred)\n",
    "print(\"原数据集MAE:%f\\n处理后数据集MAE:%f\"%(mse1,mse2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before AMLI, loss:0.044995 , n_estimators:10\n",
      "before AMLI, loss:0.047813 , n_estimators:20\n",
      "before AMLI, loss:0.047946 , n_estimators:30\n",
      "before AMLI, loss:0.047350 , n_estimators:40\n",
      "before AMLI, loss:0.047887 , n_estimators:50\n",
      "before AMLI, loss:0.047687 , n_estimators:60\n",
      "before AMLI, loss:0.048397 , n_estimators:70\n",
      "before AMLI, loss:0.047979 , n_estimators:80\n",
      "before AMLI, loss:0.048306 , n_estimators:90\n",
      "before AMLI, loss:0.048327 , n_estimators:100\n",
      "before AMLI, loss:0.048557 , n_estimators:110\n",
      "before AMLI, loss:0.048403 , n_estimators:120\n",
      "before AMLI, loss:0.048173 , n_estimators:130\n",
      "before AMLI, loss:0.048231 , n_estimators:140\n",
      "before AMLI, loss:0.048164 , n_estimators:150\n",
      "before AMLI, loss:0.048997 , n_estimators:160\n",
      "before AMLI, loss:0.048519 , n_estimators:170\n",
      "before AMLI, loss:0.048646 , n_estimators:180\n",
      "before AMLI, loss:0.048656 , n_estimators:190\n",
      "before AMLI, loss:0.048860 , n_estimators:200\n",
      "after AMLI, loss:0.041114 , n_estimators:10\n",
      "after AMLI, loss:0.041574 , n_estimators:20\n",
      "after AMLI, loss:0.042117 , n_estimators:30\n",
      "after AMLI, loss:0.043879 , n_estimators:40\n",
      "after AMLI, loss:0.045096 , n_estimators:50\n",
      "after AMLI, loss:0.046158 , n_estimators:60\n",
      "after AMLI, loss:0.047307 , n_estimators:70\n",
      "after AMLI, loss:0.048820 , n_estimators:80\n",
      "after AMLI, loss:0.049509 , n_estimators:90\n",
      "after AMLI, loss:0.050497 , n_estimators:100\n",
      "after AMLI, loss:0.050586 , n_estimators:110\n",
      "after AMLI, loss:0.050594 , n_estimators:120\n",
      "after AMLI, loss:0.051174 , n_estimators:130\n",
      "after AMLI, loss:0.051511 , n_estimators:140\n",
      "after AMLI, loss:0.051934 , n_estimators:150\n",
      "after AMLI, loss:0.052291 , n_estimators:160\n",
      "after AMLI, loss:0.052311 , n_estimators:170\n",
      "after AMLI, loss:0.053017 , n_estimators:180\n",
      "after AMLI, loss:0.053628 , n_estimators:190\n",
      "after AMLI, loss:0.053910 , n_estimators:200\n"
     ]
    }
   ],
   "source": [
    "#GBDT\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "for i in range(10,201,10):\n",
    "    model2 = GradientBoostingRegressor(n_estimators=i)\n",
    "    model2.fit(x_train,y_train)\n",
    "    y_pred = model2.predict(x_test)\n",
    "    print(\"before AMLI, loss:%f , n_estimators:%d\"%(mean_absolute_error(y_test,y_pred),i))\n",
    "for i in range(10,201,10):\n",
    "    model2 = GradientBoostingRegressor(n_estimators=i)\n",
    "    model2.fit(x_train_amli,y_train_amli)\n",
    "    y_pred = model2.predict(x_test)\n",
    "    print(\"after AMLI, loss:%f , n_estimators:%d\"%(mean_absolute_error(y_test,y_pred),i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(fun,x,y):\n",
    "    y_true = np.array([fun(i) for i in x])\n",
    "    return np.sum(abs(y-y_true))/x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:13<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "优化前:129.54131425492812\n",
      "优化后:102.85788077634928\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "def F1(x, bias_F1=10):\n",
    "    z = x\n",
    "    return np.sum(z**2) + bias_F1\n",
    "\n",
    "# def F1(x):\n",
    "#     z = x\n",
    "#     return sum([sum(z[:i])**2 for i in range(len(z))])\n",
    "n = 500\n",
    "per = [0.5,0.3,0.2]\n",
    "noise = np.hstack((np.random.normal(0,32,np.int(per[0]*n)),np.random.uniform(-8,8,np.int(per[1]*n)),np.random.normal(0,0.04,np.int(per[2]*n))))\n",
    "# y_train = y_train+ noise\n",
    "\n",
    "# 设置参数\n",
    "D = 10  # 特征维度\n",
    "lower_bound, upper_bound = -10, 10  # 取值范围\n",
    "\n",
    "# 生成偏移向量\n",
    "offset_F1 = np.random.uniform(lower_bound, upper_bound, D)\n",
    "\n",
    "# 生成样本数据并计算F1函数值\n",
    "x = np.random.uniform(lower_bound, upper_bound, (n, D))\n",
    "y = np.array([F1(sample) for sample in x])+noise\n",
    "x = x+np.hstack((np.random.normal(0,3,np.int(per[0]*D)),np.random.uniform(-1,1,np.int(per[1]*D)),np.random.normal(0,1,np.int(per[2]*D))))\n",
    "_, _, data = interpolation(x.astype(float), y.reshape(-1,1), 150, 1)\n",
    "print(f\"优化前:{calculate_mae(F1,x,y)}\\n优化后:{calculate_mae(F1,data[:,:-1],data[:,-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([284.43879448, 297.44329143, 548.49289731, 232.26925788,\n",
       "       404.45174118, 388.32062572, 473.68181502, 325.50367984,\n",
       "       292.92505631, 288.69654209, 375.53629523, 411.45117524,\n",
       "       387.11047823, 357.55987303, 215.58313494, 299.7071789 ,\n",
       "       213.73195798, 376.80915689, 413.00027243, 458.04377101,\n",
       "       125.54237109, 369.27043573, 404.24073872, 431.84381674,\n",
       "       219.14525966, 306.47264655, 354.09616406, 345.5406529 ,\n",
       "       231.5089221 , 196.54373744, 450.20081385, 402.13244653,\n",
       "       267.43707216, 380.32189602, 421.77779019, 244.23428332,\n",
       "       340.67087903,  26.02478098, 209.60878844, 408.02866518,\n",
       "       289.13575841, 208.27790633, 403.52185831, 295.22121448,\n",
       "       388.92833155, 416.63351476, 379.83984894, 460.58092588,\n",
       "       571.25159214, 298.45581244, 370.5677669 , 406.03478581,\n",
       "       256.9741942 , 251.91870553, 320.27596572, 529.02093755,\n",
       "       506.33209292, 438.74587392, 444.47711747, 423.99207921,\n",
       "       482.87543416, 484.97843926, 329.58449195, 207.30590751,\n",
       "       486.7371867 , 318.90475296, 363.42962873, 276.92073358,\n",
       "       438.98818569, 393.35975167, 315.40693963, 295.19227154,\n",
       "       281.63980164, 485.28281578, 402.43244998, 314.16146199,\n",
       "       405.51916993, 528.9181171 , 459.45165021, 222.23068804,\n",
       "       593.74018082, 446.61078535, 222.84961347, 414.44807924,\n",
       "       370.56881474, 262.38256789, 381.32726224, 502.45166864,\n",
       "       192.96798331, 335.51604991, 321.41188031, 326.47637429,\n",
       "       211.92634213, 511.76314059, 317.1705812 , 392.1412209 ,\n",
       "       321.00853314, 175.88474872, 235.2492267 , 437.32352991,\n",
       "       206.07192518, 340.30645454, 383.61742836, 430.60299462,\n",
       "       206.2812467 , 371.86927706, 274.86401437, 554.38509512,\n",
       "       354.16175103, 409.95853136, 398.49889678, 352.83937241,\n",
       "       226.12298079, 317.00620282, 336.1256654 , 344.3651288 ,\n",
       "       400.03728653, 334.93247196, 403.27804917, 349.4641575 ,\n",
       "       437.6784987 , 440.67302398, 257.95181005, 452.80949091,\n",
       "       394.32776723, 315.04400439, 328.49666315, 272.26364648,\n",
       "       313.55673028, 259.48356108, 460.30025755, 367.14179107,\n",
       "       408.9523867 , 484.9692201 , 327.18763149, 225.63046985,\n",
       "       166.11442904, 286.30345083, 356.7203326 , 293.55282953,\n",
       "       257.03387546, 322.45305573, 380.40578538, 243.95704482,\n",
       "       376.42911865, 198.63022973, 339.61457691, 233.05614188,\n",
       "       524.73642675, 424.16494939, 419.05842733, 178.76432239,\n",
       "       294.76926445, 339.14020009, 188.990005  , 423.87909591,\n",
       "       222.47856763, 325.03206009, 335.29319486, 305.90423544,\n",
       "       172.36060985, 254.80780392, 360.02794949, 353.87783525,\n",
       "       461.45210292, 324.24263109, 307.69515937, 369.7532985 ,\n",
       "       465.96045735, 275.610595  , 334.12652195, 422.73852068,\n",
       "       331.99606297, 397.51493321, 279.25527432, 469.70021439,\n",
       "       300.33523984, 267.82485588, 375.61975317, 265.35843001,\n",
       "       442.81564206, 530.86437584, 549.67678727, 191.06721921,\n",
       "       382.20066073, 338.68405922, 182.69165463, 380.47154187,\n",
       "       306.66840295, 342.57626052, 414.77608096, 489.93731275,\n",
       "       318.0537706 , 288.18056025, 218.73039402, 406.95347126,\n",
       "       248.64162544, 252.66057788, 432.29321023, 454.18597247,\n",
       "       414.4536301 , 186.93099607, 297.31250328, 236.78722617,\n",
       "       379.48594601, 365.46788258, 390.62375639, 342.59058903,\n",
       "       320.14088381, 214.68910248, 499.10663941, 268.11980008,\n",
       "       225.77005397, 245.69492775, 529.87754992, 387.24573451,\n",
       "       332.15697485, 459.55736097, 190.76064995, 293.2041134 ,\n",
       "       269.06565601, 233.50315819, 206.65877389, 301.54773414,\n",
       "       416.42898862, 255.57838262, 439.54758144, 289.55762994,\n",
       "       361.5584977 , 452.62006682, 328.29765164, 265.63322229,\n",
       "       239.11758148, 295.56067469, 340.33148777, 393.18065466,\n",
       "       329.31610242, 459.48285686, 319.05160284, 255.0389134 ,\n",
       "       183.04606269, 462.44341059, 273.17704661, 514.3700785 ,\n",
       "       392.79444276, 448.63590206, 400.2784155 , 341.82256338,\n",
       "       449.20557101, 217.54861799, 403.34633208, 382.76776433,\n",
       "       348.60337155, 391.04891501, 272.15618296, 316.49937603,\n",
       "       224.90354154, 383.19289091, 205.69468069, 315.61239759,\n",
       "       259.99338055, 197.96743322, 194.9240817 , 508.5936535 ,\n",
       "       375.41555916, 249.84113533, 352.09191985, 230.77754758,\n",
       "       118.00742698, 475.83112657, 299.90973328, 340.26731585,\n",
       "       455.87021231, 405.84148199, 327.78033415, 371.29057556,\n",
       "       277.25920592, 414.19652324, 354.23114963, 308.02597547,\n",
       "       328.19976333, 414.63120158, 161.88032811, 271.64615681,\n",
       "       121.79983676, 354.30081864, 324.72221846, 492.99197471,\n",
       "       154.50257032, 290.43883078, 325.87340236, 268.71498018,\n",
       "       325.66210132, 331.48847979, 207.46021796, 419.12577306,\n",
       "       254.89223329, 271.53625693, 139.23463937, 275.88045256,\n",
       "       348.17533106, 432.06470547, 259.57813243, 317.42021617,\n",
       "       442.37909937, 439.21814226, 295.06192999, 377.89254199,\n",
       "       144.30706794, 272.22401405, 333.67129244, 384.73238363,\n",
       "       300.6258937 , 337.09270807, 473.05272275, 317.36923901,\n",
       "       505.09614304, 369.84410352, 338.34020088, 368.22051388,\n",
       "       217.4909294 , 422.67426306, 348.72514124, 290.89269146,\n",
       "       258.96732332, 373.15925371, 328.70140646, 334.93374127,\n",
       "       487.97796044, 476.34035975, 365.25315455, 274.77009111,\n",
       "       410.52440385, 247.81934267, 347.89523688, 273.99808374,\n",
       "       479.88010244, 370.33034817, 184.21726018, 357.68547251,\n",
       "       233.22604945, 374.79787942, 308.27954715, 317.69932711,\n",
       "       475.73242589, 452.62374651, 361.30570465, 394.87777622,\n",
       "       464.69104162, 393.66452072, 259.62837591, 330.1440271 ,\n",
       "       336.01650126, 353.45668765, 397.27169465, 387.73326593,\n",
       "       350.1901833 , 483.13880515, 345.91889769, 352.07305997,\n",
       "       314.96669421, 275.34857308, 174.76971151, 195.56989191,\n",
       "       409.9749782 , 303.23887988, 457.7383653 , 536.56482181,\n",
       "        92.22285795, 289.35422167, 155.93708988, 449.54921969,\n",
       "       410.03594422, 274.19655598, 411.24119203, 473.68599264,\n",
       "       377.43408631, 339.51410345, 193.73954769, 237.26748074,\n",
       "       196.18954191, 449.59855286, 244.70391183, 482.11810619,\n",
       "       217.79578052, 348.68619708, 406.08308267, 399.69437127,\n",
       "       531.77864768, 194.76850378, 510.59244122, 326.01499774,\n",
       "       344.33033258, 555.3223367 , 317.01692369, 288.68317995,\n",
       "       402.13730223, 206.43370256, 267.67252033, 236.20265644,\n",
       "       388.88020395, 402.68423386, 346.67338247, 334.35507801,\n",
       "       541.02450239, 316.14297975, 226.19534691, 297.92705401,\n",
       "       199.97790124, 352.77933995, 340.41990937, 436.82697235,\n",
       "       206.83901511, 386.40810249, 344.92415795, 260.19881747,\n",
       "       292.98699643, 421.26993485, 281.42107933, 377.69637072,\n",
       "       389.68957768, 274.83099568, 320.02056247, 277.34024151,\n",
       "       263.11075164, 336.03406594, 330.99883163, 204.13588597,\n",
       "       253.82673123, 451.44540215, 345.11212693, 305.70798866,\n",
       "       315.0946555 , 288.18689276, 214.26011357, 244.02934074,\n",
       "       416.64120124, 271.3685979 , 281.79815915, 410.78797647,\n",
       "       344.15375502, 199.43881308, 386.89757782, 252.59540013,\n",
       "       401.158262  , 253.11439358, 351.97781118, 295.39679765,\n",
       "       311.17867926, 398.56258314, 257.9151112 , 407.41175286,\n",
       "       343.90040827, 226.2646091 , 242.15563938, 306.47008984,\n",
       "       361.41278928, 255.21514938, 359.07139989, 256.26947244,\n",
       "       284.71476218, 203.75594755, 169.90274478, 388.15741151,\n",
       "       347.27557618, 352.43199564, 242.58584887, 191.22556323,\n",
       "       316.26265375, 484.28683338, 304.1143837 , 278.99858287,\n",
       "       479.03744071, 261.92908211, 269.97291605, 495.09004413,\n",
       "       371.80154352, 257.65078622, 252.27220109, 264.81551897,\n",
       "       282.50167006, 525.77151396, 170.7512156 , 267.89367777,\n",
       "       174.11278108, 260.34307459, 380.76364336, 441.07255958,\n",
       "       260.16984187, 312.20913308, 336.24779119, 405.96998383,\n",
       "       316.54249421, 195.40090022, 252.05374853, 415.94451454,\n",
       "       268.50532612, 276.65872557, 374.61627325, 306.10832536])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d866bab37dd9ab0edb7ee7b3f9538265ec58f225140511a088397364b5079bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
