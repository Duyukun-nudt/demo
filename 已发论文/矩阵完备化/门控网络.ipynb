{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(matrix1, matrix2):\n",
    "    # 确保两个矩阵具有相同的维度\n",
    "    if matrix1.shape != matrix2.shape:\n",
    "        raise ValueError(\"Both matrices must have the same dimensions.\")\n",
    "    \n",
    "    # 计算差异\n",
    "    diff = matrix1 - matrix2\n",
    "    \n",
    "    # 计算均方误差（MSE）\n",
    "    mse = torch.mean(diff ** 2)\n",
    "    \n",
    "    # 计算RMSE\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    return rmse.item()\n",
    "\n",
    "def param_diff_loss(model, lambda_diff):\n",
    "    # 提取各个逻辑回归层的权重\n",
    "    w1 = model.log1.weight\n",
    "    w2 = model.log2.weight\n",
    "    w3 = model.log3.weight\n",
    "\n",
    "    # 计算权重之间的绝对差值\n",
    "    diff12 = torch.abs(w1 - w2).sum()\n",
    "    diff23 = torch.abs(w2 - w3).sum()\n",
    "    diff31 = torch.abs(w3 - w1).sum()\n",
    "\n",
    "    # 返回加权的参数差异损失\n",
    "    return lambda_diff * (diff12 + diff23 + diff31)\n",
    "\n",
    "def gating_diff_loss(weights, lambda_gate):\n",
    "    # 计算权重差异的绝对值\n",
    "    diff = torch.abs(torch.max(weights, dim=1)[0]-torch.min(weights, dim=1)[0]).sum()\n",
    "    # diff = (torch.abs(weights[:, :-1] - weights[:, 1:]).sum()+torch.abs(weights[:, -1] - weights[:, 0]).sum())\n",
    "    # 返回加权的差异损失\n",
    "    return -lambda_gate * diff/(weights.shape[0])\n",
    "\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_experts, dim_y):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2*hidden_size)\n",
    "        self.fc3 = nn.Linear(2*hidden_size, num_experts)\n",
    "\n",
    "        # 定义三个独立的逻辑回归模型作为专家\n",
    "        self.log1 = nn.Linear(input_size, dim_y)\n",
    "        self.log2 = nn.Linear(input_size, dim_y)\n",
    "        self.log3 = nn.Linear(input_size, dim_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 门控网络部分\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        weights = F.softmax(self.fc3(x1), dim=1)  # 得到每个专家的权重\n",
    "\n",
    "        # 专家的输出\n",
    "        y1 = torch.sigmoid(self.log1(x)) * weights[:, 0].unsqueeze(1)  # 维度对齐\n",
    "        y2 = torch.sigmoid(self.log2(x)) * weights[:, 1].unsqueeze(1)\n",
    "        y3 = torch.sigmoid(self.log3(x)) * weights[:, 2].unsqueeze(1)\n",
    "\n",
    "        # 合并各专家的加权输出\n",
    "        return y1 + y2 + y3,weights\n",
    "    \n",
    "\n",
    "def train(model, optimizer, x_train, y_train, theta,tolerance=1e-4, max_epochs=1000,lam1=1,lam2=1):\n",
    "    x_train = Variable(x_train, requires_grad=False)\n",
    "    y_train = Variable(y_train, requires_grad=False)\n",
    "\n",
    "    # last_loss = float('inf')\n",
    "    for epoch in range(max_epochs):\n",
    "        # 存储参数更新前的副本\n",
    "        param_copies = {name: p.clone().detach() for name, p in model.named_parameters()}\n",
    "        \n",
    "        optimizer[0].zero_grad()\n",
    "        outputs,weights = model(x_train)\n",
    "        \n",
    "        # loss = criterion(outputs, y_train)+param_diff_loss(model,lam1)\n",
    "        loss = ((outputs-y_train)**2).sum()/(y_train.shape[0]*y_train.shape[1])+param_diff_loss(model,lam1)\n",
    "        # a = nn.CrossEntropyLoss()\n",
    "        # loss =  F.cross_entropy(outputs,y_train, reduction='sum')+param_diff_loss(model,lam1)\n",
    "        loss.backward()\n",
    "        optimizer[0].step()\n",
    "        \n",
    "        optimizer[1].zero_grad()\n",
    "        outputs,weights = model(x_train)\n",
    "        loss1 =  ((outputs-y_train)**2).sum()/(y_train.shape[0]*y_train.shape[1])+gating_diff_loss(weights, lam2)\n",
    "        \n",
    "        loss1.backward()\n",
    "        optimizer[1].step()\n",
    "\n",
    "        # 计算参数变化\n",
    "        total_change = sum((p - param_copies[name]).abs().sum().item() for name, p in model.named_parameters())\n",
    "        \n",
    "        if (epoch+1)%100 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {(loss+loss1).item()}, Max param change: {total_change} rmse:{rmse(model(x_train)[0],theta)}')\n",
    "            # print(gating_diff_loss(weights, lam2),weights.shape)\n",
    "        \n",
    "        if total_change < tolerance:\n",
    "            print(\"Training converged.\")\n",
    "            break\n",
    "def px(x):\n",
    "    p = x@torch.inverse(x.T@x)@x.T\n",
    "    return torch.eye(p.shape[0],p.shape[1])-p\n",
    "\n",
    "\n",
    "\n",
    "def step2(x,theta,y,M,lam1=1,lam2=1,a=1):\n",
    "    beta = torch.inverse(x.T@x+lam1*torch.eye(x.shape[1],x.shape[1]))@x.T@(torch.mul(torch.mul(M,theta),y))\n",
    "    # b1 = 1/(1+2*(y.shape[0]*y.shape[1]*lam2/2)*(1-a))\n",
    "    # b2 = px(torch.mul(torch.mul(M,theta),y))\n",
    "    # a1,b,c = torch.linalg.svd(b2, full_matrices=False)\n",
    "    # b = b-a*(y.shape[0]*y.shape[1])*lam2/2\n",
    "    # for i in range(b.shape[0]):\n",
    "    #     b[i] = b[i] if b[i].item()>0 else 0\n",
    "    # b = torch.diag_embed(b, 0, -2, -1)[:a1.size(0), :c.size(1)]\n",
    "    # b = a1@b@c*b1\n",
    "    return beta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置及模拟数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混合模型数量\n",
    "K = 3\n",
    "# 样本量\n",
    "m = 400\n",
    "# 协变量特征纬度\n",
    "n1 = 20\n",
    "# 观测矩阵维度\n",
    "n2 = 400\n",
    "\n",
    "# 协变量矩阵的生成\n",
    "x = torch.randn(m,n1)\n",
    "# 模型参数生成\n",
    "w = torch.normal(0,0.2,(2,K,n1,n2))\n",
    "# w[1,1,:,:] = torch.normal(0,1,(n1,n2))\n",
    "# w[1,2,:,:] = torch.normal(0,2,(n1,n2))\n",
    "b = torch.randn(2,K,n2)*0.1\n",
    "#观测矩阵非缺失概率计算\n",
    "theta = torch.zeros((m,n2))\n",
    "y = torch.zeros((m,n2))\n",
    "for i in range(m):\n",
    "    k = np.random.choice(list(range(K)))\n",
    "    theta[i,:] = torch.sigmoid(x[i,:]@w[0,k,:,:]+b[0,k,:])\n",
    "    y[i,:]= x[i,:]@(w[1,k,:,:])\n",
    "    \n",
    "noise = torch.normal(0,((y-torch.mean(y))**2).sum()/(m*n2-1),(m,n2))\n",
    "#根据概率生成对应的示性矩阵\n",
    "M = torch.zeros((m,n2))\n",
    "for i in range(m):\n",
    "    for j in range(n2):\n",
    "        M[i,j] = 1 if np.random.uniform(0,1) <= theta[i,j] else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Estimation of $\\hat{\\theta}$ by mixed logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 36.1435661315918, Max param change: 184.3721683472395 rmse:0.1922408938407898\n",
      "Epoch 200, Loss: 33.885005950927734, Max param change: 178.98384596034884 rmse:0.1920389086008072\n",
      "Epoch 300, Loss: 33.303314208984375, Max param change: 175.33259946852922 rmse:0.19167011976242065\n",
      "Epoch 400, Loss: 33.21985626220703, Max param change: 175.41172036901116 rmse:0.19155991077423096\n",
      "Epoch 500, Loss: 33.067298889160156, Max param change: 170.57601726986468 rmse:0.19191469252109528\n",
      "Epoch 600, Loss: 33.20786666870117, Max param change: 176.42404648289084 rmse:0.19159920513629913\n",
      "Epoch 700, Loss: 33.40910720825195, Max param change: 195.41078273952007 rmse:0.19080086052417755\n",
      "Epoch 800, Loss: 32.989219665527344, Max param change: 169.85250714607537 rmse:0.19062930345535278\n",
      "Epoch 900, Loss: 32.65434646606445, Max param change: 172.6575357913971 rmse:0.19100646674633026\n",
      "Epoch 1000, Loss: 32.83058166503906, Max param change: 172.25990392267704 rmse:0.1909397542476654\n"
     ]
    }
   ],
   "source": [
    "gating_network = GatingNetwork(n1,10,3,n2)\n",
    "params_to_update = list(gating_network.log1.parameters()) + \\\n",
    "                   list(gating_network.log2.parameters()) + \\\n",
    "                   list(gating_network.log3.parameters())\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=0.1)\n",
    "\n",
    "params_to_update1 = list(gating_network.fc1.parameters()) + \\\n",
    "                   list(gating_network.fc2.parameters()) + \\\n",
    "                   list(gating_network.fc3.parameters())\n",
    "optimizer1 = torch.optim.Adam(params_to_update1, lr=0.1)\n",
    "train(gating_network, [optimizer,optimizer1], x, M,theta=theta,tolerance=1e-3, max_epochs=1000,lam1=0.1,lam2=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8316338062286377\n",
      "0.7976422905921936\n",
      "0.7974525094032288\n"
     ]
    }
   ],
   "source": [
    "weight = gating_network(x)[1]\n",
    "theta_hat = gating_network(x)[0]\n",
    "# for i in range(m):\n",
    "#     for j in range(n2):\n",
    "#         if theta_hat[i,j] <0.1:\n",
    "#             theta_hat[i,j] =0.1\n",
    "index = torch.argmin(weight,axis=1)\n",
    "y_hat = torch.zeros(m,n2)\n",
    "for i in range(max(index)+1):\n",
    "    data_index = np.where(index==i)[0].tolist()\n",
    "    x_part = x[data_index]\n",
    "    y_part = torch.mul(y,M)[data_index]\n",
    "    M_part = M[data_index]\n",
    "    theta_hat_part = 1/theta_hat[data_index]\n",
    "    beta = step2(x_part,theta_hat_part,y_part+noise[data_index],M_part,10)\n",
    "    y_hat[data_index] = x_part@beta\n",
    "print(rmse(y_hat,y))\n",
    "beta = step2(x,1/theta_hat,torch.mul(y+noise,M),M,10)\n",
    "print(rmse(x@beta,y))\n",
    "\n",
    "diff = torch.max(weight,axis=1)[0]\n",
    "for i in range(m):\n",
    "    y_hat[i] = (diff[i])*y_hat[i]+(1-diff[i])*(x@beta)[i]\n",
    "print(rmse(y_hat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4893, 0.5281, 0.5080,  ..., 0.4253, 0.5064, 0.5725],\n",
       "        [0.5767, 0.4663, 0.4610,  ..., 0.3728, 0.5987, 0.5328],\n",
       "        [0.6094, 0.5146, 0.3942,  ..., 0.4020, 0.5578, 0.4413],\n",
       "        ...,\n",
       "        [0.4065, 0.4541, 0.5654,  ..., 0.5601, 0.5444, 0.5375],\n",
       "        [0.5133, 0.5384, 0.4962,  ..., 0.4790, 0.5559, 0.5641],\n",
       "        [0.5574, 0.4775, 0.5781,  ..., 0.3848, 0.5468, 0.5708]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
