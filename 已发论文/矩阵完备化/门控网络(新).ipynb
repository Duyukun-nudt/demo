{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyk/opt/anaconda3/envs/opencv/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(matrix1, matrix2):\n",
    "    # 确保两个矩阵具有相同的维度\n",
    "    if matrix1.shape != matrix2.shape:\n",
    "        raise ValueError(\"Both matrices must have the same dimensions.\")\n",
    "    \n",
    "    # 计算差异\n",
    "    diff = matrix1 - matrix2\n",
    "    \n",
    "    # 计算均方误差（MSE）\n",
    "    mse = torch.mean(diff ** 2)\n",
    "    \n",
    "    # 计算RMSE\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    return rmse.item()\n",
    "\n",
    "def param_diff_loss(model, lambda_diff):\n",
    "    # 提取各个逻辑回归层的权重\n",
    "    w1 = model.log1.weight\n",
    "    w2 = model.log2.weight\n",
    "    w3 = model.log3.weight\n",
    "\n",
    "    # 计算权重之间的绝对差值\n",
    "    diff12 = torch.abs(w1 - w2).sum()\n",
    "    diff23 = torch.abs(w2 - w3).sum()\n",
    "    diff31 = torch.abs(w3 - w1).sum()\n",
    "\n",
    "    # 返回加权的参数差异损失\n",
    "    return lambda_diff * (diff12 + diff23 + diff31)\n",
    "\n",
    "def gating_diff_loss(weights, lambda_gate):\n",
    "    # 计算权重差异的绝对值\n",
    "    diff = torch.abs(torch.max(weights, dim=1)[0]-torch.min(weights, dim=1)[0]).sum()\n",
    "    # diff = (torch.abs(weights[:, :-1] - weights[:, 1:]).sum()+torch.abs(weights[:, -1] - weights[:, 0]).sum())\n",
    "    # 返回加权的差异损失\n",
    "    return -lambda_gate * diff/(weights.shape[0])\n",
    "\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_experts, dim_y):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2*hidden_size)\n",
    "        self.fc3 = nn.Linear(2*hidden_size, num_experts)\n",
    "\n",
    "        # 定义三个独立的逻辑回归模型作为专家\n",
    "        self.log1 = nn.Linear(input_size, dim_y)\n",
    "        self.log2 = nn.Linear(input_size, dim_y)\n",
    "        self.log3 = nn.Linear(input_size, dim_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 门控网络部分\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        weights = F.softmax(self.fc3(x1), dim=1)  # 得到每个专家的权重\n",
    "\n",
    "        # 专家的输出\n",
    "        y1 = torch.sigmoid(self.log1(x)) * weights[:, 0].unsqueeze(1)  # 维度对齐\n",
    "        y2 = torch.sigmoid(self.log2(x)) * weights[:, 1].unsqueeze(1)\n",
    "        y3 = torch.sigmoid(self.log3(x)) * weights[:, 2].unsqueeze(1)\n",
    "\n",
    "        # 合并各专家的加权输出\n",
    "        return y1 + y2 + y3,weights\n",
    "    \n",
    "class GatingNetwork1(nn.Module):\n",
    "    def __init__(self, input_size, dim_y):\n",
    "        super(GatingNetwork1, self).__init__()\n",
    "        self.log1 = nn.Linear(input_size, dim_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 专家的输出\n",
    "        y1 = torch.sigmoid(self.log1(x)) # 维度对齐\n",
    "        # 合并各专家的加权输出\n",
    "        return y1\n",
    "\n",
    "\n",
    "def train(model, optimizer, x_train, y_train, theta,tolerance=1e-4, max_epochs=1000,lam1=1,lam2=1):\n",
    "    x_train = Variable(x_train, requires_grad=False)\n",
    "    y_train = Variable(y_train, requires_grad=False)\n",
    "\n",
    "    # last_loss = float('inf')\n",
    "    for epoch in range(max_epochs):\n",
    "        # 存储参数更新前的副本\n",
    "        param_copies = {name: p.clone().detach() for name, p in model.named_parameters()}\n",
    "        \n",
    "        optimizer[0].zero_grad()\n",
    "        outputs,weights = model(x_train)\n",
    "        \n",
    "        # loss = criterion(outputs, y_train)+param_diff_loss(model,lam1)\n",
    "        loss = ((outputs-y_train)**2).sum()/(y_train.shape[0]*y_train.shape[1])+param_diff_loss(model,lam1)\n",
    "        loss.backward()\n",
    "        optimizer[0].step()\n",
    "        \n",
    "        optimizer[1].zero_grad()\n",
    "        outputs,weights = model(x_train)\n",
    "        loss1 =  ((outputs-y_train)**2).sum()/(y_train.shape[0]*y_train.shape[1])+gating_diff_loss(weights, lam2)\n",
    "        \n",
    "        loss1.backward()\n",
    "        optimizer[1].step()\n",
    "\n",
    "        # 计算参数变化\n",
    "        total_change = sum((p - param_copies[name]).abs().sum().item() for name, p in model.named_parameters())\n",
    "        \n",
    "        if (epoch+1)%100 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {(loss+loss1).item()}, Max param change: {total_change} rmse:{rmse(model(x_train)[0],theta)}')\n",
    "            # print(gating_diff_loss(weights, lam2),weights.shape)\n",
    "        \n",
    "        if total_change < tolerance:\n",
    "            print(\"Training converged.\")\n",
    "            break\n",
    "        \n",
    "def train1(model, optimizer, x_train, y_train, theta, max_epochs=1000):\n",
    "    x_train = Variable(x_train, requires_grad=False)\n",
    "    y_train = Variable(y_train, requires_grad=False)\n",
    "\n",
    "    # last_loss = float('inf')\n",
    "    for epoch in range(max_epochs):\n",
    "        # 存储参数更新前的副本\n",
    "        param_copies = {name: p.clone().detach() for name, p in model.named_parameters()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        \n",
    "        # lossf = torch.nn.CrossEntropyLoss()\n",
    "        loss = ((outputs-y_train)**2).sum()/(y_train.shape[0]*y_train.shape[1])\n",
    "        # loss = lossf(outputs,y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算参数变化\n",
    "        total_change = sum((p - param_copies[name]).abs().sum().item() for name, p in model.named_parameters())\n",
    "        \n",
    "        if (epoch+1)%100 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {(loss).item()}, Max param change: {total_change} rmse:{rmse(model(x_train),theta)}')\n",
    "            # print(gating_diff_loss(weights, lam2),weights.shape)\n",
    "        if total_change < 1e-5:\n",
    "            print(\"Training converged.\")\n",
    "            break\n",
    "\n",
    "\n",
    "def px(x):\n",
    "    p = x@torch.inverse(x.T@x)@x.T\n",
    "    return torch.eye(p.shape[0],p.shape[1])-p\n",
    "\n",
    "\n",
    "\n",
    "def step2(x,theta,y,M,lam1=1,lam2=1,a=1):\n",
    "    beta = torch.inverse(x.T@x+lam1*torch.eye(x.shape[1],x.shape[1]))@x.T@(torch.mul(torch.mul(M,theta),y))\n",
    "    \n",
    "    b1 = 1/(1+2*(y.shape[0]*y.shape[1]*lam2/2)*(1-a))\n",
    "    b2 = px(x)@(torch.mul(torch.mul(M,theta),y))\n",
    "    a1,b,c = torch.linalg.svd(b2, full_matrices=False)\n",
    "    b = b-a*(y.shape[0]*y.shape[1])*lam2/2\n",
    "    for i in range(b.shape[0]):\n",
    "        b[i] = b[i] if b[i].item()>0 else 0\n",
    "    b = torch.diag_embed(b, 0, -2, -1)[:a1.size(0), :c.size(1)]\n",
    "    b = a1@b@c*b1\n",
    "    return beta,b\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置及模拟数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混合模型数量\n",
    "K = 3\n",
    "# 样本量\n",
    "m = 400\n",
    "# 协变量特征纬度\n",
    "n1 = 20\n",
    "# 观测矩阵维度\n",
    "n2 = 400\n",
    "\n",
    "# 协变量矩阵的生成\n",
    "x = torch.randn(m,n1)\n",
    "# 模型参数生成\n",
    "w = torch.normal(0.3,0.01,(2,K,n1,n2))\n",
    "\n",
    "b = torch.normal(-1.5,0.01,(K,n2))\n",
    "#观测矩阵非缺失概率计算\n",
    "theta = torch.zeros((m,n2))\n",
    "y = torch.zeros((m,n2))\n",
    "b0 = px(x)@torch.randn(m,10)@torch.randn(10,n2)\n",
    "for i in range(m):\n",
    "    k = np.random.choice(list(range(K)))\n",
    "    theta[i,:] = torch.sigmoid(x[i,:]@w[0,k,:,:]+b[k,:])\n",
    "    y[i,:]= x[i,:]@w[1,0,:,:]+b0[i]\n",
    "    \n",
    "    \n",
    "noise = torch.normal(0,((y-torch.mean(y))**2).sum()/(m*n2-1),(m,n2))\n",
    "#根据概率生成对应的示性矩阵\n",
    "M = torch.zeros((m,n2))\n",
    "for i in range(m):\n",
    "    for j in range(n2):\n",
    "        M[i,j] = 1 if np.random.uniform(0,1) <= theta[i,j] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2345)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Estimation of $\\hat{\\theta}$ by mixed logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 35549420.0, Max param change: 171.7458719909191 rmse:0.1939687281847\n",
      "Epoch 200, Loss: 33063674.0, Max param change: 167.15387245453894 rmse:0.19379764795303345\n",
      "Epoch 300, Loss: 32862960.0, Max param change: 164.71856048703194 rmse:0.19372862577438354\n",
      "Epoch 400, Loss: 32546252.0, Max param change: 164.5410539219156 rmse:0.19370025396347046\n",
      "Epoch 500, Loss: 32721902.0, Max param change: 162.84946984238923 rmse:0.19378426671028137\n",
      "Epoch 600, Loss: 32813868.0, Max param change: 161.4576179459691 rmse:0.19366776943206787\n",
      "Epoch 700, Loss: 32746408.0, Max param change: 163.47981677297503 rmse:0.19360977411270142\n",
      "Epoch 800, Loss: 32690972.0, Max param change: 164.1170417163521 rmse:0.19368976354599\n",
      "Epoch 900, Loss: 32452222.0, Max param change: 161.72854388784617 rmse:0.1935988813638687\n",
      "Epoch 1000, Loss: 32227192.0, Max param change: 160.58252535853535 rmse:0.19355902075767517\n"
     ]
    }
   ],
   "source": [
    "gating_network = GatingNetwork(n1,10,3,n2)\n",
    "params_to_update = list(gating_network.log1.parameters()) + \\\n",
    "                   list(gating_network.log2.parameters()) + \\\n",
    "                   list(gating_network.log3.parameters())\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=0.1)\n",
    "\n",
    "params_to_update1 = list(gating_network.fc1.parameters()) + \\\n",
    "                   list(gating_network.fc2.parameters()) + \\\n",
    "                   list(gating_network.fc3.parameters())\n",
    "optimizer1 = torch.optim.Adam(params_to_update1, lr=0.1)\n",
    "train(gating_network, [optimizer,optimizer1], x, M,theta=theta,tolerance=1e-3, max_epochs=1000,lam1=100000,lam2=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.12595325708389282, Max param change: 4.564533472061157 rmse:0.11738263815641403\n",
      "Epoch 200, Loss: 0.12592995166778564, Max param change: 0.44202226400375366 rmse:0.11730387061834335\n",
      "Epoch 300, Loss: 0.12592314183712006, Max param change: 0.27587613463401794 rmse:0.11796247959136963\n",
      "Epoch 400, Loss: 0.12591825425624847, Max param change: 0.22762538492679596 rmse:0.11842868477106094\n",
      "Epoch 500, Loss: 0.12591366469860077, Max param change: 0.21742883324623108 rmse:0.11881932616233826\n",
      "Epoch 600, Loss: 0.125908762216568, Max param change: 0.20900645852088928 rmse:0.11914129555225372\n",
      "Epoch 700, Loss: 0.12590429186820984, Max param change: 0.20194783806800842 rmse:0.11939924955368042\n",
      "Epoch 800, Loss: 0.12590032815933228, Max param change: 0.19979135692119598 rmse:0.11961182951927185\n",
      "Epoch 900, Loss: 0.12589658796787262, Max param change: 0.19168363511562347 rmse:0.11979876458644867\n",
      "Epoch 1000, Loss: 0.12589222192764282, Max param change: 0.25402073562145233 rmse:0.11998186260461807\n",
      "Epoch 1100, Loss: 0.12588758766651154, Max param change: 0.2792225629091263 rmse:0.12013889104127884\n",
      "Epoch 1200, Loss: 0.1258832812309265, Max param change: 0.3090501129627228 rmse:0.12027961015701294\n",
      "Epoch 1300, Loss: 0.12587910890579224, Max param change: 0.2868731617927551 rmse:0.12041439116001129\n",
      "Epoch 1400, Loss: 0.1258746236562729, Max param change: 0.4512733817100525 rmse:0.12055020034313202\n",
      "Epoch 1500, Loss: 0.1258699893951416, Max param change: 0.4718601107597351 rmse:0.12068281322717667\n",
      "Epoch 1600, Loss: 0.12586566805839539, Max param change: 0.6736366748809814 rmse:0.12080217152833939\n",
      "Epoch 1700, Loss: 0.12585993111133575, Max param change: 0.7373425960540771 rmse:0.12089768797159195\n",
      "Epoch 1800, Loss: 0.12585455179214478, Max param change: 0.9553773999214172 rmse:0.12100816518068314\n",
      "Epoch 1900, Loss: 0.12584859132766724, Max param change: 0.9689599871635437 rmse:0.12112705409526825\n",
      "Epoch 2000, Loss: 0.1258426457643509, Max param change: 2.0228984355926514 rmse:0.12122645974159241\n"
     ]
    }
   ],
   "source": [
    "gating_network1 = GatingNetwork1(n1,n2)\n",
    "params_to_update = list(gating_network1.log1.parameters())\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=0.1)\n",
    "train1(gating_network1, optimizer, x, M,theta=theta, max_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3718421459198\n",
      "3.4501516819000244\n"
     ]
    }
   ],
   "source": [
    "theta_hat = gating_network(x)[0]\n",
    "beta,b = step2(x,1/(theta_hat+0.01),torch.mul(y+noise,M),M,lam1=10000,lam2=0.01,a=0.06)\n",
    "print(rmse(x@beta+b,y))\n",
    "\n",
    "theta_hat = gating_network1(x)\n",
    "beta,b = step2(x,1/(theta_hat+0.01),torch.mul(y+noise,M),M,lam1=10000,lam2=0.01,a=0.06)\n",
    "print(rmse(x@beta+b,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2229, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(theta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.391761064529419\n"
     ]
    }
   ],
   "source": [
    "# # weight = gating_network(x)[1]\n",
    "# theta_hat = gating_network(x)[0]\n",
    "# # index = torch.argmin(weight,axis=1)\n",
    "# # y_hat = torch.zeros(m,n2)\n",
    "# # for i in range(max(index)+1):\n",
    "# #     data_index = np.where(index==i)[0].tolist()\n",
    "# #     x_part = x[data_index]\n",
    "# #     y_part = torch.mul(y,M)[data_index]\n",
    "# #     M_part = M[data_index]\n",
    "# #     theta_hat_part = 1/theta_hat[data_index]\n",
    "# #     beta,b = step2(x_part,theta_hat_part,y_part+noise[data_index],M_part,1000,0.001,0.6)\n",
    "# #     y_hat[data_index] = x_part@beta+b\n",
    "\n",
    "# # print(rmse(y_hat,y))\n",
    "# beta,b = step2(x,1/theta_hat,torch.mul(y+noise,M),M,lam1=100000,lam2=0.1,a=0.06)\n",
    "# print(rmse(x@beta+b,y))\n",
    "# # diff = torch.max(weight,axis=1)[0]\n",
    "# # for i in range(m):\n",
    "# #     y_hat[i] = (1-diff[i])*y_hat[i]+(diff[i])*(x@beta+b)[i]\n",
    "# # print(rmse(y_hat,y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
