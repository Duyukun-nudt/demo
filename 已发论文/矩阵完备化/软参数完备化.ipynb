{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models(nn.Module):\n",
    "    def __init__(self, num1,num2, num_models, activation_fn=nn.Sigmoid()):\n",
    "        super(Models, self).__init__()\n",
    "        self.models = nn.ModuleDict({\n",
    "            f\"fc{i+1}\": nn.Sequential(\n",
    "                nn.Linear(num1, num2),\n",
    "                activation_fn,\n",
    "                nn.Linear(num2,1),\n",
    "                activation_fn\n",
    "            ) for i in range(num_models)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs =  [model(x) for _, model in self.models.items()]\n",
    "        data = torch.cat(outputs, dim=-1)\n",
    "        return data\n",
    "\n",
    "def train(model, optimizer, x_train, y_train,max_epochs=1000,lam=1):\n",
    "    x_train = Variable(x_train, requires_grad=False)\n",
    "    y_train = Variable(y_train, requires_grad=False)\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs= model(x_train)\n",
    "        # loss1 = nn.CrossEntropyLoss()\n",
    "        loss2 = 0\n",
    "        # if lam!=0:\n",
    "        #     for i in range(y_train.shape[1]):\n",
    "        #         for k in range(i+1,y_train.shape[1]):\n",
    "        #             loss2 += abs(model.models[f'fc{i+1}'][0].weight-model.models[f'fc{k+1}'][0].weight).sum()\n",
    "        #             loss2 += abs(model.models[f'fc{i+1}'][2].weight-model.models[f'fc{k+1}'][2].weight).sum()\n",
    "        if lam != 0:\n",
    "            weights = [model.models[f'fc{i+1}'][0].weight for i in range(y_train.shape[1])]\n",
    "            # 对所有权重层两两之间计算差异的绝对值和\n",
    "            for i in range(len(weights)):\n",
    "                for j in range(i + 1, len(weights)):\n",
    "                    loss2 += torch.abs(weights[i] - weights[j]).sum()\n",
    "\n",
    "        loss = ((outputs-y_train)**2).sum()+lam*loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1)%100 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {(loss).item()}')\n",
    "\n",
    "def px(x):\n",
    "    p = x@torch.inverse(x.T@x)@x.T\n",
    "    return torch.eye(p.shape[0],p.shape[1])-p\n",
    "\n",
    "def rmse(matrix1, matrix2):\n",
    "    # 确保两个矩阵具有相同的维度\n",
    "    if matrix1.shape != matrix2.shape:\n",
    "        raise ValueError(\"Both matrices must have the same dimensions.\")\n",
    "    # 计算差异\n",
    "    diff = matrix1 - matrix2\n",
    "    # 计算均方误差（MSE）\n",
    "    mse = torch.mean(diff ** 2)\n",
    "    # 计算RMSE\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse.item()\n",
    "\n",
    "def step2(x,theta,y,M,lam1=1,lam2=1,a=1):\n",
    "    beta = torch.inverse(x.T@x+lam1*torch.eye(x.shape[1],x.shape[1]))@x.T@(torch.mul(torch.mul(M,theta),y))\n",
    "    b1 = 1/(1+2*(y.shape[0]*y.shape[1]*lam2/2)*(1-a))\n",
    "    b2 = px(x)@(torch.mul(torch.mul(M,theta),y))\n",
    "    a1,b,c = torch.linalg.svd(b2, full_matrices=False)\n",
    "    b = b-a*(y.shape[0]*y.shape[1])*lam2/2\n",
    "    for i in range(b.shape[0]):\n",
    "        b[i] = b[i] if b[i].item()>0 else 0\n",
    "    b = torch.diag_embed(b, 0, -2, -1)[:a1.size(0), :c.size(1)]\n",
    "    b = a1@b@c*b1\n",
    "    return beta,b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100 # 样本量\n",
    "n1 = 4 # 协变量矩阵特征纬度\n",
    "n2 = 10 # 观测矩阵纬度\n",
    "\n",
    "\n",
    "\n",
    "w = torch.normal(0.3,1,(n1,n2))\n",
    "b = torch.normal(-1.5,0.01,(n2,1)).reshape(-1)\n",
    "for i in range(1,n2):\n",
    "    w[:,i] = w[:,0]+torch.normal(0,0.2,(n1,1)).reshape(-1)\n",
    "x = torch.normal(0,1,(m,n1))#生成固定分布的协变量矩阵\n",
    "theta = torch.sigmoid(x@w+b)\n",
    "\n",
    "w1 = torch.normal(0.3,0.01,(n1,n2))\n",
    "b1 = px(x)@torch.randn(m,10)@torch.randn(10,n2)\n",
    "y = x@w1+b1\n",
    "noise = torch.normal(0,((y-torch.mean(y))**2).sum()/(m*n2-1),(m,n2))\n",
    "\n",
    "#定义示性矩阵\n",
    "M = torch.zeros((m,n2))\n",
    "for i in range(m):\n",
    "    for j in range(n2):\n",
    "        M[i,j] = 1 if np.random.uniform(0,1) <= theta[i,j] else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 184/1000 [00:00<00:01, 616.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 147.68936157226562\n",
      "Epoch 200, Loss: 135.5389404296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 380/1000 [00:00<00:00, 643.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300, Loss: 128.22332763671875\n",
      "Epoch 400, Loss: 121.49529266357422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 576/1000 [00:00<00:00, 644.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss: 115.40435028076172\n",
      "Epoch 600, Loss: 110.0126724243164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 773/1000 [00:01<00:00, 651.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700, Loss: 104.27096557617188\n",
      "Epoch 800, Loss: 97.87535858154297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 636.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900, Loss: 92.19554901123047\n",
      "Epoch 1000, Loss: 87.27667999267578\n",
      "平均误差:0.15355032682418823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Models(n1,n1+2,n2)\n",
    "opt = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "train(model,opt,x,M,lam=0,max_epochs=1000)\n",
    "mae = abs(model(x)-theta).sum()/(m*n2)\n",
    "print(f\"平均误差:{mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算缺失位置误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.033164978027344"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta,b = step2(x,model(x)**(-1),torch.mul(y+noise,M),M,lam1=1000000,lam2=0.8,a=0.8)\n",
    "rmse(torch.mul(x@beta+b,M),torch.mul(y,M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算全部位置误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.422530174255371"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(x@beta+b,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
